{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Follow the tutorial material from Kaggle (use TF-IDF instead)\n",
    "* 1-gram\n",
    "* 5000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./labeledTrainData.tsv', delimiter='\\t', quoting=3)\n",
    "test = pd.read_csv('./testData.tsv', delimiter='\\t', quoting=3)[:50]\n",
    "\n",
    "# get the number of training and test examples\n",
    "n_train = len(train)\n",
    "n_test = len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review2words(review):\n",
    "    \"\"\" function to convert input review into string of words \"\"\"\n",
    "    # Remove HTML\n",
    "    review_text = BeautifulSoup(review, 'lxml').get_text() \n",
    "\n",
    "    # Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "\n",
    "    # Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "\n",
    "    # Join the words and return the result.\n",
    "    return \" \".join(words)\n",
    "\n",
    "# get train label\n",
    "train_y = train['sentiment'].values\n",
    "\n",
    "# transform reviews into words list\n",
    "train_review = list(map(review2words, train['review']))\n",
    "test_review = list(map(review2words, test['review']))\n",
    "\n",
    "# combine train and test reviews\n",
    "all_review = train_review + test_review\n",
    "\n",
    "# perform TF-IDF transformation\n",
    "vectorizer = TfidfVectorizer(min_df=3, analyzer=\"word\", strip_accents='unicode', \n",
    "                             sublinear_tf=True, stop_words='english', \n",
    "                             max_features=5000, ngram_range=(1, 1)) \n",
    "\n",
    "# fit and transform the data\n",
    "all_features = vectorizer.fit_transform(all_review)\n",
    "\n",
    "# trainsform into array\n",
    "train_features = all_features[:n_train, :].toarray()\n",
    "test_features = all_features[n_train:, :].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the Logistic model\n",
    "logit1 = LogisticRegression(penalty='l1', tol=0.0001, C=2.7825549, random_state=2017, \n",
    "                            solver='liblinear', n_jobs=-1, verbose=0)\n",
    "logit1 = logit1.fit(train_features, train_y)\n",
    "\n",
    "# make predictions\n",
    "pred1 = logit1.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with Ridge Penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit the Logistic model\n",
    "logit2 = LogisticRegression(penalty='l2', tol=0.0001, C=2.7825549, random_state=2017, \n",
    "                            solver='liblinear', n_jobs=-1, verbose=0)\n",
    "logit2 = logit2.fit(train_features, train_y)\n",
    "\n",
    "# make predictions\n",
    "pred2 = logit2.predict_proba(test_features)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Final Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge predictions\n",
    "pred = 0.5 * pred1 + 0.5 * pred2\n",
    "label = (pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Positive and Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find all the words used\n",
    "words = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "# get coefficients and importance\n",
    "logit1coef = logit1.coef_[0, :]\n",
    "logit2coef = logit2.coef_[0, :]\n",
    "\n",
    "# make importances relative to max importance\n",
    "logit1_idx = np.argsort(logit1coef, kind='mergesort')\n",
    "logit2_idx = np.argsort(logit2coef, kind='mergesort')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define positive and negative words for eath model\n",
    "N = 200\n",
    "logit1_neg = words[logit1_idx][:N]\n",
    "logit1_pos = words[logit1_idx][-N:]\n",
    "\n",
    "logit2_neg = words[logit2_idx][:N]\n",
    "logit2_pos = words[logit2_idx][-N:]\n",
    "\n",
    "# fine common words in both model\n",
    "pos_words = list(set(logit1_pos) & set(logit2_pos))\n",
    "neg_words = list(set(logit1_neg) & set(logit2_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159 ['noir', 'touching', 'quiet', 'terrific', 'love', 'especially', 'steals', 'marvelous', 'tears', 'todd', 'superbly', 'sensitive', 'draws', 'world', 'whoopi', 'amazing', 'perfectly', 'apartment', 'loved', 'available', 'bit', 'plenty', 'subtle', 'timeless', 'spectacular', 'prince', 'humour', 'soccer', 'jackie', 'simple', 'incredible', 'definitely', 'job', 'masterpiece', 'finest', 'disappoint', 'bourne', 'seen', 'themes', 'beautifully', 'jack', 'strong', 'bound', 'wonderfully', 'tight', 'subtitles', 'favorites', 'extraordinary', 'hooked', 'moving', 'packed', 'covers', 'paramount', 'captures', 'dramas', 'underrated', 'fascinating', 'helps', 'superb', 'rare', 'pleasantly', 'surprised', 'satisfying', 'favourite', 'makes', 'unforgettable', 'edie', 'mysteries', 'flawless', 'beauty', 'fears', 'excellent', 'glad', 'delightful', 'ralph', 'vengeance', 'solid', 'raines', 'vance', 'innocent', 'spring', 'court', 'victor', 'release', 'worlds', 'beautiful', 'sweet', 'driven', 'liked', 'perfect', 'unique', 'fantastic', 'chavez', 'hilarious', 'good', 'recommended', 'friendship', 'shines', 'unlike', 'provoking', 'chilling', 'smiling', 'deeply', 'funniest', 'deal', 'brilliantly', 'enjoyable', 'fashioned', 'best', 'disagree', 'dirty', 'freedom', 'sent', 'greatest', 'eerie', 'enjoyed', 'kurosawa', 'victoria', 'believable', 'gem', 'cool', 'suspenseful', 'haunting', 'shows', 'powerful', 'vhs', 'easy', 'wonderful', 'refreshing', 'elvira', 'tragic', 'fun', 'today', 'true', 'marty', 'ride', 'surprisingly', 'highly', 'carrey', 'genius', 'appreciated', 'works', 'joan', 'outstanding', 'succeeds', 'entertaining', 'discovers', 'marie', 'ramones', 'favorite', 'lonely', 'focuses', 'atmosphere', 'brilliant', 'great', 'enjoy', 'intense', 'worth', 'troubled']\n",
      "155 ['grade', 'unless', 'just', 'blatant', 'weak', 'guess', 'furthermore', 'button', 'nudity', 'incoherent', 'worse', 'avoid', 'reason', 'sadly', 'cheap', 'basically', 'cardboard', 'unrealistic', 'confusing', 'rubbish', 'uninspired', 'mildly', 'poor', 'uninteresting', 'annoying', 'wooden', 'hollow', 'stupidity', 'paid', 'minutes', 'instead', 'pile', 'wasting', 'depressing', 'disappointing', 'villain', 'junk', 'oh', 'fest', 'script', 'stinker', 'desperately', 'stinks', 'insulting', 'couldn', 'endless', 'lacks', 'trite', 'effort', 'tries', 'horrible', 'worst', 'save', 'pathetic', 'walked', 'generous', 'designed', 'redeeming', 'silly', 'stilted', 'appalling', 'crap', 'appears', 'lee', 'problem', 'screaming', 'atrocious', 'lame', 'embarrassment', 'unconvincing', 'boom', 'seagal', 'credibility', 'badly', 'forgettable', 'mess', 'skip', 'crappy', 'lack', 'awful', 'absurd', 'unwatchable', 'premise', 'bland', 'hoping', 'original', 'turkey', 'brain', 'failed', 'ugly', 'dysfunctional', 'idea', 'ludicrous', 'pretentious', 'wonder', 'dreadful', 'boring', 'honestly', 'dull', 'fails', 'disappointed', 'blah', 'disgusting', 'dreck', 'mst', 'waste', 'unintentional', 'useless', 'pass', 'inept', 'laughable', 'terrible', 'miscast', 'handed', 'paper', 'sorry', 'obvious', 'predictable', 'garbage', 'tiresome', 'supposedly', 'alright', 'stupid', 'pointless', 'bad', 'mediocre', 'insult', 'okay', 'attempt', 'synopsis', 'offensive', 'tedious', 'paint', 'embarrassing', 'shelf', 'devoid', 'barely', 'monkey', 'slightest', 'positive', 'baldwin', 'supposed', 'poorly', 'hype', 'boredom', 'propaganda', 'unfortunately', 'bored', 'lousy', 'material', 'remotely', 'unfunny', 'obnoxious', 'ridiculous', 'disappointment']\n"
     ]
    }
   ],
   "source": [
    "print(len(pos_words), pos_words)\n",
    "print(len(neg_words), neg_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defind the head of the html file\n",
    "start_string = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<style>@import \"textstyle.css\"</style>\n",
    "</head>\n",
    "<body>\n",
    "<h1 align=\"middle\">Movie Review Visualization</h1> \n",
    "<h3 align=\"middle\"> STAT 542 Project 4 Part II, by Jifu Zhao & Jinsheng Wang </h2>\n",
    "<h2> 1. Top Positive and Negative Words</h2>\n",
    "<div>\n",
    "<img src=\"./top_words.png\" width=40%>\n",
    "<img src=\"./importance.png\" width=40%>\n",
    "</div>\n",
    "<p>\n",
    "&ensp;&ensp;&ensp;&ensp; The above figures are generated from Logistic Regression \n",
    "with Ridge Penalty and Random Forest model\n",
    "</p><br>\n",
    "<h2> 2. Detailed Text Analysis</h2>\n",
    "<h3 align=\"middle\"> <span class=\"pos\">Positive Words</span> vs. <span class=\"neg\">Negative Words</span> </h2>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# process and write to html file\n",
    "with open('./visualization.html', 'w') as f:\n",
    "    f.write(start_string)\n",
    "    f.write('<ul>\\n')\n",
    "    \n",
    "    for num in range(len(test)):\n",
    "        idx = test['id'][num][1:-1]\n",
    "        sentiment = label[num]\n",
    "        \n",
    "        # split and replace the review words\n",
    "        tmp_text = test['review'][num][1:-1]\n",
    "        tmp_text = tmp_text.replace('\\\\', '')\n",
    "        tmp_text = BeautifulSoup(tmp_text, 'lxml').get_text()\n",
    "        \n",
    "        # add space after comma that has no space\n",
    "        tmp_text = re.sub(r'([,]+)(?![ ])', r'\\1 ', tmp_text)\n",
    "        tmp_text = tmp_text.split(' ')\n",
    "        \n",
    "        # write the header part of each review\n",
    "        f.write('<hr>\\n&ensp;')\n",
    "        f.write('<strong> id: </strong> ' + str(idx) + ' &emsp; &emsp;\\n')\n",
    "        f.write('<strong> sentiment: </strong>' + str(sentiment) + ' <br>\\n')\n",
    "        f.write('<hr>\\n')\n",
    "        \n",
    "        # find all the indexes for the positive and negative words\n",
    "        pos_idx = []\n",
    "        neg_idx = []\n",
    "        for word in pos_words:\n",
    "            # regular pattern match\n",
    "#             regex = re.compile('.*(' + word + ').*', re.IGNORECASE)\n",
    "            regex = re.compile('\\\\b' + word + '.*', re.IGNORECASE)\n",
    "            pos_idx += [i for i in range(len(tmp_text)) if regex.search(tmp_text[i])]\n",
    "        \n",
    "        for word in neg_words:\n",
    "            # regular pattern match\n",
    "#             regex = re.compile('.*(' + word + ').*', re.IGNORECASE)\n",
    "            regex = re.compile('\\\\b' + word + '.*', re.IGNORECASE)\n",
    "            neg_idx += [i for i in range(len(tmp_text)) if regex.search(tmp_text[i])]\n",
    "            \n",
    "        # replace all positive words with pre-defined html class\n",
    "        for i in pos_idx:\n",
    "            tmp_text[i] = '<span class=\"pos\">' + tmp_text[i] + '</span>'\n",
    "            \n",
    "        # replace all negative words with pre-defined html class\n",
    "        for i in neg_idx:\n",
    "            tmp_text[i] = '<span class=\"neg\">' + tmp_text[i] + '</span>'\n",
    "            \n",
    "        # write into html file\n",
    "        f.write(' '.join(tmp_text))  \n",
    "        f.write('\\n<br><br>\\n\\n')\n",
    "        \n",
    "    # end of the html file\n",
    "    f.write('</ul>\\n</body>\\n</html>\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

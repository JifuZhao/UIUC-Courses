\input{setting.tex}

\usepackage{graphicx, amssymb, amsmath, listings, float, mathtools}
\usepackage{color, url}
\usepackage{romannum}
\usepackage{subcaption}
\usepackage{mwe}
\lstset{language = Python}
\lstset{breaklines}
\lstset{extendedchars=false}
 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}

\oddsidemargin 0in
\evensidemargin 0in
\textwidth 6.5in
\topmargin -0.6in
\textheight 9.0in
\pagenumbering{arabic}

\begin{document}
% -----------------------------------------------------------------------------
\solution{\large Jifu Zhao}{\large 12/15/2016}{\bf \Large IE 529 \hspace{0.5cm} 
		Fall 2016 \hspace{0.5cm} Computational Assignment 2}

% -----------------------------------------------------------------------------
\section{\Large Clustering Result Comparison}

\begin{description}
% -----------------------------------------------------------------------------
\item[\Romannum{1}.] Lloyd's (K-means)Algorithm

\begin{description}
\item{1.} In this part, with the Lloyd's algorithm for k-means clustering, we choose the best distortion D as the the objection function. The change of D versus cluster number K is shown in Fig. \ref{fig:k-means-loss}, where the result for clustering.txt is shown in Fig. \ref{fig:1a} and the result for bigClusteringData.txt is show in Fig. \ref{fig:1b}.

%  -----------------------------------------------------------------------------
\begin{figure}[H]
\centering
\centering
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/loss_clustering_kMeans.png}
            \caption{clustering.txt}\label{fig:1a}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.49\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/loss_bigClustering_kMeans.png}
            \caption{bigClusteringData.txt}\label{fig:1b}
        \end{subfigure}
\caption{Change of Distoration versus Cluster Number K for K-Means Algorithm}
\label{fig:k-means-loss} 
\end{figure}

\item{2.} The scatter plot of the clustering result for clustering.txt is shown in Fig. \ref{fig:kmean_clustering} and the scatter plot of the clustering result for bigClusteringData.txt is shown in Fig. \ref{fig:kmean_bigCustering}. The cluster centroids are clearly marked and different clusters are denoted by different colors. 

%  -----------------------------------------------------------------------------
\begin{figure}[!h]
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/clustering_kMeans_3.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kMeans_4.png}
        \end{subfigure}
%        \vskip\baselineskip        
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kMeans_5.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kMeans_6.png}
        \end{subfigure}
%        \vskip\baselineskip     
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kMeans_7.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kMeans_8.png}
        \end{subfigure}
%        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kMeans_9.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kMeans_10.png}
        \end{subfigure}
        
        \caption{Clustering Result for clustering.txt with K-Means Algorithm}
        \label{fig:kmean_clustering}
\end{figure}

%  -----------------------------------------------------------------------------
\begin{figure}[!h]
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/bigClustering_kMeans_3.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kMeans_4.png}
        \end{subfigure}
%        \vskip\baselineskip        
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kMeans_5.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kMeans_6.png}
        \end{subfigure}
%        \vskip\baselineskip     
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kMeans_7.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kMeans_8.png}
        \end{subfigure}
%        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kMeans_9.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kMeans_10.png}
        \end{subfigure}
        
        \caption{Clustering Result for bigClusteringData.txt with K-Means Algorithm}
        \label{fig:kmean_bigCustering}
\end{figure}

\newpage
\item{3.} The Python code used for K-Means Algorithm is shown in Listing 1.

% -----------------------------------------------------------------------------
\begin{lstlisting}[language=Python, caption={K-Means Algorithm Python Code}]
import numpy as np
import time

def kMeans(X, K, tol=0.00001, random_state=None, verbose=True):
    """ function to implement the Lloyd's algorithm for k-means problem """
    np.random.seed(random_state)
    t0 = time.time()

    N, d = X.shape  # number of observations and dimensions
    index = np.random.choice(range(N), size=K, replace=False)
    Y = X[index, :]  # initial k centers
    C = np.zeros(N)
    D = 100
    count = 0
    diff = 100  # difference between D1 and D0

    while diff >= tol:
        D0 = D
        for i in range(N):
            # assign centers to ith data
            C[i] = np.argmin(np.sum((Y - X[i, :]) ** 2, axis=1))

        D = 0
        # re-compute the new centers
        for j in range(K):
            Y[j, :] = np.mean(X[C == j, :], axis=0)

        # compute the loss
        loss = np.zeros((N, K))
        for i in range(K):
            loss[:, i] = np.sqrt(np.sum((X - Y[i, :])**2, axis=1))
        D = np.max(np.min(loss, axis=1))
        diff = abs(D - D0)
        count += 1

    if verbose is True:
        t = np.round(time.time() - t0, 4)
        print('K-Means finished in ' + str(t) + 's, ' + str(count) + ' iters')

    return Y, C, D
\end{lstlisting}

\end{description}

\newpage
% -----------------------------------------------------------------------------
\item[\Romannum{2}.] Greedy K-centers Algorithm

\begin{description}
\item{1.} In this part, with the Lloyd's algorithm for k-means clustering, we choose the best distortion D as the the objection function. The change of D versus cluster number K is shown in Fig. \ref{fig:k-centers-loss}, where the result for clustering.txt is shown in Fig. \ref{fig:4a} and the result for bigClusteringData.txt is show in Fig. \ref{fig:4b}.

%  -----------------------------------------------------------------------------
\begin{figure}[H]
\centering
\centering
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/loss_clustering_kCenter.png}
            \caption{clustering.txt}\label{fig:4a}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.49\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/loss_bigClustering_kCenter.png}
            \caption{bigClusteringData.txt}\label{fig:4b}
        \end{subfigure}
\caption{Change of Distoration versus Cluster Number K for K-Center Algorithm}
\label{fig:k-centers-loss} 
\end{figure}

\item{2.} The scatter plot of the clustering result for clustering.txt is shown in Fig. \ref{fig:kcenter_clustering} and the scatter plot of the clustering result for bigClusteringData.txt is shown in Fig. \ref{fig:kcenter_bigClustering}. The cluster centroids are clearly marked and different clusters are denoted by different colors. 

%  -----------------------------------------------------------------------------
\begin{figure}[!h]
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/clustering_kCenter_3.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kCenter_4.png}
        \end{subfigure}
%        \vskip\baselineskip        
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kCenter_5.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kCenter_6.png}
        \end{subfigure}
%        \vskip\baselineskip     
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kCenter_7.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kCenter_8.png}
        \end{subfigure}
%        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kCenter_9.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_kCenter_10.png}
        \end{subfigure}
        
        \caption{Clustering Result for clustering.txt with K-Center Algorithm}
        \label{fig:kcenter_clustering}
\end{figure}

%  -----------------------------------------------------------------------------
\begin{figure}[!h]
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/bigClustering_kCenter_3.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kCenter_4.png}
        \end{subfigure}
%        \vskip\baselineskip        
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kCenter_5.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kCenter_6.png}
        \end{subfigure}
%        \vskip\baselineskip     
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kCenter_7.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kCenter_8.png}
        \end{subfigure}
%        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kCenter_9.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_kCenter_10.png}
        \end{subfigure}
        
        \caption{Clustering Result for bigClusteringData.txt with K-Center Algorithm}
        \label{fig:kcenter_bigClustering}
\end{figure}

\newpage
\item{3.} The Python code used for K-Centers Algorithm is shown in Listing 2.

% -----------------------------------------------------------------------------
\begin{lstlisting}[language=Python, caption=K-Centers Algorithm Python Code]
import numpy as np
import time

def kCenters(X, K, random_state=None, verbose=True):
    """ function to implement the greedy k-centers algorithm """
    np.random.seed(random_state)
    t0 = time.time()

    N, d = X.shape
    # find the initial center
    index = np.random.choice(range(N), size=1)
    Q = np.zeros((K, d))
    Q[0, :] = X[index, :]
    idx = [index]

    i = 1
    while i < K:
        distance = np.zeros((N, i))
        for j in range(i):
            distance[:, j] = np.sum((X - Q[j, :])**2, axis=1)
        min_distance = np.min(distance, axis=1)
        new_index = np.argmax(min_distance)
        idx.append(new_index)
        Q[i, :] = X[new_index, :]
        i += 1

    loss = np.zeros((N, K))
    for i in range(K):
        loss[:, i] = np.sqrt(np.sum((X - Q[i, :])**2, axis=1))
    D = np.max(np.min(loss, axis=1))
    C = np.argmin(loss, axis=1)

    if verbose is True:
        t = np.round(time.time() - t0, 4)
        print('K-Centers is finished in ' + str(t) + 's')

    return Q, C, D, idx
\end{lstlisting}

\end{description}

\newpage
% -----------------------------------------------------------------------------
\item[\Romannum{3}.] Single-Swap Algorithm

\begin{description}
\item{1.} In this part, with the Lloyd's algorithm for k-means clustering, we choose the best distortion D as the the objection function. The change of D versus cluster number K is shown in Fig. \ref{fig:single-swap-loss}, where the result for clustering.txt is shown in Fig. \ref{fig:7a} and the result for bigClusteringData.txt is show in Fig. \ref{fig:7b}.

%  -----------------------------------------------------------------------------
\begin{figure}[H]
\centering
\centering
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/loss_clustering_singleSwap.png}
            \caption{clustering.txt}\label{fig:7a}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.49\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/loss_bigClustering_singleSwap.png}
            \caption{bigClusteringData.txt}\label{fig:7b}
        \end{subfigure}
\caption{Change of Distoration versus Cluster Number K for Single-Swap Algorithm}
\label{fig:single-swap-loss} 
\end{figure}

\item{2.} The scatter plot of the clustering result for clustering.txt is shown in Fig. \ref{fig:single-swap-clusterimg} and the scatter plot of the clustering result for bigClusteringData.txt is shown in Fig. \ref{fig:single-swap-bigClustering}. The cluster centroids are clearly marked and different clusters are denoted by different colors. 

%  -----------------------------------------------------------------------------
\begin{figure}[!h]
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/clustering_singleSwap_3.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_singleSwap_4.png}
        \end{subfigure}
%        \vskip\baselineskip        
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_singleSwap_5.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_singleSwap_6.png}
        \end{subfigure}
%        \vskip\baselineskip     
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_singleSwap_7.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_singleSwap_8.png}
        \end{subfigure}
%        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_singleSwap_9.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_singleSwap_10.png}
        \end{subfigure}
        
        \caption{Clustering Result for clustering.txt with Single-Swap Algorithm}
        \label{fig:single-swap-clusterimg}
\end{figure}

%  -----------------------------------------------------------------------------
\begin{figure}[!h]
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/bigClustering_singleSwap_3.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_singleSwap_4.png}
        \end{subfigure}
%        \vskip\baselineskip        
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_singleSwap_5.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_singleSwap_6.png}
        \end{subfigure}
%        \vskip\baselineskip     
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_singleSwap_7.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_singleSwap_8.png}
        \end{subfigure}
%        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_singleSwap_9.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_singleSwap_10.png}
        \end{subfigure}
        
        \caption{Clustering Result for bigClusteringData.txt with Single-Swap Algorithm}
        \label{fig:single-swap-bigClustering}
\end{figure}

\newpage
\item{3.} The Python code used for K-Centers Algorithm is shown in Listing 3.

% -----------------------------------------------------------------------------
\begin{lstlisting}[language=Python, caption=Single-Swap Algorithm Python Code]
import numpy as np
import time
from k_centers import kCenters

def singleSwap(X, K, tau=0.05, random_state=None, verbose=True):
    """ function to implement the single-swap for k-centers algorithm """
    t0 = time.time()

    # calculate the initial centers
    Q, _, pre_cost, _ = kCenters(X, K, random_state=random_state,
                                 verbose=False)
    N, d = X.shape

    # compute the distance based on current centers
    distance = np.zeros((N, K))
    for idx in range(K):
        distance[:, idx] = np.sqrt(np.sum((X - Q[idx, :])**2, axis=1))
    cost = np.max(np.min(distance, axis=1))  # calculate cost

    i = 0
    while i < K:
        if i == 0:
            min_dist = np.min(distance[:, 0:], axis=1)
        elif i == (K - 1):
            min_dist = np.min(distance[:, :-1], axis=1)
        else:
            min_dist = np.minimum(np.min(distance[:, :i], axis=1),
                                  np.min(distance[:, (i + 1):], axis=1))
        swap = False  # keep recording whether or not swaped
        for j in range(N):
            tmp_dist = np.sqrt(np.sum((X - X[j, :])**2, axis=1))
            new_cost = np.max(np.minimum(min_dist, tmp_dist))
            if new_cost / cost < (1 - tau):
                Q[i, :] = X[j, :]
                distance[:, i] = tmp_dist
                swap = True
                cost = new_cost
        i += 1

        if swap is False:
            if i == K - 1:
                break
            else:
                i += 1
        elif (swap is True) and (i == K):
            i = 0

    C = np.argmin(distance, axis=1)

    if verbose is True:
        t = np.round(time.time() - t0, 4)
        print('Single-Swap is finished in ' + str(t) + 's')

    return Q, C, cost
\end{lstlisting}

\end{description}

\newpage
% -----------------------------------------------------------------------------
\item[\Romannum{4}.] Spectral Clustering Algorithm

\begin{description}
\item{1.} In this part, with the Lloyd's algorithm for k-means clustering, we choose the best distortion D as the the objection function. The change of D versus cluster number K is shown in Fig. \ref{fig:spectral-loss}, where the result for clustering.txt is shown in Fig. \ref{fig:10a} and the result for bigClusteringData.txt is show in Fig. \ref{fig:10b}.

%  -----------------------------------------------------------------------------
\begin{figure}[H]
\centering
\centering
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/loss_clustering_spectral.png}
            \caption{clustering.txt}\label{fig:10a}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.49\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/loss_bigClustering_spectral.png}
            \caption{bigClusteringData.txt}\label{fig:10b}
        \end{subfigure}
\caption{Change of Distoration versus Cluster Number K for Spectral Clustering}
\label{fig:spectral-loss} 
\end{figure}


\item{2.} The scatter plot of the clustering result for clustering.txt is shown in Fig. \ref{fig:spectral_clustering} and the scatter plot of the clustering result for bigClusteringData.txt is shown in Fig. \ref{fig:spectral_bigClustering}. The cluster centroids are clearly marked and different clusters are denoted by different colors. 

%  -----------------------------------------------------------------------------
\begin{figure}[!h]
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/clustering_spectral_3.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_spectral_4.png}
        \end{subfigure}
%        \vskip\baselineskip        
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_spectral_5.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_spectral_6.png}
        \end{subfigure}
%        \vskip\baselineskip     
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_spectral_7.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_spectral_8.png}
        \end{subfigure}
%        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_spectral_9.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_spectral_10.png}
        \end{subfigure}
        
        \caption{Clustering Result for clustering.txt with Spectral Clustering}
        \label{fig:spectral_clustering}
\end{figure}

%  -----------------------------------------------------------------------------
\begin{figure}[!h]
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/bigClustering_spectral_3.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_spectral_4.png}
        \end{subfigure}
%        \vskip\baselineskip        
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_spectral_5.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_spectral_6.png}
        \end{subfigure}
%        \vskip\baselineskip     
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_spectral_7.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_spectral_8.png}
        \end{subfigure}
%        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_spectral_9.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_spectral_10.png}
        \end{subfigure}
        
        \caption{Clustering Result for bigClusteringData.txt with Spectral Clustering}
        \label{fig:spectral_bigClustering}
\end{figure}

\newpage
\item{3.} The Python code used for K-Centers Algorithm is shown in Listing 4.

% -----------------------------------------------------------------------------
\begin{lstlisting}[language=Python, caption=Spectral Clustering Algorithm Python Code]
import numpy as np
import time
from k_centers import kCenters

def spectralClustering(X, K, random_state=None, verbose=True):
    """ function to implement the spectral clustering algorithm """
    t0 = time.time()

    N, d = X.shape
    W = np.zeros((N, N))  # adjacency matrix W
    for i in range(N):
        distance = np.sqrt(np.sum((X - X[i, :])**2, axis=1))
        W[:, i] = distance

    diag = np.sum(W, axis=1)
    D = np.diag(diag)  # diagnoal matrix D
    L = D - W  # Laplacian matrix L
    L = np.identity(N) - np.dot(np.linalg.inv(D), W)
    eigvals, U = np.linalg.eigh(L)
    U = U[:, -K:]  # first K eigenvectors

    # call k-means for clustering
    _, C, _, idx = kCenters(U, K, random_state=random_state, verbose=False)

    Q = X[idx, :]
    loss = np.zeros((N, K))
    for i in range(K):
        loss[:, i] = np.sqrt(np.sum((X - Q[i, :])**2, axis=1))
    D = np.max(np.min(loss, axis=1))

    if verbose is True:
        t = np.round(time.time() - t0, 4)
        print('Spectral Clustering finished in ' + str(t) + 's')

    return W, U, Q, C, D
\end{lstlisting}

\end{description}

\newpage
% -----------------------------------------------------------------------------
\item[\Romannum{5}.] Expectation Maximization (EM) Algorithm

\begin{description}
\item{1.} In this part, with the Lloyd's algorithm for k-means clustering, we choose the best distortion D as the the objection function. The change of D versus cluster number K is shown in Fig. \ref{fig:EM-loss}, where the result for clustering.txt is shown in Fig. \ref{fig:13a} and the result for bigClusteringData.txt is show in Fig. \ref{fig:13b}.

%  -----------------------------------------------------------------------------
\begin{figure}[H]
\centering
\centering
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/loss_clustering_EM.png}
            \caption{clustering.txt}\label{fig:13a}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.49\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/loss_bigClustering_EM.png}
            \caption{bigClusteringData.txt}\label{fig:13b}
        \end{subfigure}
\caption{Change of Distoration versus Cluster Number K for EM Algorithm}
\label{fig:EM-loss} 
\end{figure}

\item{2.} The scatter plot of the clustering result for clustering.txt is shown in Fig. \ref{fig:EM_clustering} and the scatter plot of the clustering result for bigClusteringData.txt is shown in Fig. \ref{fig:EM_bigClustering}. The cluster centroids are clearly marked and different clusters are denoted by different colors. 

%  -----------------------------------------------------------------------------
\begin{figure}[!h]
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/clustering_EM_3.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_EM_4.png}
        \end{subfigure}
%        \vskip\baselineskip        
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_EM_5.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_EM_6.png}
        \end{subfigure}
%        \vskip\baselineskip     
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_EM_7.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_EM_8.png}
        \end{subfigure}
%        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_EM_9.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/clustering_EM_10.png}
        \end{subfigure}
        
        \caption{Clustering Result for clustering.txt with EM Algorithm}
        \label{fig:EM_clustering}
\end{figure}

%  -----------------------------------------------------------------------------
\begin{figure}[!h]
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/bigClustering_EM_3.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_EM_4.png}
        \end{subfigure}
%        \vskip\baselineskip        
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_EM_5.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_EM_6.png}
        \end{subfigure}
%        \vskip\baselineskip     
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_EM_7.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_EM_8.png}
        \end{subfigure}
%        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_EM_9.png}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_EM_10.png}
        \end{subfigure}
        
        \caption{Clustering Result for bigClusteringData.txt with EM Algorithm}
        \label{fig:EM_bigClustering}
\end{figure}

\newpage
\item{3.} The Python code used for K-Centers Algorithm is shown in Listing 5.

% -----------------------------------------------------------------------------
\begin{lstlisting}[language=Python, caption=EM Algorithm Python Code]
import numpy as np
import time

class EM(object):
    """ self-defined calss for EM algorithm """
    def __init__(self, m, threshold=0.01, random_state=None, maxIter=500):
        """ initialize the EM algorithm """
        self.m = m
        self.threshold = threshold
        self.random_state = random_state
        self.maxIter = maxIter
        self.w = None
        self.gamma = None
        self.mu = None
        self.sigma = None
        self.gaussianProb = None
        self.logLikelihood = None
        self.distance = None
        self.D = None

    def train(self, x, verbose=False):
        """ function to perform EM algorithm on X """
        t0 = time.time()
        np.random.seed(self.random_state)

        # initialize the mean and covariance matrix
        self.initialize(x)

        # iterate through E and M steps
        for i in range(1, self.maxIter + 1):
            self.estep(x)
            self.mstep(x)
            if abs(self.logLikelihood[-1] - self.logLikelihood[-2]) \
               / abs(self.logLikelihood[-2]) < self.threshold:
                for i in range(self.m):
                    self.distance[:, i] = np.sqrt(np.sum((x - self.mu[i])**2,
                                                         axis=1))
                self.D = np.max(np.min(self.distance, axis=1))
                if verbose is True:
                    t = np.round(time.time() - t0, 4)
                    print('Reach threshold at', i,
                          'th iters in ' + str(t) + 's')
                return

        for i in range(self.m):
            self.distance[:, i] = np.sqrt(np.sum((x - self.mu[i])**2, axis=1))
        self.D = np.max(np.min(self.distance, axis=1))
        if verbose is True:
            t = np.round(time.time() - t0, 4)
            print('Stopped, reach the maximum iteration ' + str(t) + 's')

    def initialize(self, x):
        """ function to initialize the parameters """
        n, dim = x.shape  # find the dimensions
        self.distance = np.zeros((n, self.m))
        self.w = np.ones(self.m) * (1 / self.m)
        self.gamma = np.zeros((n, self.m))
        self.gaussianProb = np.zeros((n, self.m))
        self.mu = [None] * self.m
        self.sigma = [None] * self.m
        self.logLikelihood = []

        cov = np.cov(x.T)
        mean = np.mean(x, axis=0)
        for k in range(self.m):
            self.mu[k] = mean + np.random.uniform(-0.5, 0.5, dim)
            self.sigma[k] = cov

        # update gamma
        self.gamma = self.gammaprob(x, self.w, self.mu, self.sigma)

        # calculate the expectation of log-likelihood
        self.logLikelihood.append(self.likelihood())

    def estep(self, x):
        """ function to conduct E-Step for EM algorithm """
        self.gamma = self.gammaprob(x, self.w, self.mu, self.sigma)

    def mstep(self, x):
        """ function to conduct M-Step for EM algorithm """
        n, dim = x.shape
        sumGamma = np.sum(self.gamma, axis=0)
        self.w = sumGamma / n

        for k in range(self.m):
            self.mu[k] = np.sum(x.T * self.gamma[:, k], axis=1) / sumGamma[k]
            diff = x - self.mu[k]
            weightedDiff = diff.T * self.gamma[:, k]
            self.sigma[k] = np.dot(weightedDiff, diff) / sumGamma[k]
            if np.linalg.matrix_rank(self.sigma[k]) != 3:
                randv = np.random.random(dim) / 10000
                self.sigma[k] = self.sigma[k] + np.diag(randv)

        # calculate the expectation of log-likelihood
        self.logLikelihood.append(self.likelihood())

    def gammaprob(self, x, w, mu, sigma):
        """ function to calculate the gamma probability """
        for k in range(self.m):
            self.gaussianProb[:, k] = self.gaussian(x, mu[k], sigma[k])

        weightedSum = np.sum(w * self.gaussianProb, axis=1)
        gamma = ((w * self.gaussianProb).T / weightedSum).T

        return gamma

    def gaussian(self, x, mu, sigma):
        """ function to calculate the multivariate gaussian probability """
        inversion = np.linalg.inv(sigma)
        part1 = (-0.5 * np.sum(np.dot(x - mu, inversion) * (x - mu), axis=1))
        part2 = 1 / ((2 * np.pi) ** (len(mu) / 2) *
                     (np.linalg.det(sigma) ** 0.5))

        pdf = part2 * np.exp(part1)

        return pdf

    def likelihood(self):
        """ function to calculate the log likelihood """
        log = np.log(np.sum(self.w * self.gaussianProb, axis=1))
        logLikelihood = np.sum(log)

        return logLikelihood

    def get_label(self):
        """ function to predict the classes using calculated parameters """
        label = np.argmax(self.w * self.gaussianProb, axis=1)

        return label
\end{lstlisting}
\end{description}
\end{description}

% -----------------------------------------------------------------------------
\section{\Large Natural Clusters Discussion}

%  -----------------------------------------------------------------------------
\begin{figure}[H]
\centering
\centering
        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{./figures/clustering_scatter.png}
            \caption{clustering.txt}\label{fig:16a}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.49\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{./figures/bigClustering_scatter.png}
            \caption{bigClusteringData.txt}\label{fig:16b}
        \end{subfigure}
\caption{Scatter Plot of Original Data}
\label{fig:clusters} 
\end{figure}

% -----------------------------------------------------------------------------
\section{\Large K-Means Convergence Discussion}


% -----------------------------------------------------------------------------
\section{\Large Computational Effort Discussion}

\begin{table}[H]
	\centering
	\caption{Computation Time Comparison for clustering.txt}
	\label{table:time_clustering}	
	\begin{tabular}{ c | c | c | c | c | c }
		\hline \hline
		K  	&	K-Means    & K-Centers    & Single-Swap    & Spectral Clustering    & EM \\[0.1cm]
		\hline
		3	   &	0.2673     & 0.0067   & 0.1582    & 1.5716    & 0.4412 \\[0.1cm]
		4    &	0.2543     & 0.0010   & 0.1971    & 1.4587    & 0.5078 \\[0.1cm]
		5    &	0.3624     & 0.0012   & 0.1895    & 1.4132    & 0.4881 \\[0.1cm]
		6    &	0.4325     & 0.0013   & 0.2668    & 1.4840    & 0.7462 \\[0.1cm]
		7    &	0.2837     & 0.0015   & 0.2693    & 1.4877    & 0.8227 \\[0.1cm]
		8    &	0.1923     & 0.0019   & 0.3306    & 1.4708    & 0.9216 \\[0.1cm]
		9    &	0.3481     & 0.0024   & 0.3012    & 1.4669    & 1.0106 \\[0.1cm]
		10  &	0.1989     & 0.0026   & 0.3790    & 1.5053    & 1.1277 \\[0.1cm]
		\hline	
	\end{tabular}
\end{table}
{\centering Note: all measured time is in unit of seconds (s)}

\begin{table}[H]
	\centering
	\caption{Computation Time Comparison for bigClusteringData.txt}
	\label{table:time_bigClustering}	
	\begin{tabular}{ c | c | c | c | c | c }
		\hline \hline
		K  	&	K-Means     & K-Centers    & Single-Swap    & Spectral Clustering    & EM \\[0.1cm]
		\hline
		3	   &	0.4725     & 0.0020   & 0.4511    & 11.1860    & 0.3412 \\[0.1cm]
		4    &	0.8326     & 0.0019   & 0.5692    & 10.2542    & 0.2553 \\[0.1cm]
		5    &	0.4722     & 0.0021   & 0.6445    & 10.5166    & 0.5763 \\[0.1cm]
		6    &	1.2580     & 0.0024   & 0.8809    & 10.2167    & 0.6196 \\[0.1cm]
		7    &	0.9619     & 0.0025   & 0.7638    & 9.8986      & 1.1591 \\[0.1cm]
		8    &	0.7767     & 0.0034   & 0.7815    & 9.4616      & 1.3177 \\[0.1cm]
		9    &	0.7177     & 0.0037   & 0.9010    & 9.8733      & 1.4977 \\[0.1cm]
		10    &	1.2369     & 0.0041   & 1.0744    & 9.8636      & 1.7560 \\[0.1cm]
		\hline	
	\end{tabular}
\end{table}
{\centering Note: all measured time is in unit of seconds (s)}

% -----------------------------------------------------------------------------
\section{\Large Single-Swap Algorithm Discussion}


% -----------------------------------------------------------------------------
%\begin{figure}[H]
%\centering
%\includegraphics[width=0.9\textwidth]{./figures/rss.png}
%\caption{\label{fig:RSS} RSS for different models}
%\end{figure}

\clearpage

%\bibliographystyle{plain}
%\bibliographystyle{unsrt}
%\bibliography{reference.bib}

\end{document}


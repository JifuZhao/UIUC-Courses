{
 "metadata": {
  "name": "",
  "signature": "sha256:ac3972ee22e161c84be23da20515bd4ce102e934eb3451e63a6cbf8909df2a59"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<DIV ALIGN=CENTER>\n",
      "\n",
      "# Introduction to Hadoop\n",
      "## Professor Robert J. Brunner\n",
      "  \n",
      "</DIV>  \n",
      "-----\n",
      "-----"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Introduction\n",
      "\n",
      "In this Notebook, we will demonstrate how to run a Hadoop Streaming\n",
      "Map/Reduce job in a docker container. Our setup will be using a single\n",
      "Hadoop node, which will not be very fast, especially when compared to\n",
      "simply running the map/reduce Python code directly. However, the full\n",
      "Hadoop process will be demonstrated, including the use of the Hadoop\n",
      "file system (HDFS) and the Hadoop Streaming process model. Before\n",
      "proceeding with this Notebook, be sure to (at least start to) download\n",
      "the SequenceIQ Hadoop Docker container.\n",
      "\n",
      "Typically, basic Hadoop is operated on a large cluster that runs both\n",
      "Hadoop and HDFS, although with the development of Yarn, more diverse\n",
      "workflows are now possible. In this Notebook, we only explore the basic\n",
      "Hadoop components of Hadoop and HDFS, which work together to run code on\n",
      "the nodes that hold the relevant data in order to maximize throughput.\n",
      "[Other resources][hort] exist to learn more about Yarn and other Hadoop\n",
      "workflows. The basic Hadoop task is a map/reduce process, where a map\n",
      "process analyzes data and creates a sequential list of key-value pairs\n",
      "(like a Python dictionary). The Hadoop process model sorts the output of\n",
      "the mappers before passing the results to a reduce process. The reduce\n",
      "process combines the key-value pairs to generate final output. The\n",
      "prototype map/reduce example is the [word-count problem][wcp], where a\n",
      "large corpus is analyzed to quantify how many times each word appears\n",
      "(one can quickly see how this model can be extended to analyze website\n",
      "as opposed to texts).\n",
      "\n",
      "Thus to complete a map/reduce task in Hadoop we need to complete the\n",
      "following tasks:\n",
      "\n",
      "1. create a Map program\n",
      "2. create a Reduce program\n",
      "3. obtain a data set to analyze\n",
      "4. load our data into HDFS\n",
      "5. execute our map/reduce program by using Hadoop\n",
      "\n",
      "The rest of this Notebook will demonstrate how to perform each of these\n",
      "tasks. We first will create the two Python programs, download a sample\n",
      "text, and also download the hadoop-streaming jar file into a shared\n",
      "local directory from within our course Docker container. Once these\n",
      "steps are complete, we will start our Hadoop Docker container to\n",
      "complete the rest of the process.\n",
      "\n",
      "In the next code cell, we start the process by running a shell script\n",
      "that creates (and deletes first if it exists) the shared directory that\n",
      "will hold the Python codes and data for our Map/Reduce Hadoop project.\n",
      "\n",
      "-----\n",
      "[hort]: http://hortonworks.com/hadoop-tutorial/introducing-apache-hadoop-developers/\n",
      "[wcp]: https://hadoop.apache.org/docs/r2.6.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%bash\n",
      "#!/usr/bin/env bash\n",
      "# A Bash Shell Script to delete the Hadoop diorectory if it exists, afterwhich\n",
      "# make a new Hadoop directory\n",
      "\n",
      "# Our directory name\n",
      "DIR=/notebooks/i2ds/hadoop\n",
      "\n",
      "# Delete if exists\n",
      "if [ -d \"$DIR\" ]; then\n",
      "    rm -rf \"$DIR\"\n",
      "fi\n",
      "\n",
      "# Now make the directory\n",
      "mkdir \"$DIR\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----\n",
      "### Mapper: Word Count\n",
      "\n",
      "The first Python code we will write is the map Python program. This\n",
      "program simply reads data from STDIN, tokenizes each line into words and\n",
      "outputs each word on a separate line along with a count of one. Thus our\n",
      "map program generates a list of word tokens as the keys and the value is\n",
      "always one.\n",
      "\n",
      "-----"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile /notebooks/i2ds/hadoop/mapper.py\n",
      "#!/usr/bin/env python3\n",
      "\n",
      "import sys\n",
      "\n",
      "# We explicitly define the word/count separator token.\n",
      "sep = '\\t'\n",
      "\n",
      "# We open STDIN and STDOUT\n",
      "with sys.stdin as fin:\n",
      "    with sys.stdout as fout:\n",
      "    \n",
      "        # For every line in STDIN\n",
      "        for line in fin:\n",
      "        \n",
      "            # Strip off leading and trailing whitespace\n",
      "            line = line.strip()\n",
      "            \n",
      "            # We split the line into word tokens. Use whitespace to split.\n",
      "            # Note we don't deal with punctuation.\n",
      "            \n",
      "            words = line.split()\n",
      "            \n",
      "            # Now loop through all words in the line and output\n",
      "\n",
      "            for word in words:\n",
      "                fout.write(\"{0}{1}1\\n\".format(word, sep))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----\n",
      "###Reducer: Word Count\n",
      "\n",
      "The second Python program we write is our reduce program. In this code,\n",
      "we read key-value pairs from STDIN and use the fact that the Hadoop\n",
      "process first sorts all key-value pairs before sending the map output to\n",
      "the reduce process to accumulate the cumulative count of each word. The\n",
      "following code could easily be made more sophisticated by using `yield`\n",
      "statements and iterators, but for clarity we use the simple approach of\n",
      "tracking when the current word becomes different than the previous word\n",
      "to output the key-cumulative count pairs.\n",
      "\n",
      "-----"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile /notebooks/i2ds/hadoop/reducer.py\n",
      "#!/usr/bin/env python3\n",
      "\n",
      "import sys\n",
      "\n",
      "# We explicitly define the word/count separator token.\n",
      "sep = '\\t'\n",
      "\n",
      "# We open STDIN and STDOUT\n",
      "with sys.stdin as fin:\n",
      "    with sys.stdout as fout:\n",
      "    \n",
      "        # Keep track of current word and count\n",
      "        cword = None\n",
      "        ccount = 0\n",
      "        word = None\n",
      "   \n",
      "        # For every line in STDIN\n",
      "        for line in fin:\n",
      "        \n",
      "            # Strip off leading and trailing whitespace\n",
      "            # Note by construction, we should have no leading white space\n",
      "            line = line.strip()\n",
      "            \n",
      "            # We split the line into a word and count, based on predefined\n",
      "            # separator token.\n",
      "            # Note we haven't dealt with punctuation.\n",
      "            \n",
      "            word, scount = line.split('\\t', 1)\n",
      "            \n",
      "            # We wil assume count is always an integer value\n",
      "            \n",
      "            count = int(scount)\n",
      "            \n",
      "            # word is either repeated or new\n",
      "            \n",
      "            if cword == word:\n",
      "                ccount += count\n",
      "            else:\n",
      "                # We have to handle first word explicitly\n",
      "                if cword != None:\n",
      "                    fout.write(\"{0:s}{1:s}{2:d}\\n\".format(cword, sep, ccount))\n",
      "                \n",
      "                # New word, so reset variables\n",
      "                cword = word\n",
      "                ccount = count\n",
      "        else:\n",
      "            # Output final word count\n",
      "            if cword == word:\n",
      "                fout.write(\"{0:s}{1:s}{2:d}\\n\".format(word, sep, ccount))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Writing /notebooks/rppds/hadoop/reducer.py\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----\n",
      "\n",
      "### Hadoop Streaming\n",
      "\n",
      "The Hadoop Docker container we will use in this course by default does\n",
      "not support Hadoop streaming. In order to use Python codes with Hadoop,\n",
      "however, we must have Hadoop Streaming. Fortunately, there is a simple\n",
      "solution, we just have to grab the appropriate Hadoop streaming jar file\n",
      "from the appropriate Hadoop release (our container is Hadoop version\n",
      "2.6.0). We do this below with wget, where we now explicitly specify the\n",
      "appropriate directory path and output filename (we use the much shorter\n",
      "`hs.jar` which will be asier to enter at the command line).\n",
      "\n",
      "-----"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# We need to explicitly grab hadoop streaming jar file.\n",
      "!wget --output-document=/notebooks/i2ds/hadoop/hs.jar \\\n",
      "http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.6.0/hadoop-streaming-2.6.0.jar"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "--2015-03-16 18:17:08--  http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.6.0/hadoop-streaming-2.6.0.jar\r\n",
        "Resolving central.maven.org (central.maven.org)... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "23.235.44.209\r\n",
        "Connecting to central.maven.org (central.maven.org)|23.235.44.209|:80... connected.\r\n",
        "HTTP request sent, awaiting response... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "200 OK\r\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Length: 104979 (103K) [application/java-archive]\r\n",
        "Saving to: \u2018/notebooks/rppds/hadoop/hs.jar\u2019\r\n",
        "\r\n",
        "\r",
        " 0% [                                       ] 0           --.-K/s              "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "98% [=====================================> ] 103,369      419KB/s             \r",
        "100%[======================================>] 104,979      425KB/s   in 0.2s   \r\n",
        "\r\n",
        "2015-03-16 18:17:09 (425 KB/s) - \u2018/notebooks/rppds/hadoop/hs.jar\u2019 saved [104979/104979]\r\n",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----\n",
      "\n",
      "### Word Count\n",
      "\n",
      "Our simple map/reduce programs require text data to operate. While there\n",
      "are a number of possible options, for this example we can grab a free\n",
      "book from [Project Gutenberg][pg]:\n",
      "\n",
      "    wget --directory-prefix=/notebooks/i2ds/hadoop/ --output-document=book.txt \\\n",
      "        http://www.gutenberg.org/cache/epub/4300/pg4300.txt`\n",
      "\n",
      "In this case, we have grabbed the full text of the novel _Ulysses_, by\n",
      "James Joyce.\n",
      "\n",
      "-----\n",
      "[pg]: http://www.gutenberg.org"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Grab a book to process\n",
      "!wget --output-document=/notebooks/i2ds/hadoop/book.txt \\\n",
      "http://www.gutenberg.org/cache/epub/4300/pg4300.txt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "--2015-03-16 18:17:10--  http://www.gutenberg.org/cache/epub/4300/pg4300.txt\r\n",
        "Resolving www.gutenberg.org (www.gutenberg.org)... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "152.19.134.47\r\n",
        "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\r\n",
        "HTTP request sent, awaiting response... "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "200 OK\r\n",
        "Length: 1573151 (1.5M) [text/plain]\r\n",
        "Saving to: \u2018/notebooks/rppds/hadoop/book.txt\u2019\r\n",
        "\r\n",
        "\r",
        " 0% [                                       ] 0           --.-K/s              "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        " 5% [>                                      ] 80,559       372KB/s             "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "14% [====>                                  ] 226,807      521KB/s             "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "29% [==========>                            ] 458,487      692KB/s             "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "50% [==================>                    ] 795,871      904KB/s             "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "78% [=============================>         ] 1,234,615   1.08MB/s             "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\r",
        "100%[======================================>] 1,573,151   1.24MB/s   in 1.2s   \r\n",
        "\r\n",
        "2015-03-16 18:17:12 (1.24 MB/s) - \u2018/notebooks/rppds/hadoop/book.txt\u2019 saved [1573151/1573151]\r\n",
        "\r\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----\n",
      "### Testing Python Map-Reduce\n",
      "\n",
      "Before we begin using Hadoop, we should first test our Python codes out\n",
      "to ensure they work as expected. First, we should change the permissions\n",
      "of the two programs to be executable, which we can do with the Unix\n",
      "`chmod` command.\n",
      "\n",
      "-----"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!chmod u+x /notebooks/i2ds/hadoop/mapper.py "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!chmod u+x /notebooks/i2ds/hadoop/reducer.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----\n",
      "\n",
      "#### Testing Mapper.py\n",
      "\n",
      "To test out the map Python code, we can run the Python `mapper.py` code\n",
      "and specify that the code should redirect STDIN to read the book text\n",
      "data. This is done in the following code cell, we pipe the output into\n",
      "the Unix `head` command in order to restrict the output, which would be\n",
      "one line per word found in the book text file. In the second code cell,\n",
      "we next pipe the output of  `mapper.py` into the Unix `sort` command,\n",
      "which is done automatically by Hadoop. To see the result of this\n",
      "operation, we next pipe the result into the Unix `uniq` command to count\n",
      "duplicates, pipe this result into a new sort routine to sort the output\n",
      "by the number of occurrences of a word, and finally display the last few\n",
      "lines with the Unix `tail` command to verify the program is operating\n",
      "correctly.\n",
      "\n",
      "-----"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!/notebooks/i2ds/hadoop/mapper.py <  book.txt | wc -l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "113156\r\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!/notebooks/i2ds/hadoop/mapper.py <  book.txt | sort -n -k 1 | \\\n",
      " uniq -c -d | sort -n -k 1 | tail -10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "    799 which\t1\r\n",
        "   1031 The\t1\r\n",
        "   1106 was\t1\r\n",
        "   1151 by\t1\r\n",
        "   1848 a\t1\r\n",
        "   2151 to\t1\r\n",
        "   2689 in\t1\r\n",
        "   3035 and\t1\r\n",
        "   7289 of\t1\r\n",
        "  10312 the\t1\r\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----\n",
      "\n",
      "#### Testing Reducer.py\n",
      "\n",
      "To test out the reduce Python code, we run the previous code cell, but\n",
      "rather than piping the result into the Unix `tail` command, we pipe the\n",
      "result of the sort command into the Python `reducer.py` code. This\n",
      "simulates the Hadoop model, where the map output is key sorted before\n",
      "being passed into the reduce process. First, we will simply count the\n",
      "number of lines displayed by the reduce process, which will indicate the\n",
      "number of  unique _word tokens_ in the book. Next, we will sort the\n",
      "output by the number of times each word token appears and display the\n",
      "last few lines to compare with the previous results.\n",
      "\n",
      "-----"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!/notebooks/i2ds/hadoop/mapper.py <  book.txt | sort -n -k 1 | \\\n",
      " /notebooks/i2ds/hadoop/reducer.py | wc -l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "21296\r\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!/notebooks/i2ds/hadoop/mapper.py <  book.txt | sort -n -k 1 | \\\n",
      " /notebooks/i2ds/hadoop/reducer.py | sort -n -k 2 | tail -10"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "which\t799\r\n",
        "The\t1031\r\n",
        "was\t1106\r\n",
        "by\t1151\r\n",
        "a\t1848\r\n",
        "to\t2151\r\n",
        "in\t2689\r\n",
        "and\t3035\r\n",
        "of\t7289\r\n",
        "the\t10312\r\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "-----\n",
      "## Running Hadoop\n",
      "\n",
      "To use Hadoop in this course, we will need to start up our Hadoop Docker\n",
      "container. You should have already completed the tasks outlined in the\n",
      "[Preface](preface.ipynb) IPython Notebook regarding pulling and\n",
      "retagging the SequenceIQ Ubuntu Hadoop Docker container.Once those steps\n",
      "are completed, we can start a new Hadoop Docker container by running the\n",
      "container. To use the files we created or downloaded earlier in this\n",
      "notebook, we need to share folders between the host OS and our Hadoop\n",
      "container, which we do with the `-v` flag. You will need to change the\n",
      "host path to match your personal laptop. We aso run the\n",
      "`/etc/bootstrap.sh` script, which will properly initialize the Hadoop\n",
      "environment, and the `-bash` flag indicates that we wish to be given\n",
      "access to the Hadoop container via a Bash shell.\n",
      "\n",
      "    docker run -it -v /Users/rb/i2ds:/i2ds hadoop /etc/bootstrap.sh -bash\n",
      "\n",
      "When this command is run, a series of messages are displayed that\n",
      "indicate the status of different server daemons starting up, including\n",
      "an SSH server, a namenode, a datanode, secondary nameodes, yarn\n",
      "daemons, a resourcemanager, and a nodemanager, as shown in the\n",
      "following screenshot.\n",
      "\n",
      "![Docker Hadoop startup](images/dh-start.png)\n",
      "\n",
      "If these message are all displayed and you are left with a prompt that\n",
      "resembles the string `root@185b748bbfa7:/#` you will have successfully\n",
      "started the Hadoop Docker container. As a final verification, you can\n",
      "enter the following command:\n",
      "\n",
      "    echo $HADOOP_PREFIX\n",
      "\n",
      "which should display `/usr/local/hadoop`. \n",
      "\n",
      "-----"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Python Map/Reduce\n",
      "\n",
      "At this point, we first need to change into the directory where we\n",
      "created our Python mapper and reducer programs, and where we downloaded\n",
      "the hadoop-streaming jar file and the sample book to analyze. In the\n",
      "Hadoop Docker container, enter `cd i2ds/hadoop`, which will change our\n",
      "current working directory to the appropriate location, which is\n",
      "indicated by a change in the shell prompt to `/i2ds/hadoop#`. \n",
      "\n",
      "Before proceeding, we should test our Python codes, but now within the\n",
      "Hadoop Docker container, which will have a different python environment\n",
      "than our class container. We can easily do this by modifying our earlier\n",
      "test to now use the correct path in the Hadoop Docker container:\n",
      "\n",
      "    /i2ds/hadoop/mapper.py <  book.txt | sort -n -k 1 |  \\\n",
      "        /i2ds/hadoop/reducer.py | sort -n -k 2 | tail -10\n",
      "\n",
      "Doing this, however, now gives an `UnicodeDecodeError`. The simplest\n",
      "solution is to explicitly state that the Python interpreter should use\n",
      "`utf-8` for all IO operations, which we can do by setting the Python\n",
      "environment variable `PYTHONIOENCODING` to `utf-8`. We do this by\n",
      "entering the following command at the container prompt:\n",
      "\n",
      "    export PYTHONIOENCODING=utf-8\n",
      "\n",
      "After setting this environment variable, the previous Unix command\n",
      "string will now produce the correct output.\n",
      "\n",
      "### HDFS\n",
      "\n",
      "At this point, we need to move our data to process into the Hadoop\n",
      "Distributed File system, or HDFS. HDFS is a a file system that is designed\n",
      "to work effectively with the Hadoop environment. In a typical Hadoop\n",
      "cluster, files would be broken up and distributed to different Hadoop\n",
      "nodes. The processing is moved to the data in this model, which can\n",
      "produce high throughput, especially for map/reduce programming tasks.\n",
      "However, this means you can not simply move around the HDFS file system\n",
      "in the same manner as a traditional Unix file system, since the\n",
      "components of a particular file are not all col-located. Instead, we\n",
      "must use the [HDFS file system interface][hdfs], which is invoked by\n",
      "using `$HADOOP_PREFIX/bin/hdfs`. Running this command by itself in your\n",
      "Hadoop Docker container will list the available commands, as shown in\n",
      "the following screenshot.\n",
      "\n",
      "![HDFS command listing](images/hdfs-commands.png)\n",
      "\n",
      "The standard command we will use is `dfs` which runs a filesystem\n",
      "command on the HDFS file system that is supported by Hadoop. The [list\n",
      "of supported `dfs` commands][dfsl] is extensive, and mirrors many of the\n",
      "traditional Unix file systems commands. The full listing can be obtained\n",
      "by entering `$HADOOP_PREFIX/bin/hdfs dfs` at the prompt in our Hadoop\n",
      "Docker container. Some of the more useful commands for this class\n",
      "include:\n",
      "\n",
      "- `cat`: copies the source path to STDOUT.\n",
      "\n",
      "- `count -h`: counts the number of directories, files and byts under the\n",
      "path specified. With the `-h` flag, the output is displayed in a\n",
      "human-readable format.\n",
      "\n",
      "- `expunge`: empties the trash. By default, files and directories are\n",
      "not removed from HDFS with the `rm` command, they are simply moved to the\n",
      "trash. This can be useful when HDFS supplies a `Name node is in safe\n",
      "mode.` message. \n",
      "\n",
      "- `ls`: lists the contents of the indicated directory in HDFS.\n",
      "\n",
      "- `-mkdir -p`: creates a new directory in HDFS at the specified\n",
      "location. With the `-p` flag any parent directory specified in the full\n",
      "path will also be created as necessary.\n",
      "\n",
      "- `put`: copies indicated file(s) from local host file system into the\n",
      "specified path in HDFS.\n",
      "\n",
      "- `rm -f -r`: delete the indicated file or directory. With the `-r -f`\n",
      "flags, the command will not display any message and any will delete any\n",
      "files or directories under the indicated directory. The `-skipTrash`\n",
      "flag should be used to delete the indicated resource immediately.\n",
      "\n",
      "- `tail`: display the last kilobyte of the indicated file to STDOUT.\n",
      "\n",
      "-----\n",
      "\n",
      "At this point, we first need to create an directory to hold the input\n",
      "and output of our Hadoop task. We will create a new directory called\n",
      "`wc` with a subdirectory called `in` to hold the input data for our\n",
      "Hadoop task. Second, we will need to copy the book text file into this\n",
      "new HDFS directory. This means we will need to run the following two\n",
      "commands at the prompt in our Hadoop Docker container:\n",
      "\n",
      "1. `$HADOOP_PREFIX/bin/hdfs dfs -mkdir -p wc/in`\n",
      "2. `$HADOOP_PREFIX/bin/hdfs dfs -put book.txt wc/in/book.txt`\n",
      "\n",
      "The following screenshot displays the result of running these two\n",
      "commands, as well as the `dfs -ls` command to display the contents of\n",
      "our new HDFS directory, and the `dfs -count` command to show the size of\n",
      "the directory contents.\n",
      "\n",
      "![Hadoop DFS ls](images/hdfs-ls.png)\n",
      "\n",
      "-----\n",
      "\n",
      "[hdfs]: https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html#dfs\n",
      "[dfsl]: https://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-common/FileSystemShell.html"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Python Hadoop Streaming\n",
      "\n",
      "We are now ready tio actually run our Python codes via Hadoop Streaming.\n",
      "The main command to perform this task is `$HADOOP_PREFIX/bin/hadoop jar\n",
      "hs.jar`, where `hs.jar` is the hadoop-streaming jar file we downloaded\n",
      "earlier in this Notebook. Running this command will display a usage\n",
      "message that is not extremely useful, supplying the `-help` flag will\n",
      "provide more a more useful summary. For our map/reduce Python example to\n",
      "run successfully, we will need to specify six flags:\n",
      "\n",
      "1. `-files`: a comma separated list of files to be copied to the Hadoop cluster.\n",
      "2. `-input`: the HDFS input file(s) to be used for the map task.\n",
      "3. `-output`: the HDFS output directory, used for the reduce task.\n",
      "4. `-mapper`: the command to run for the map task.\n",
      "5. `-reducer`: the command to run for the reduce task.\n",
      "6. `-cmdenv`: set environment variables for a Hadoop streaming task.\n",
      "\n",
      "Given our previous setup, we will run the full command as follows:\n",
      "\n",
      "    $HADOOP_PREFIX/bin/hadoop jar hs.jar -files mapper.py,reducer.py -input wc/in \\\n",
      "        -output wc/out -mapper mapper.py -reducer reducer.py -cmdenv PYTHONIOENCODING=utf-8\n",
      "\n",
      "When this command is run, a series of messages will be displayed to the\n",
      "screen (STDOUT) showing the progress of our Hadoop Streaming task. At\n",
      "the end of the stream of information messages will be a statement\n",
      "indicating the location of the output directory as shown below:\n",
      "\n",
      "![Hadoop Success](images/hadoop-success.png)\n",
      "\n",
      "In order to view the results of our Hadoop Streaming task, we must use\n",
      "HDFS DFS commands to examine the directory and files generated by our\n",
      "Python Map/Reduce programs. The following list of DFS commands might\n",
      "prove useful to view the results of this map/reduce job.\n",
      "\n",
      "    $HADOOP_PREFIX/bin/hdfs dfs -ls wc\n",
      "\n",
      "    $HADOOP_PREFIX/bin/hdfs dfs -ls wc/out\n",
      "\n",
      "    $HADOOP_PREFIX/bin/hdfs dfs -count -h wc/out/part-00000\n",
      "\n",
      "    $HADOOP_PREFIX/bin/hdfs dfs -tail wc/out/part-00000\n",
      "\n",
      "To compare this map/reduce Hadoop Streaming task output to our previous\n",
      "output, we can use the `$HADOOP_PREFIX/bin/hdfs dfs -cat\n",
      "wc/out/part-00000 | sort -n -k 2 | tail -10`, which should be executed at\n",
      "a Hadoop Docker container shell prompt. This code listing provides the\n",
      "succesful output of this command, following a succesful map/reduce\n",
      "processing task.\n",
      "\n",
      "```\n",
      "/i2ds/hadoop# $HADOOP_PREFIX/bin/hdfs dfs -cat wc/out/part-00000 | \\\n",
      "    sort -n -k 2 | tail -10\n",
      "\n",
      "with\t2391\n",
      "I\t2432\n",
      "he\t2712\n",
      "his\t3035\n",
      "in\t4606\n",
      "to\t4787\n",
      "a\t5842\n",
      "and\t6542\n",
      "of\t8127\n",
      "the\t13600\n",
      "```\n",
      "\n",
      "-----"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Hadoop Cleanup\n",
      "\n",
      "Following the succesful run of our map/reduce Python programs, we have\n",
      "created a new directory `wc/out`, which contains two files. If we wish\n",
      "to rerun this Hadoop Streaming map/reduce task, we must either specify a\n",
      "different output directory, or else we must clean up the results of the\n",
      "previous run. To remove the output directory, we can simply use the DFS\n",
      "`-rm -r -f -skipTrash wc/out` command, which will immediately delete the\n",
      "`wc/out` directory. The successful completion of this command is\n",
      "indicated by Hadoop, and this can also be verified by listing the\n",
      "contents of the `wc` directory as shown in the following screenshot.\n",
      "\n",
      "![Hadoop Cleanup](images/hadoop-clean.png)\n",
      "\n",
      "-----"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Additional References\n",
      "\n",
      "1. [Hadoop][1] Documentation\n",
      "\n",
      "-----\n",
      "\n",
      "[1]: http://\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
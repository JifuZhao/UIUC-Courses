## Problem 11.1. SQLite Benchmarks

- The IPython notebook template for this problem is
  [benchmark.ipynb](http://nbviewer.ipython.org/github/INFO490/spring2015/blob/master/week11/benchmark.ipynb).

You might have wondered why relational databases and SQLite were introduced in
weeks 10 and 11.
   One of the strengths of SQLite is that, once a database is stored,
   it is much faster when we have to make frequent queries.
   In this problem, we will test the question, "how fast is SQLite really?",
   by measuring the CPU time to
   - use pure Python to brute-force search a CSV file,
   - create an SQLite database by reading in a CSV file, and
   - query an existing SQLite database.

For simplicity, we will extract only two columns: `month` (the 2nd colum) and
`cancelled` (the 24th column).

(Note: The `cancelled` column is actually the 22nd column,
  and the 24th column is `diverted`. This bug was discovered too late,
  so in the following simply pretend that the 24th column is `cancelled`,
  but keep in mind that we are actually counting the number of diverted
  flights.)

    %%bash
    cat /data/airline/2001.csv | awk -F, '{print $2 "," $24}' > /data/airline/week11.csv

The `count_cancelled()` function use a simple `for` loop to read in a CSV file
line by line.
  On my machine, the results were

```python
%time n = count_cancelled(9, "/data/airline/week11.csv")
```

```text
CPU times: user 26.4 s, sys: 24 ms, total: 26.4 s
Wall time: 26.4 s
```

The `create_db()` function creates an SQLite database.

```python
%time db_name = create_db("/data/airline/week11.csv")
```

```text
CPU times: user 2min 17s, sys: 6.87 s, total: 2min 24s
Wall time: 2min 35s
```

The `query()` function uses the databse we have created with `create_db()`
  and makes a `SELECT` query to count the number of cancellations in September.

```python
%time n = query(db_name)
```

```text
CPU times: user 8 ms, sys: 0 ns, total: 8 ms
Wall time: 7.93 ms
```

That's 5.35 thousands of a second per query.
  We can see that it takes a while to create an SQLite database,
  but once it has been created, the query speed is impressive.
  This is a toy problem, but imagine we have to make 1000 such queries.
  It would take 26.4 sec $\times$ 1000 = 7 hours 20 minutes for plain Python,
  while it would take 2 min 17 sec + 7.93 msec $\times$ 1000 =  2 minutes 25
seconds in SQLite.

### Function: create\_db()

First, write a function named `create_db()` that takes a string (the name of the
CSV file)
  and returns a string (the name of the SQLite database name you have created).
  You should also create a [multi-column
index](https://www.sqlite.org/queryplanner.html)
  on **both** the `month` and `cancelled` columns (Name the index `idx`).
  Here's the schema you should use:


    create_flights = '''
    CREATE TABLE flights (
        month INT,
        cancelled INT
    );
    '''

### Function: query()

Next, write a function named `query` that takes a string (the name of the SQL
database you created)
  and an integer (the month of the year to query).
  It should return an integer (the number of cancellations in that month).

Hint: The [multi-column indices](https://www.sqlite.org/queryplanner.html) we
have
  created in `create_cb() is optmized for WHERE ... AND ... terms. Use WHERE ...
AND ...


### Function: count\_cancelled()

Finally, I have provided a function that goes through a CSV file line by line
  and counts the number of cancellations in September.
  You don't have to write this function.

    

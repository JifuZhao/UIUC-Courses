\input{../../cs446.tex}
\usepackage{amsmath,url,graphicx,amssymb}
\usepackage{xcolor}
\sloppy

\oddsidemargin 0in
\evensidemargin 0in
\textwidth 6.5in
\topmargin -0.5in
\textheight 9.0in

\newcommand{\bb}[1]{{\bf #1}}
\newcommand{\tight}[1]{\!#1\!}
\newcommand{\loose}[1]{\;#1\;}

\begin{document}

\assignment{Fall 2015}{7}{November $19^{th}$, $2015$}{December $3^{rd}$, $2015$}

\begin{footnotesize}
  \begin{itemize}
    \item Feel free to talk to other members of the class in doing the homework.
      I am more concerned that you learn how to solve the problem than that you
      demonstrate that you solved it entirely on your own.  You should, however,
      write down your solution yourself.  Please try to keep the solution brief
      and clear.

    \item Please use Piazza first if you have questions about the homework.
      Also feel free to send us e-mails and come to office hours.

    \item Please, no handwritten solutions.  You will submit your solution
      manuscript as a single pdf file.

    \item The homework is due at \textbf{11:59 PM} on the due date. We will be
      using Compass for collecting the homework assignments. Please submit an
      electronic copy via Compass2g (\texttt{http://compass2g.illinois.edu}).
      Please do NOT hand in a hard copy of your write-up.  Contact the TAs if you
      face technical difficulties in submitting the assignment.

    \item \textcolor{red}{You may only use 24 hours of late submission credit hours for this problem set.}

    \item No code is needed for any of these problems. You can do the
      calculations however you please. You need to turn in only the report. Please
      name your report as \texttt{$\langle$NetID$\rangle$-hw7.pdf}.
  \end{itemize}
\end{footnotesize}

\begin{enumerate}
  \item {\bf [EM Algorithm - 70 points]}

    Given a collection of documents $\{d_1, d_2, \dots , d_M\}$ where each 
    document consists of words from vocabulary 
    $\{w_1, w_2, \dots, w_V\}$, we want to cluster this collection into two 
    categories, $c_1$ and $c_2$. 
    In this model, words and documents are observed variables and the category assignment of the
    documents are latent variables. The words of the vocabulary are drawn according
		to some probability distribution given the category, and the categories are drawn
    according to some probability distribution given the document. Thus, the model parameters are:

    \begin{itemize}
      \item $P(d_i)$ is the probability of observing a particular document $d_i$.
      \item $P(c_k | d_i)$ is the probability that the document $d_i$ has 
        category $c_k$.
      \item $P(w_j| c_k)$ is the probability that word $w_j$ appears in the category $c_k$.
    \end{itemize}


    Using these definitions, we can think about the generative process that 
    resulted in the observed collection of documents as follows:

    \begin{enumerate}
      \item[1.] Pick a document $d_i$ with probability $P(d_i)$. We generate 
        $M$ documents.
      \item[2.] For each word position in document $d_i$, pick a category $c_k$ with probability $P(c_k | d_i)$.
      \item[3.] For each word position, we generate a word from 
        $\{w_1, \dots, w_V\}$ based on the category assignment $c_k$. That is, we generate a word $w_j$ according to 
        $P(w_j | c_k)$.
    \end{enumerate}

    One way to estimate the parameters of this model is to use the EM algorithm. We 
    are going to guide you through the steps of the EM algorithm for this model. Please 
    use the notations defined above to answer following questions.

    \begin{enumerate}
      \item {\bf [10 points]}
        What is $P(w_j, d_i)$, the probability of observing the word $w_j$ within document $d_i$ at a particular position? 

      \item  {\bf [10 points]}
        In the E-step, we estimate the posterior distribution of the latent variables given
        the current parameters. Derive $P(c_k | w_j, d_i)$.

      \item {\bf [15 points]}
        In the M-step, we maximize the expected complete data 
        log-likelihood $E[LL]$ of the entire collection of documents. Derive 
        $E[LL]$. (Please use $n(d_i, w_j)$ to denote the number of occurrences 
        of $w_j$ in document $d_i$. Note that it's possible that 
        $n(d_i, w_j)=0$ for some $i$ and $j$.)

      \item {\bf [20 points]}
        Solve the optimization problem you formulated in (c) to derive the update rules for
        $P(d_i)$, $P(c_k | d_i)$ and $P(w_j | c_k)$.

      \item {\bf [10 points]}
        Examine the update rules and explain what they represent in English. 

      \item {\bf [5 points]}
        Describe in pseudocode how you would run the algorithm: initialization, iteration, 
        and termination. 
				%What equations in the previous answers would you use 
        %at which steps in the algorithm?

    \end{enumerate}

    %  \item Instead of estimating variables directly by the EM algorithm, we can use Gibbs 
    %    sampling to estimate the posterior of latent variables. That is, we sample
    %    the latent topic of a specific word in each iteration based on the current 
    %    posterior probability,
    %    \begin{align*}
    %      P(z^{(i)} | z^{(-i)}, w),
    %    \end{align*}
    %    where $z^{(i)}$ is the variable indicates the topic assignment of the word 
    %    at the $i$-th position in the collection. $z^{(-i)}$ represents current topic 
    %    assignments of all words except the $i$-th position, and $w$ is all words 
    %    in the collection. To sample a topic for $z^{(i)}$, we need to calculate 
    %    this probability distribution over all topics. For a specific topic $z_k$,
    %    we know that 
    %    \begin{align}
    %      P(z^{(i)}=z_k | z^{(-i)}, w) \propto P(w^{(i)} | z^{(i)} = z_k, z^{(-i)}, 
    %      w^{(-i)}) P(z^{(i)} = z_k | z^{(-i)}) \label{eq:post}
    %    \end{align}
    %    Note that we can drop all the terms independent of $z^{(i)}$ since they won't
    %    affect the probability distribution. So it ends up with only these two 
    %    relevant terms in equation (\ref{eq:post})
    %    \begin{enumerate}
    %      \item Derive the first term in (\ref{eq:post}), $P(w^{(i)} | z^{(i)} = z_k, z^{(-i)}, 
    %      w^{(-i)})$.
    %      \item Derive the second term in (\ref{eq:post}), $P(z^{(i)} = z_k | z^{(-i)})$.
    %      \item After sampling latent variables several rounds, how could we 
    %        derive the parameters $\theta^{(i)}_k$ and $\phi^{(k)}_j$?
    %    \end{enumerate}



    \item {\bf [Tree Dependent Distributions - 30 points]}

      % Assume an undirected tree $T$ obtained by the algorithm described in class for
      % learning tree dependent distributions.  We would like to show that the step of
      % directing the tree by choosing an arbitrary node is okay.

      %{\bf Note:} In this problem, we will be looking at tree dependent
      %distributions that will be covered in class soon. You may go through the
      %lecture notes or wait for it to be taught in class before you attempt this
      %problem. A brief introduction is given below.

      A tree dependent distribution is a probability distribution over $n$
      variables, $\{x_1,\ldots,x_n\}$ that can be represented as a tree built
      over $n$ nodes corresponding to the variables. If there is a directed edge
      from variable $x_i$ to variable $x_j$, then $x_i$ is said to be the parent
      of $x_j$. Each directed edge $\langle x_i, x_j\rangle$ has a weight that
      indicates the conditional probability $\Pr(x_j \loose{|} x_i)$. In addition,
      we also have probability $\Pr(x_r)$ associated with the root node $x_r$.
      While computing joint probabilities over tree-dependent distributions, we
      assume that a node is independent of all its non-descendants given its
      parent. For instance, in our example above, $x_j$ is independent of all its
      non-descendants given $x_i$.

      To learn a tree-dependent distribution, we need to learn three things: the
      structure of the tree, the conditional probabilities on the edges of the tree, and the
      probabilities on the nodes.  Assume that you have an algorithm to learn an
      {\em undirected} tree $T$ with all required probabilities. To clarify, for
      all {\em undirected} edges $\langle x_i, x_j\rangle$, we have learned both
      probabilities, $\Pr(x_i \loose{|} x_j)$ and $\Pr(x_j \loose{|} x_i)$.
      (There exists such an algorithm and we will be covering that in class.) The
      only aspect missing is the directionality of edges to convert this
      undirected tree to a directed one.

      However, it is okay to not learn the directionality of the edges explicitly. In
      this problem, you will show that choosing any arbitrary node as the root
      and directing all edges away from it is sufficient, and that two directed
      trees obtained this way from the same underlying undirected tree $T$ are
      equivalent.

      \begin{enumerate}
        \item {\bf [10 points]}
          State exactly what is meant by the statement: ``\emph{The two directed trees
          obtained from $T$ are equivalent.}''

        \item {\bf [20 points]}
          Show that no matter which node in $T$ is chosen as the root for the
          ``direction'' stage, the resulting directed trees are all equivalent (based
          on your definition above).

      \end{enumerate}

  \end{enumerate}
  \end{document}

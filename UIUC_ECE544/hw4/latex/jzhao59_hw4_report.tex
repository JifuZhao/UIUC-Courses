\input{setting.tex}

\usepackage{graphicx, amssymb, amsmath, listings, float, mathtools}
\usepackage{color, url}
\lstset{language = Python}
\lstset{breaklines}
\lstset{extendedchars=false}
\usepackage{xcolor}

\oddsidemargin 0in
\evensidemargin 0in
\textwidth 6.5in
\topmargin -0.6in
\textheight 9.0in

\newsavebox\MBox
\newcommand\Cline[2][red]{{\sbox\MBox{$#2$}%
  \rlap{\usebox\MBox}\color{#1}\rule[-2\dp\MBox]{\wd\MBox}{2pt}}}

\begin{document}

\solution{\large Jifu Zhao}{\large 10/30/2016}{\bf \Large ECE 544NA \hspace{0.5cm} 
		Fall 2016 \hspace{0.5cm} Assignment 4}

\section*{\Large I. Pencil-and-Paper}
Suppose $(V, H)$ denote the visible and hidden random variable which takes values ($v \in \{0, 1\}^m$, $h \in \{0, 1\}^n$). And the joint probability is $p(v, h; \theta) = \frac{1}{Z}e^{-E(v, h; \theta)}$, where E is the energy function:
\begin{equation}
\begin{split}
E(v, h; \theta) & = -\sum_{i=1}^n \sum_{j=1}^m w_{ij} h_i v_j - \sum_{j=1}^m b_j v_j - \sum_{i=1}^n c_i h_i \\
				& = -(\mathbf{v^T W h + v^T b + h^T c})
\end{split}
\end{equation}
where $Z = \sum_v \sum_h e^{-E(v, h, \theta)}$ and $\theta = (W, b, c)$.


\begin{description}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 1
\item{\bf \large 1. } Find $p(v|h, \theta)$ and $\mathbb{E}(v|h, \theta)$.

From the structure of RBM, it means that the hidden variables are independent given the state of the visible variables and the visible variables are independent given the state of the hidden variables. In this way,
\begin{equation}
\begin{split}
\Cline[red]{p(v| h, \theta)} 
	& = \frac{p(v, h | \theta)}{p(h| \theta)} \\
	& = \frac{\prod_i^n \prod_j^m p(v_j, h_i|\theta)}{\sum_{v} \prod_i^n \prod_j^m p(v_j, h_i|\theta)}
\end{split}
\end{equation}

So, we also have:
\begin{equation}
\mathbb{E}(v|h, \theta) =  sigmoid(W^T h + b)
\end{equation}
(The prof will be shown later)

For $p(v_j|h)$, 
\begin{equation}
p(v_j|h) = \frac{p(v_j, h)}{p(h)} = \frac{p(v_j, h)}{p(v_j=0, h) + p(v_j=1, h)}
\end{equation}

Since that $p(v_j=0, h) = \frac{1}{Z}e^{-E(v_j=0, h; \theta)}$ and $p(v_j=1, h) = \frac{1}{Z}e^{-E(v_j=1, h; \theta)}$. From equation (1), we can think $E(v, h; \theta) = v_j \alpha_j + \beta$, where $\beta$ doesn't contain $v_j$. 

In this way, we have:
\begin{equation}
\begin{split}
p(v_j|h) & = \frac{p(v_j, h)}{p(v_j=0, h) + p(v_j=1, h)} \\
		 & = \frac{exp(-v_j \alpha_j - \beta)}{exp(-\beta) + exp(-\alpha_j - \beta)} \\
		 & = \frac{exp(-v_j \alpha_j)}{1 + exp(-\alpha_j)}
\end{split} 
\end{equation}

So, we have
\begin{equation}
p(v_j = 1|h) = \frac{exp(-\alpha_j)}{1 + exp(-\alpha_j)} = \frac{1}{1 + exp(\alpha_j)}
\end{equation}
and 
\begin{equation}
p(v_j = 0|h) = \frac{1}{1 + exp(-\alpha_j)} = 1 - \frac{1}{1 + exp(\alpha_j)}
\end{equation}

From equation (1), we can see that $\alpha_j = -\sum_{i=1}^n w_{ij}h_i - b_j$, so we can see that:
\begin{equation}
\Cline[red]{p(v_j = 1|h) = sigmoid(-\alpha_j) = sigmoid(\sum_{i=1}^n w_{ij}h_i + b_j)}
\end{equation}
and 
\begin{equation}
\Cline[red]{p(v_j = 0|h) = 1 - sigmoid(-\alpha_j) = 1 - sigmoid(\sum_{i=1}^n w_{ij}h_i + b_j)}
\end{equation}

From equation (8) and (9), we can see that:
\begin{equation}
\mathbb{E}(v_j|h, \theta) = 1 \cdot p(v_j = 1|h, \theta) + 0 \cdot p(v_j = 0|h, \theta) = sigmoid(\sum_{i=1}^n w_{ij}h_i + b_j)
\end{equation}
In the vector format:
\begin{equation}
\Cline[red]{\mathbb{E}(v|h, \theta) = sigmoid(W^T h + b)}
\end{equation}\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 2
\item{\bf \large 2. } Find $p(h|v, \theta)$ and $\mathbb{E}(h|v, \theta)$.

Similar to the procedures in 1, since that the hidden variables are independent given the state of the visible variables. In this way,
\begin{equation}
\begin{split}
\Cline[red]{p(h| v, \theta)} 
	& = \frac{p(v, h | \theta)}{p(v| \theta)} \\
	& = \frac{\prod_i^n \prod_j^m p(v_j, h_i|\theta)}{\sum_{h} \prod_i^n \prod_j^m p(v_j, h_i|\theta)}
\end{split}
\end{equation}

So, we also have:
\begin{equation}
\mathbb{E}(h|v, \theta) =  sigmoid(W v + c)
\end{equation}

For $p(h_i|v)$, 
\begin{equation}
p(h_i|v) = \frac{p(h_i, v)}{p(v)} = \frac{p(h_i, v)}{p(h_i=0, v) + p(h_i=1, v)}
\end{equation}

Since that $p(h_i=0, v) = \frac{1}{Z}e^{-E(v, h_i=0; \theta)}$ and $p(h_i=1, v) = \frac{1}{Z}e^{-E(v, h_i=1; \theta)}$.
From equation (1), we can think $E(v, h; \theta) = h_i \gamma_i + \beta'$, where $\beta'$ doesn't contain $h_i$. In this way, we have:
\begin{equation}
\begin{split}
p(h_i|v) & = \frac{p(h_i, v)}{p(h_i=0, v) + p(h_i=1, v)} \\
		 & = \frac{exp(-h_i \gamma_i - \beta')}{exp(-\beta') + exp(-\gamma_i - \beta')} \\
		 & = \frac{exp(-h_i \gamma_i)}{1 + exp(-\gamma_i)}
\end{split} 
\end{equation}

So, we have
\begin{equation}
p(h_i=1|v) = \frac{exp(-\gamma_i)}{1 + exp(-\gamma_i)} = \frac{1}{1 + exp(\gamma_i)}
\end{equation}
and 
\begin{equation}
p(h_i=0|v) = \frac{1}{1 + exp(-\gamma_i)} = 1 - \frac{1}{1 + exp(\gamma_i)}
\end{equation}

From equation (1), we can see that $\gamma_i = -\sum_{j=1}^m w_{ij}v_j - c_i$, so we can see that:
\begin{equation}
\Cline[red]{p(h_i=1|v) = sigmoid(-\gamma_i) = sigmoid(\sum_{j=1}^m w_{ij}v_j + c_i)}
\end{equation}
and 
\begin{equation}
\Cline[red]{p(h_i=0|v) = 1 - sigmoid(-\gamma_i) = 1 - sigmoid(\sum_{j=1}^m w_{ij}v_j + c_i)}
\end{equation}

From equation (18) and (19), we can see that:
\begin{equation}
\mathbb{E}(h_i|v, \theta) = 1 \cdot p(h_i=1|v, \theta) + 0 \cdot p(h_i=0|v, \theta) = sigmoid(\sum_{j=1}^m w_{ij}v_j + c_i)
\end{equation}
In the vector format:
\begin{equation}
\Cline[red]{\mathbb{E}(h|v, \theta) = sigmoid(W v + c)}
\end{equation}\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 3
\item{\bf \large 3. } Compute $\frac{\partial \mathcal{L}(D|\theta)}{\partial W_{ij}}$, $\frac{\partial \mathcal{L}(D|\theta)}{\partial b_{j}}$ and $\frac{\partial \mathcal{L}(D|\theta)}{\partial c_{i}}$

Now, suppose the given dataset is $D=\{v_1, v_2, \cdots, v_N\}$, then the log-likelihood can be calculated as:
\begin{equation}
\mathcal{L}(D|\theta) = \sum_{t=1}^N log(p(v_t|\theta))
\end{equation}

Since that 
\begin{equation}
p(v_t|\theta) = \sum_{h_t} p(v_t, h_t|\theta) = \frac{1}{Z}\sum_{h_t} exp(-E(v_t, h_t; \theta))
\end{equation}
where $Z = \sum_{v_t}\sum_{h_t}exp(-E(v_t, h_t; \theta))$

we can get:
\begin{equation}
\begin{split}
log(p(v_t|\theta)) & = log(\frac{1}{Z}\sum_{h_t} exp(-E(v_t, h_t; \theta))) \\
				   & = log(\sum_{h_t}exp(-E(v_t, h_t; \theta))) - log(\sum_{v_t}\sum_{h_t}exp(-E(v_t, h_t; \theta))) 
\end{split}
\end{equation}

So, to calculate the derivation, we can get:
\begin{equation}
\begin{split}
\frac{\partial log(p(v_t|\theta))}{\partial \theta} 
	& = \frac{\partial}{\partial \theta} log(\sum_{h_t}exp(-E(v_t, h_t; \theta))) - \frac{\partial}{\partial \theta} log(\sum_{v_t}\sum_{h_t}exp(-E(v_t, h_t; \theta)))  \\
	& = -\frac{1}{\sum_{h_t} e^{-E}} \sum_{h_t} e^{-E} \cdot \frac{\partial E}{\partial \theta} + 
		 \frac{1}{\sum_{v_t}\sum_{h_t} e^{-E}} \sum_{v_t}\sum_{h_t} e^{-E} \cdot \frac{\partial E}{\partial \theta} \\
	& = -\frac{\sum_{h_t} e^{-E} \cdot \frac{\partial E}{\partial \theta}}{Z \cdot p(v_t|\theta)}
	    + \sum_{v_t}\sum_{h_t} \frac{e^{-E}}{Z} \cdot \frac{\partial E}{\partial \theta} \\
	& = -\sum_{h_t} p(h_t|v_t, \theta) \cdot \frac{\partial E}{\partial \theta}
	    +\sum_{v_t}\sum_{h_t} p(v_t, h_t|\theta) \cdot \frac{\partial E}{\partial \theta} \\
	& = \Cline[red]{-\sum_{h} p(h|v) \cdot \frac{\partial E(v, h)}{\partial \theta}
	    +\sum_{v}\sum_{h} p(v, h) \cdot \frac{\partial E(v, h)}{\partial \theta}}
\end{split}
\end{equation}

From equation (25), we can get:
\begin{equation}
\begin{split}
\frac{\partial \mathcal{L}(D|\theta)}{\partial W_{ij}} 
	& = -\sum_{h} p(h|v) \cdot \frac{\partial E(v, h)}{\partial w_{ij}}
	    +\sum_{v}\sum_{h} p(v, h) \cdot \frac{\partial E(v, h)}{\partial w_{ij}} \\
	& = \sum_{h} h_i v_j \cdot p(h|v) - \sum_h \sum_v h_i v_j \cdot p(v, h) \\
	& = \Cline[red]{p(h_i=1|v) \cdot v_j - \sum_v p(v) \cdot p(h_i=1|v) \cdot v_j} \\
	& = \Cline[red]{p(h_i=1|v) \cdot v_j - \mathbf{E}[v_j h_i]} 
\end{split}
\end{equation}

In the similar way, we can have:
\begin{equation}
\begin{split}
\frac{\partial \mathcal{L}(D|\theta)}{\partial b_{j}} 
	& = \sum_{h} v_j \cdot p(h|v) - \sum_h \sum_v v_j \cdot p(v, h) \\
	& = \Cline[red]{v_j - \sum_v p(v) \cdot v_j} \\
	& = \Cline[red]{v_j - \mathbf{E}[v_j]}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\frac{\partial \mathcal{L}(D|\theta)}{\partial c_{i}} 
	& = \sum_{h} h_i \cdot p(h|v) - \sum_h \sum_v h_i \cdot p(v, h) \\
	& = \Cline[red]{p(h_i=1|v) - \sum_v p(v) \cdot p(h_i=1|v)} \\
	& = \Cline[red]{p(h_i=1|v) - \mathbf{E}[h_i]}
\end{split}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problem 4
\item{\bf \large 4. } Contrastive divergence

From equation (26), (27) and (28), we can find that $p(v, h)$ is actually a computationally intractable term. To solve this problem, we should consider 1-step contrastive divergence to solve this problem.

Considering Hinton approximation:
\begin{equation}
\mathbf{E}[v_j h_i] \simeq \mathbf{E}[v_j|h] \mathbf{E}[h_i|v]
\end{equation}

And the k-step contrastive divergence is:
\begin{equation}
CD_k(\theta, v^{(0)}) = -\sum_h p(h|v^{(0)})\frac{\partial E(^{(0)}, h)}{\partial \theta}
					+\sum_h p(h|v^{(k)})\frac{\partial E(v^{(k)}, h)}{\partial \theta}
\end{equation}


First, choose one visible data $v^{(0)}$ from the given training set, sample hidden nodes $h^{(0)}$ according to $p(h|v^{(0)})$. Then, calculate the probability $p(v|h^{(0)})$ using the sampled hidden nodes, and then reconstruct the visible nodes $v^{(1)}$ according to $p(v|h^{(0)})$. Finally, calculate the probability of $p(h^{(1)}|v^{(1)})$. Using these result to approximate and calculate the update rules for $W$, $b$ and $c$.

{\bf Note:} The above method is for 1-step CD sampling, if needed, this procedure can be repeated k times.

\end{description}

\newpage
\section*{\Large II. Code-from-Scratch}

\subsection*{\large 1. Methods}

\subsection*{\large 2. Results}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/filter.pdf}\
\caption{\label{fig:single_filter} Learned Filter}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/filter_batch.pdf}\
\caption{\label{fig:batch_filter} Learned Filter}
\end{figure}

\newpage
\section*{\Large III. TensorFlow}

\subsection*{\large 1. Methods}

\subsection*{\large 2. Results}


\begin{table}[H]
	\centering
	\caption{Training and testing accuracy for different models}
	\label{table:BESTacc}	
	\begin{tabular}{c | c | c}
		\hline \hline
		Methods  	&	Training Accuracy 	&	Testing Accuracy \\[0.1cm]
		\hline
		Raw Image	&	92.99\%				& 	92.59\%			 \\[0.1cm]
		RBM			&	95.15\%				& 	95.27\%			 \\[0.1cm]
		PCA			& 	92.34\%				& 	92.04\%			 \\[0.1cm]
		Stacked RBM & 	96.35\%				& 	96.24\%			 \\[0.1cm]
		\hline	
	\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/train_raw.pdf}\
\caption{\label{fig:train_raw} Training Set Confusion Matrix Using Raw Image}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/test_raw.pdf}\
\caption{\label{fig:test_raw} Test Set Confusion Matrix Using Raw Image}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/train_rbm.pdf}\
\caption{\label{fig:train_rbm} Training Set Confusion Matrix Using RBM}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/test_rbm.pdf}\
\caption{\label{fig:test_rbm} Test Set Confusion Matrix Using RBM}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/train_pca.pdf}\
\caption{\label{fig:train_pca} Training Set Confusion Matrix Using PCA}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/test_pca.pdf}\
\caption{\label{fig:test_pca} Test Set Confusion Matrix Using PCA}
\end{figure}


\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/train_stacking.pdf}\
\caption{\label{fig:train_stack} Training Set Confusion Matrix Using Stacked RBM}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{./figures/test_stacking.pdf}\
\caption{\label{fig:test_stack} Test Set Confusion Matrix Using Stacked RBM}
\end{figure}


\clearpage
%\bibliographystyle{plain}
%\bibliographystyle{unsrt}
%\bibliography{reference.bib}

\end{document}


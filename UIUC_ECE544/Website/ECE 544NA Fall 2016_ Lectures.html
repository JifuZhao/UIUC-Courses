<!DOCTYPE html>
<html>
  <head>
    <title>ECE 544NA Fall 2016: Lectures</title>
    <link rel="stylesheet" type="text/css" href="main.css" />
  </head>
  <body>
    <div id="banner">
      <a href="http://ece.illinois.edu"><img alt="University of Illinois Logo" src="imark_bold.gif" /></a>
      <h1>ECE 544NA Fall 2016: Lectures</h1>
    </div>
    <div id="columns">
      <div id="side">
	<ul>
	  <li><a href="index.html">Home</a></li>
	  <li><a href="syllabus.html">Syllabus</a></li>
	  <li><a href="lectures.html">Lectures</a></li>
	  <li><a href="homework.html">Homework</a></li>
	  <li><a href="exams.html">Exams</a></li>
	  <li><a href="project.html">Project</a></li>
	</ul>
      </div>
      <div id="main">
	<p>Almost every lecture will be accompanied by an article.
	  Exams will contain two parts, with roughly half the points
	  for each part: (1) short-answer or multiple-choice questions
	  about the assigned articles, and (2) long-answer questions
	  based on the homework.</p>

	<p>Supplementary readings are provided for your information only;
	  exam questions will not be drawn from those.</p>
	
	<ol>
	  <li><b>Section 1: Supervised Learning</b>
	    <ul>
	      <li><b>August 23:</b> SVD, pseudo-inverse, linear
		regression.  M. Planitz,
		<a href="http://www.jstor.org/stable/3617890">Inconsistent
		  Systems of Linear Equations,</a> the Mathematical
		Gazette 63(425):181-185, 1979.<ul>
		  <li>Supplementary
		  reading: Christopher
		  Bishop, <a href="http://cs.du.edu/~mitchell/mario_books/Neural_Networks_for_Pattern_Recognition_-_Christopher_Bishop.pdf">Neural
		  Networks for Pattern Recognition</a></li></ul>
	      </li>
	      
	      <li><b>August 25:</b> logistic regression. Frank
		Baker, <a href="http://info.worldbank.org/etools/docs/library/117765/Item%20Response%20Theory%20-%20F%20Baker.pdf">The
		Basics of Item Response Theory, second edition, chapter 3.</a>
		Publisher: ERIC Clearinghouse on Assessment and
		Evaluation.<ul><li>
		Supplementary
		  reading: Christopher
		  Bishop, <a href="http://cs.du.edu/~mitchell/mario_books/Neural_Networks_for_Pattern_Recognition_-_Christopher_Bishop.pdf">Neural
		  Networks for Pattern Recognition</a></li></ul>
	      </li>

	      <li><b>August 30:</b> no lecture.

	      <li><b>September 1:</b> perceptron. Mehryar Mohri and
		Afshin
		Rostamizadeh, <a href="https://arxiv.org/pdf/1305.0208">Perceptron
		  Mistake Bounds,</a> Arxiv 1305.0208, 2013 (only sections 1-2 required)
		<ul><li>Supplementary
		  reading: Christopher
		  Bishop, <a href="http://cs.du.edu/~mitchell/mario_books/Neural_Networks_for_Pattern_Recognition_-_Christopher_Bishop.pdf">Neural
		  Networks for Pattern Recognition</a></li></ul>
	      </li>

	      <li><b>September 6:</b> support vector machine.
		C.J.C. Burges, <a href="https://www.microsoft.com/en-us/research/publication/a-tutorial-on-support-vector-machines-for-pattern-recognition/">
		A Tutorial on Support Vector Machines for Pattern
		Recognition</a>, Knowledge Discovery and Data Mining
		2(2), 1998 (only sections 3.1 and 3.5 required)
	      </li>

	      <li><b>September 8:</b> logistic regression, multivariate logistic regression <a href='guest_lecture1.pdf'> [lecture slides]</a>
	      </li>
	      <li><b>September 13:</b> principal component analysis, python + numpy + TensorFlow tutorial
		<a href='guest_lecture2.pdf'> [lecture slides]</a>
		<a href='python_numpy_tensorflow_tutorial.ipynb'> [ipython notebook]</a>
		<a href='python_numpy_tensorflow_tutorial.pdf'> [ipython pdf]</a>
	      </li>
	      <li><b>September 15:</b> No lecture</li>



	      <li><b>September 20:</b> back-propagation.  David
	      E. Rumelhart, Geoffrey E. Hinton, and Ronald
	      J. Williams, <a href="http://www.nature.com/nature/journal/v323/n6088/abs/323533a0.html">
	      Learning representations by back-propagating errors,</a>
		Nature 323:533-536, 1986.
		<ul><li>Supplementary
		reading: JSALT Slides on Neural Networks,
      <li><a href="http://isle.illinois.edu/sst/courses/minicourse/2015/ws15ann.pdf">Automatic
		Neural Networks (ANN)</a></li><li> Christopher
		Bishop, <a href="http://cs.du.edu/~mitchell/mario_books/Neural_Networks_for_Pattern_Recognition_-_Christopher_Bishop.pdf">Neural
		Networks for Pattern Recognition</a> (Chapter 4)</li></ul>
	      </li>
		
	      <li><b>September 22:</b> convolutional neural networks.  Yoshua
		Bengio and Yann LeCun,
		<a href="http://yann.lecun.com/exdb/publis/pdf/bengio-lecun-07.pdf">Scaling
		  Learning Algorithms Towards AI,</a> in Large-Scale
		  Kernel Machines, L. Bottou, O. Chapelle, D. Decoste
		and J. Weston, Eds., 2007 (only section 6.2 required)</li>

	      <li><b>September 27:</b> Time-delay neural networks.
		Alexander Waibel, Toshiyuki Hanazawa, Geoffrey Hinton,
		Kiyohiro Shikano and Kevin
		J. Lang, <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=21701">Phoneme
		Recognition Using Time-Delay Neural Networks,</a> IEEE
		Trans. ASSP 37:328-339, 1989 (only section II required)
		<ul>
		  <li>Supplementary
		    reading: <a href="544na_cnn_lecture.pdf">CNN
		   Lecture Slides</a></li>
		</ul>
	      </li>

	      <li><b>September 29:</b> Exam 1 review</li>
	      <li><b>October 4:</b> Exam 1</li>
	    </ul>
	  </li>
	  <li><b>Topic 2: Unsupervised Learning</b>
	    <ul>
	      <li><b>October 6:</b> Expectation maximization.
		A.P. Dempster, N.M. Laird, and D.B. Rubin,
		<a href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.133.4884">
		Maximum likelihood from incomplete data via the EM
		algorithm,</a> Journal of the Royal Statistical
		Society Series B 39(1):1-38, 1977</li>

	      <li><b>October 11:</b> Gaussian mixture models. Jeff Bilmes,
		<a href="http://ssli.ee.washington.edu/people/bilmes/mypapers/em.pdf">
		  A gentle tutorial of the EM algorithm and its
		  application to parameter estimation for Gaussian mixture
		  and hidden Markov models,</a> International Computer
		Sciences Institute TR-97-021, 1997</li>

	      <li><b>October 13:</b> Multivariate Gaussians.  Sam
		Roweis, <a href="https://www.cs.nyu.edu/~roweis/papers/empca.pdf">EM
		Algorithms for PCA and SPCA</a>, Cal Tech. Research
		Note, 1999 (required: section 2, definition of
		PCA)</li>

	      <li><b>October 17:</b> GMMs again.</li>
	      
	      <li><b>October 20:</b> restricted Boltzmann
		machines. Smolensky, <a href="http://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2015/pdfs/Smolensky.1986.pdf">Harmony
		Theory</a>, pages 213-220.  (Pages 213-220 talk about
		what the variables mean.  These pages are really
		useful so that you can understand WHY we use RBMs.
		Pages 220-232 describe HOW we use RBMs, specifically,
		how you compute the hidden vector from the observed
		vector: log probability on p. 221, joint probability
		on p. 228, posterior probability on pp. 231-2).</li>
	      
	      <li><b>October 25:</b> Contrastive Divergence.  Geoffrey
		  E. Hinton, <a href="http://www.cs.toronto.edu/~fritz/absps/tr00-004.pdf">Training
		  products of experts by minimizing contrastive
		  divergence,</a> Neural computation 14:(8):1771-1800,
		  2002 (required: section 2, section 7).
	      </li>

	      <li><b>October 27:</b> Parzen windows.  Emanuel Parzen,
		<a href="http://projecteuclid.org/euclid.aoms/1177704472">On
		  Estimation of a Probability Density Function
			 and Mode,</a> Annals of Mathematical
			 Statistics 33(3):1065-1076, 1962.</li>

	    </ul>
	  </li>
	  <li><b>Topic 3: Time</b>
	    <ul>	
	      <li><b>November 1:</b> back-propagation through time.
	      Paul Werbos, <a href="http://werbos.com/Neural/BTT.pdf">
	      Back-propagation through time: What it is and why we do
	      it</a>, Proceedings of the IEEE 78(10):1550-1560,
	      1990.</li>

	      <li><b>November 3:</b> LSTM (long short-term memory
		network).  Sepp Hochreiter and Jurgen Schmidhuber,
		<a href="http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf">
		  Long Short-Term Memory.</a>  Neural Computation
		9(8):1735-1780, 1997.</li>

	      <li><b>November 8:</b> hidden Markov models.  Lawrence
	      R. Rabiner, <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=18626">
	      A Tutorial on Hidden Markov Models and Selected
	      Applications in Speech Recognition,</a> Proc. IEEE
	      77(2):257-286</li>

	      <li><b>November 10:</b>  Hybrid NN-HMM.  Yoshua Bengio, Renato
		de Mori, Giovanni Flammia and Ralf Kompe,
		<a href="bengio92a.pdf">Global Optimization of a
		  Neural Network-Hidden Markov Model Hybrid,</a> 1992.
		  Hybrid LSTM-HMM. Alex Graves, Navdeep Jaitly and
		  Abdel-rahman Mohamed,
		<a href="http://www.cs.toronto.edu/~graves/asru_2013.pdf">
		  Hybrid Speech Recognition with Deep Bidirectional LSTM</a>,
		Proc. IEEE ASRU, 2013</li>

	      <li><b>November 15:</b> Lecture canceled.</li>

	      <li><b>November 17:</b> generative adversarial networks.
		Raymond Yeh, Chen Chen, Teck Yian Lim, Mark
		Hasegawa-Johnson and Minh N. Do,
		<a href="https://arxiv.org/abs/1607.07539">
						    Semantic Image
		Inpainting with Perceptual and Contextual Losses,</a>
		ArXiv 26 Jul 2016</li>

	      <li><b>December 6:</b> finite state transducers. Mehryar
		Mohri, Fernando Pereira and Michael
		Riley, <a href="http://repository.upenn.edu/cis_papers/11/">
		  Weighted finite-state transducers in speech recognition,</a>
		Computer Speech and Language 16:69-88, 2002</li>

	      <li><b>December 8:</b> Exam 3 review.</li>
	      <li><b>Final exam week:</b> Exam 3</li>
	    </ul>
	  </li>
	</ol>

	<h2>Extra Readings</h2>
	<p>Here are some papers that are not critical to the course
	material (not covered in any quiz) but might be of interest to
	  some of you.</p>
	<ul>
	  <li> self-training. H.J. Scudder,
	    <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1053799">
	      Probability of Error of Some Adaptive
	      Pattern-Recognition Machines</a>, IEEE Trans. Information
	    Theory 11:363-371, 1965</li>

	  <li>G. Hinton, L. Deng, D. Yu, G.E. Dahl, A.R. Mohamed,
	    N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
	    T.N. Sainath and B. Kingsbury, "Deep neural networks for
	    acoustic modeling in speech recognition: The shared views
	    of four research groups," IEEE Signal Processing Magazine
	    29(6):82-97, 2012</li>

	  <li>simulated annealing. Bruce Hajek,
	    <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4048399">A
	    Tutorial Survey of Theory and Applications of Simulated
	    Annealing,</a> Proc. 24th Conference on Decision and
	    Control, 755-760, 1985</li>

	  <li>higher-order learning.  Diederik P. Kingma and Jimmy
	  P. Ba, <a href="https://arxiv.org/pdf/1412.6980v8.pdf">Adam:
	  A method for stochastic optimization</a>, Arxiv 1402.6980,
	  2014</li>

	  <li>transfer learning.  Y. Bengio, "Deep learning of
	    representations for unsupervised and transfer learning,"
	    JMLR: Proceedings of Unsupervised and Transfer Learning
	    Challenge and Workshop, pp. 17-36, 2012</li>

	  <li>multi-task learning. R. Caruana, "Multitask learning",
	    Machine Learning 28(1):41-75, 1997</li>

	  <li>Hybrid neural net-HMMs. Nelson Morgan and Herve
	    Bourlard, <a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=381844">
	    Neural networks for statistical recognition of continuous
	    speech,</a> Proceedings of the IEEE 83(5):742-770,
	    1995.</li>

	  <li>Spiking neurons.  Fred Rieke,
	    D. Warland, and
	    W. Bialek, <a href="http://iopscience.iop.org/article/10.1209/0295-5075/22/2/013/meta">Coding
	      efficiency and information rates in sensory neurons,</a>
	    Europhys. Lett. 22:151-6, 1993.</li>

	  <li>Polychronization.  Eugene
	    M. Izhikevich, <a href="http://www.ncbi.nlm.nih.gov/pubmed/16378515">Polychronization:
	    Computation with Spikes.</a>  Neural Compuutation
	    18:245-282, 2006.</li>

	  <li>encoder-decoder networks.  William Chan, Navdeep Jaitly,
		Quoc V. Le, and Oriol Vinyals,
		<a href="https://arxiv.org/abs/1508.01211"> Listen,
		  Attend and Spell,</a> arXiv:1508.01211, 2015.</li>

	</ul>
	
	  
      </div>
    </div>
  </body>
</html>

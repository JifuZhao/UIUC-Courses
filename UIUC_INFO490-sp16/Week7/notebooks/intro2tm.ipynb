{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Text Mining\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we explore more advanced machine learning\n",
    "techniques with text data. First, we introduce the concept of n-grams,\n",
    "which are combinations of one or more tokens. For example, bigrams are\n",
    "combinations of two tokens, while trigrams are combinations of three\n",
    "tokens. Next, we introduce the concept of stemming, which is used to\n",
    "convert tokens into their root forms so that token frequencies match the\n",
    "use of the root token rather than being spread across multiple similar\n",
    "tokens. Finally, we explore the application of clustering and feature\n",
    "selection to text data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/opt/conda/lib/python3.5/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "# Set up Notebook\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "# Standard imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "Formally, a [_n-gram_][ngd] is a contiguous sequence of **n** items from a\n",
    "parent sequence of items, such as characters or words in a text\n",
    "document. In general, we will focus solely on words in a document. Thus,\n",
    "our initial approach has simply been to look at unigrams or single\n",
    "words in a document when building a classification model. However,\n",
    "sometimes the combination of words can be more descriptive, for example,\n",
    "_unbelievably bad_ is generally viewed as a more powerful description\n",
    "than just _bad_. As a result, the concept of an _n-gram_ was created,\n",
    "where collections of words can be treated as features. In fact google\n",
    "allows a user to search for [specific n-gram][gnv] combinations in books that\n",
    "they have scanned.\n",
    "\n",
    "While this clearly can improve classification power, it also increases\n",
    "computational requirements. This is a result of the exponential rise in\n",
    "the number of possible features. For example, given $n$ words, we have\n",
    "$n \\times (n - 1)$ possible bigrams, and so on for higher order\n",
    "combinations. While this is not a problem for small vocabularies, for\n",
    "larger vocabularies (and corresponding documents) the number of possible\n",
    "features can quickly become very large. Thus, many text mining\n",
    "applications will make use of Hadoop or Spark clusters to leverage the\n",
    "inherent parallelism in these tasks.\n",
    "\n",
    "To demonstrate using n-grams, we first demonstrate the concept on a\n",
    "single sentence.\n",
    "\n",
    "-----\n",
    "[gnv]: https://books.google.com/ngrams\n",
    "[ngd]: https://en.wikipedia.org/wiki/N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'info490', 'introduces', 'many', 'concepts', 'in', 'data', 'science',\n",
      "  'info490 introduces', 'introduces many', 'many concepts', 'concepts in',\n",
      "  'in data', 'data science', 'info490 introduces many',\n",
      "  'introduces many concepts', 'many concepts in', 'concepts in data',\n",
      "  'in data science']\n"
     ]
    }
   ],
   "source": [
    "my_text = 'INFO490 introduces many concepts in data science.'\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "tk_func = cv.build_analyzer()\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "pp.pprint(tk_func(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token mapping:\n",
      "----------------------------------------\n",
      "0 concepts\n",
      "1 concepts in\n",
      "2 concepts in data\n",
      "3 data\n",
      "4 data science\n",
      "5 in\n",
      "6 in data\n",
      "7 in data science\n",
      "8 info490\n",
      "9 info490 introduces\n",
      "10 info490 introduces many\n",
      "11 introduces\n",
      "12 introduces many\n",
      "13 introduces many concepts\n",
      "14 many\n",
      "15 many concepts\n",
      "16 many concepts in\n",
      "17 science\n",
      "----------------------------------------\n",
      "['INFO490 is data science']\n",
      "----------------------------------------\n",
      "[[0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "in_list = []\n",
    "in_list.append(my_text)\n",
    "\n",
    "cv = cv.fit(in_list)\n",
    "\n",
    "import operator\n",
    "my_voc = sorted(cv.vocabulary_.items(), key=operator.itemgetter(1))\n",
    "\n",
    "print('Token mapping:')\n",
    "print(40*'-')\n",
    "\n",
    "for tokens, rank in my_voc:\n",
    "    print(rank, tokens)\n",
    "\n",
    "print(40*'-')\n",
    "out_list = ['INFO490 is data science']\n",
    "xsm = cv.transform(out_list)\n",
    "print(out_list)\n",
    "print(40*'-')\n",
    "print(xsm.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we used `CountVectorizer` to create n-gram\n",
    "tokens. Now that you have run the Notebook, go back and make the\n",
    "following changes to see how the results change.\n",
    "\n",
    "1. Change the `CountVectorizer` to use stop words and change the tokens\n",
    "to all lowercase. How does this change the tokens and mappings? \n",
    "2. Create your own sentence, do the tokens and n-grams make sense?\n",
    "3. Try changing the ngram range to different values (e.g.,\n",
    "`ngram_range=(1,4)` or `ngram_range=(2,3)`). Notice how the number of\n",
    "tokens quickly increase.\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "## N-gram Classification\n",
    "\n",
    "Having n-grams offers improved classification, since word or token\n",
    "combinations often include more information than single words or tokens.\n",
    "For example, _University Illinois_ means more than just _University_ and\n",
    "_Illinois_. We can build on our previous simple text classification\n",
    "pipeline to now develop a more complete code example that builds a\n",
    "feature vector containing both single words and b-grams from the\n",
    "documents. We use this new sparse matrix to classify the documents by\n",
    "using our simple Naive Bayes classifier, which obtains slightly better\n",
    "results. First, we load the movie review data.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/data_scientist/data/nltk_data/corpora/movie_reviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0c4d302a66c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#data_dir = '/home/data_scientist/nltk_data/corpora/movie_reviews'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmvr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Number of Reviews: {0}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmvr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.5/site-packages/sklearn/datasets/base.py\u001b[0m in \u001b[0;36mload_files\u001b[1;34m(container_path, description, categories, load_content, shuffle, encoding, decode_error, random_state)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[0mfilenames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     folders = [f for f in sorted(listdir(container_path))\n\u001b[0m\u001b[0;32m    199\u001b[0m                if isdir(join(container_path, f))]\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/data_scientist/data/nltk_data/corpora/movie_reviews'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "mvr = nltk.corpus.movie_reviews\n",
    "\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "data_dir = '/home/data_scientist/data/nltk_data/corpora/movie_reviews'\n",
    "\n",
    "\n",
    "#data_dir = '/home/data_scientist/nltk_data/corpora/movie_reviews'\n",
    "mvr = load_files(data_dir, shuffle = False)\n",
    "print('Number of Reviews: {0}'.format(len(mvr.data)))\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "mvr_train, mvr_test, y_train, y_test = train_test_split(\n",
    "    mvr.data, mvr.target, test_size=0.25, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mvr_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-02ecdb71d006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mpclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv__stop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'english'\u001b[0m\u001b[1;33m,\u001b[0m                 \u001b[0mcv__ngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m                 \u001b[0mcv__lowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mpclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmvr_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmvr_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmvr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mvr_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "tools = [('cv', CountVectorizer()), ('nb', MultinomialNB())]\n",
    "pclf = Pipeline(tools)\n",
    "\n",
    "\n",
    "# Lowercase and restrict ourselves to about half the available features\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,2), \\\n",
    "                cv__lowercase=True)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 421010\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Now we can repeat the results, but now use unigrams, bigrams, and\n",
    "trigrams. Since this will produce a document term matrix that likely\n",
    "exceeds the computational resources of our Docker containers, we impose\n",
    "two cuts on the tokens included in our DTM. First, we impose a minimum\n",
    "feature term that requires a term to be present in at least two\n",
    "documents. Second, we set a maximum frequency of 50%, such that any term\n",
    "occurring in more than fifty percent of all documents will be ignored\n",
    "(these are likely all stop words, but this value can be reduced to a\n",
    "lower value to trim frequently occurring words that add little to the\n",
    "accuracy. Notice that our overall number of features is much smaller,\n",
    "even though we are also using trigrams in this classification pipeline.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.84      0.78      0.81       259\n",
      "        pos       0.78      0.83      0.81       241\n",
      "\n",
      "avg / total       0.81      0.81      0.81       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,3), \\\n",
    "                cv__lowercase=True, \\\n",
    "                cv__min_df=2, \\\n",
    "                cv__max_df=0.5)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 62735\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we included n-grams in the classification\n",
    "process by using a `CountVectorizer`.  Now that you have run the\n",
    "Notebook, go back and make the following changes to see how the results\n",
    "change.\n",
    "\n",
    "1. Try to determine a value of `max_df` that replicates the effects of\n",
    "using stop words. \n",
    "2. Change `CountVectorizer` to `TfidfVectorizer`. Do the results change?\n",
    "3. Try using a more powerful classification algorithm, as opposed to\n",
    "`MultinomialNB`. Do the results change?\n",
    "\n",
    "-----\n",
    "\n",
    "## Stemming\n",
    "\n",
    "So far, we have looked at several techniques to remove redundant or\n",
    "unimportant features. For example, we changed the case of all text to\n",
    "lowercase and we have applied stop words. However, there still is the\n",
    "issue of different forms of the same word, for example compute,\n",
    "computer, computed, and computing. The process of changing words back to\n",
    "their root, or basic form (by removing prefixes and suffixes) is known as\n",
    "stemming. \n",
    "\n",
    "The most widely used stemmer, or program/method that performs stemming,\n",
    "is the _Porter Stemmer_, which was originally published in 1980 by\n",
    "Martin Porter. An improved version was released in 2000, which fixed a\n",
    "number of errors. NLTK includes the Porter Stemmer, which can be used\n",
    "with scikit learn by creating a special function that tokenizes text\n",
    "documents and passing this function as an argument to the\n",
    "`CountVectorizer` via the `tokenizer` attribute. By performing stemming\n",
    "inside this tokenize method, we can return a set of tokens for a\n",
    "document that have been stemmed. In the following code cell, we use a\n",
    "custom `tokenize` method that first builds a list of tokens by using\n",
    "nltk, and then maps the Porter stemmer to the list of tokens to generate\n",
    "a stemmed list.\n",
    "\n",
    "\n",
    "-----\n",
    "[ws]: https://en.wikipedia.org/wiki/Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.83      0.77      0.80       259\n",
      "        pos       0.77      0.83      0.80       241\n",
      "\n",
      "avg / total       0.80      0.80      0.80       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = map(stemmer.stem, tokens)\n",
    "    return stems\n",
    "\n",
    "pclf.set_params(cv__stop_words = 'english', \\\n",
    "                cv__ngram_range=(1,3), \\\n",
    "                cv__lowercase=True, \\\n",
    "                cv__tokenizer=tokenize)\n",
    "\n",
    "pclf.fit(mvr_train, y_train)\n",
    "y_pred = pclf.predict(mvr_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names = mvr.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Features = 80529\n"
     ]
    }
   ],
   "source": [
    "# Extract the classifier\n",
    "clf = pclf.steps[1][1]\n",
    "print('Number of Features = {}'.format(clf.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we incorporated the Porter Stemmer into the\n",
    "classification pipeline. Now that you have run the Notebook, go back and\n",
    "make the following changes to see how the results change.\n",
    "\n",
    "1. Did the Porter Stemmer improve the classification results (note you\n",
    "need to be sure you are comparing exactly the same pipeline (including\n",
    "the use of ngrams)? \n",
    "\n",
    "2. Can you compute the number of features that the Porter Stemmer\n",
    "generates? How does this compare the number of features without stemming?\n",
    "\n",
    "3. Try using a different stemming algorithm, such as [_snowball_][nsw].\n",
    "How do the classification results change? \n",
    "\n",
    "-----\n",
    "[nsw]: http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.snowball\n",
    "\n",
    "## Clustering Analysis\n",
    "\n",
    "We can also apply clustering analysis to our feature matrix. While\n",
    "finding an unknown number of clusters in text documents can be\n",
    "difficult, we can learn about our data by identifying the clusters for\n",
    "our **known** labels. To demonstrate, in the following code cells, we\n",
    "employ k-means to find two clusters in our feature matrix (the moview reviews), after\n",
    "which we identify the most frequently used words in each cluster.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=2, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_k = 2\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Verify attributes\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', \\\n",
    "                     ngram_range=(1, 3), max_features=100000)\n",
    "\n",
    "train_counts = cv.fit_transform(mvr_train)\n",
    "test_data = cv.transform(mvr_test)\n",
    "\n",
    "km.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens per cluster:\n",
      "\n",
      "Cluster 0: film like movie just time good character story way characters scene does really films make life plot people man scenes\n",
      "\n",
      "Cluster 1: movie film like just good time story character way make new plot characters little bad man movies people does really\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_tokens = 20\n",
    "labels = ['Neg', 'Pos']\n",
    "\n",
    "print('Top {} tokens per cluster:\\n'.format(top_tokens))\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "\n",
    "for idx in range(true_k):\n",
    "    print(\"Cluster {0}:\".format(idx), end='')\n",
    "    for jdx in order_centroids[idx, :top_tokens]:\n",
    "        print(' {0}'.format(terms[jdx]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can perform the same analysis on a more complex problem by analyzing\n",
    "the twenty newsgroup data set. First we load the data, after which we\n",
    "apply k-means and identify the most common tokens in each cluster.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='train', shuffle=True, random_state=23)\n",
    "test = fetch_20newsgroups(data_home='/home/data_scientist/data/textdm', subset='test', shuffle=True, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(copy_x=True, init='k-means++', max_iter=100, n_clusters=20, n_init=1,\n",
       "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
       "    verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "true_k = 20\n",
    "\n",
    "km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Verify attributes\n",
    "\n",
    "cv = CountVectorizer(stop_words = 'english', max_features=100000)\n",
    "train_counts = cv.fit_transform(train['data'])\n",
    "test_data = cv.transform(test['data'])\n",
    "\n",
    "km.fit(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 tokens per cluster:\n",
      "\n",
      "Cluster 0: cancer hiv health 1993 medical number april 25 11 disease drug information volume newsletter hicnet page pages patients national aids\n",
      "\n",
      "Cluster 1: edu subject com lines organization writes article university don like just posting host nntp know people think does time new\n",
      "\n",
      "Cluster 2: dos windows microsoft tcp ms mouse amiga software pc graphics higher macintosh network mbytes version 00 ip memory support card\n",
      "\n",
      "Cluster 3: 92 12 10 hiv 17 11 aids patients et 03 30 medical 25 milk cd4 31 tb 1993 number 04\n",
      "\n",
      "Cluster 4: jpeg image gif file color format images quality version files bit free programs available use jfif software don display edu\n",
      "\n",
      "Cluster 5: said people president adl think mr know don god openwindows did stephanopoulos just general time use files going mac say\n",
      "\n",
      "Cluster 6: edu 25 com file image ftp use server gopher images pub azerbaijan windows program kinsey information time files make like\n",
      "\n",
      "Cluster 7: jehovah elohim lord god christ father mcconkie unto son ps jesus said gods shall thou thee mormon thy earth stated\n",
      "\n",
      "Cluster 8: pms vitamin magnesium supplementation symptoms placebo b6 premenstrual experimental study group studies pts significantly significant women received double reported blind\n",
      "\n",
      "Cluster 9: edu graphics pub mail ray 128 send ftp 3d com server objects amiga rayshade archie image images file files archive\n",
      "\n",
      "Cluster 10: mb m4 ms ma mz mm mo m1 mc mu mw mh mf mp mj mt mx mk mr mn\n",
      "\n",
      "Cluster 11: image edu data available graphics ftp software pub processing images package format analysis display formats information file version files program\n",
      "\n",
      "Cluster 12: 00 50 20 1st 10 wolverine appears art 40 man sabretooth hulk new comics 80 liefeld list 15 app edu\n",
      "\n",
      "Cluster 13: ma mx mb mm ml m7 m1 m8 ms 0f mc mq mp mg mi 3x m6 m4 88 m9\n",
      "\n",
      "Cluster 14: det tor bos chi la pit nyr nyi nj buf que mtl van stl edm wsh phi min hfd wpg\n",
      "\n",
      "Cluster 15: zarathushtra daniel iranian tradition great book sources 12 avesta religious years iran religion prophet historical form century pahlavi persian zoroaster\n",
      "\n",
      "Cluster 16: planet earth spacecraft solar surface sun venus moon atmosphere planets jupiter kilometers miles space years moons degrees mars saturn mariner\n",
      "\n",
      "Cluster 17: myers ms president think don dee ll know said going decision does house white today believe justice just board department\n",
      "\n",
      "Cluster 18: venus space kilometers soviet miles vega balloon planet earth new probes venera mission probe surface asa society soviets lander radar\n",
      "\n",
      "Cluster 19: kuwait al sheikh sabah iraq british kuwaiti ottoman history arabia oil abu government gulf ruler relations arab time following expedition\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_tokens = 20\n",
    "labels = test['target']\n",
    "\n",
    "print('Top {} tokens per cluster:\\n'.format(top_tokens))\n",
    "\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = cv.get_feature_names()\n",
    "\n",
    "for idx in range(true_k):\n",
    "    print(\"Cluster {0}:\".format(idx), end='')\n",
    "    for jdx in order_centroids[idx, :top_tokens]:\n",
    "        print(' {0}'.format(terms[jdx]), end='')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we used k-means clustering to find clusters in\n",
    "text data, and to identify the most important tokens in these clusters.\n",
    "Now that you have run the Notebook, go back and make the following\n",
    "changes to see how the results change.\n",
    "\n",
    "1. Change the vectorizer to use TF-IDF. Does this change the tokens in\n",
    "each cluster? \n",
    "2. Change the vectorizer to use ngrams. Does this change the tokens in\n",
    "each cluster?\n",
    "3. Include stemming in the vectorizer. Does this change the tokens in\n",
    "each cluster?\n",
    "4. Try using DBSCAN instead of k-means. How many clusters are found?\n",
    "What are the tokens in each cluster?\n",
    "\n",
    "Finally, the k-means algorithm finds clusters. Do these clusters map\n",
    "directly into the _real_ categories? Feel free to discuss this on the\n",
    "course forums.\n",
    "\n",
    "-----\n",
    "\n",
    "## Dimension Reduction\n",
    "\n",
    "The document term matrices we have constructed in these examples can\n",
    "become quite large. We have already reduced the number of features used\n",
    "in classification problems by using stop words, by using a consistent\n",
    "case, and by performing stemming. On the other hand, we have enabled\n",
    "exponential increases in the feature space by including n-grams\n",
    "(although we once again restricted the feature space by using the\n",
    "`max_features` or the `max_df` and `min_df` attributes). The\n",
    "traditional dimensional reduction technique we have used in the past is\n",
    "PCA. However, PCA can be difficult to use with text data given the large\n",
    "sizes of the matrices (since a matrix inversion can be required). We can\n",
    "employ alternative techniques, such as incremental PCA or Truncated SVD. \n",
    "\n",
    "But in reality, we are less interested in finding a reduced dimensional\n",
    "space than we are in removing features that contain little or no\n",
    "information (combining features is essentially increasing the ngram\n",
    "range). In this case, the problem of dimension reduction becomes one of\n",
    "optimal feature selection. For this, we can use the scikit learn\n",
    "`SelectKBest` method to identify the best $k$ features. In the following\n",
    "code cells, we first create a vectorizer, train it on data to\n",
    "demonstrate the accuracy and the number of features required to achieve\n",
    "that accuracy. After which, we employ `SelectKBest` to identify the best\n",
    "number of features, where number is predetermined, to achieve a similar\n",
    "accuracy. In the end, we find that less than ten percent of the original\n",
    "features are required to achieve the same level of accuracy.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Following Example was insipred by scikit learn demo\n",
    "# http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy =  82.0%\n",
      "Number of Features = 129792\n"
     ]
    }
   ],
   "source": [
    "# First, train on normal set of features, baseline performance.\n",
    "\n",
    "train_counts = tf.fit_transform(train['data'])\n",
    "test_data = tf.transform(test['data'])\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb = nb.fit(train_counts, train['target'])\n",
    "predicted = nb.predict(test_data)\n",
    "\n",
    "print(\"Prediction accuracy = {0:5.1f}%\".format(100.0 * nb.score(test_data, test['target'])))\n",
    "print('Number of Features = {}'.format(nb.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now employ feature selection\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "# Number of features to keep\n",
    "num_k = 10000\n",
    "\n",
    "# Employ select k best wiht chi2 since this works with sparse matrices.\n",
    "ch2 = SelectKBest(chi2, k=num_k)\n",
    "xtr = ch2.fit_transform(train_counts, train['target'])\n",
    "xt = ch2.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB prediction accuracy =  82.0%\n",
      "Number of Features = 10000\n"
     ]
    }
   ],
   "source": [
    "# Train simple model and compute accuracy\n",
    "nb = nb.fit(xtr, train['target'])\n",
    "predicted = nb.predict(xt)\n",
    "\n",
    "print(\"NB prediction accuracy = {0:5.1f}%\".format(100.0 * nb.score(xt, test['target'])))\n",
    "print('Number of Features = {}'.format(nb.feature_log_prob_.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can use the feature selection to identify the top features for each\n",
    "category. First we extract the feature names, after which we extract the\n",
    "importance (or support) for each feature. By sorting the array containing\n",
    "the important features, we can identify the top tokens.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feature_names = tf.get_feature_names()\n",
    "\n",
    "indices = ch2.get_support(indices=True)\n",
    "feature_names = np.array([feature_names[idx] for idx in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "alt.atheism:\n",
      "['keith', 'god', 'caltech', 'atheists', 'livesey']\n",
      "\n",
      "comp.graphics:\n",
      "['graphics', 'image', 'thanks', '3d', 'files']\n",
      "\n",
      "comp.os.ms-windows.misc:\n",
      "['windows', 'dos', 'file', 'files', 'driver']\n",
      "\n",
      "comp.sys.ibm.pc.hardware:\n",
      "['drive', 'card', 'scsi', 'ide', 'bus']\n",
      "\n",
      "comp.sys.mac.hardware:\n",
      "['mac', 'apple', 'quadra', 'drive', 'centris']\n",
      "\n",
      "comp.windows.x:\n",
      "['window', 'motif', 'mit', 'com', 'server']\n",
      "\n",
      "misc.forsale:\n",
      "['sale', 'offer', 'shipping', 'distribution', 'condition']\n",
      "\n",
      "rec.autos:\n",
      "['car', 'cars', 'com', 'article', 'engine']\n",
      "\n",
      "rec.motorcycles:\n",
      "['bike', 'dod', 'com', 'ride', 'motorcycle']\n",
      "\n",
      "rec.sport.baseball:\n",
      "['baseball', 'year', 'game', 'team', 'players']\n",
      "\n",
      "rec.sport.hockey:\n",
      "['hockey', 'team', 'game', 'ca', 'nhl']\n",
      "\n",
      "sci.crypt:\n",
      "['clipper', 'key', 'encryption', 'chip', 'com']\n",
      "\n",
      "sci.electronics:\n",
      "['com', 'use', 'circuit', 'host', 'power']\n",
      "\n",
      "sci.med:\n",
      "['pitt', 'geb', 'banks', 'gordon', 'com']\n",
      "\n",
      "sci.space:\n",
      "['space', 'nasa', 'moon', 'orbit', 'henry']\n",
      "\n",
      "soc.religion.christian:\n",
      "['god', 'jesus', 'christians', 'rutgers', 'christian']\n",
      "\n",
      "talk.politics.guns:\n",
      "['gun', 'guns', 'com', 'people', 'batf']\n",
      "\n",
      "talk.politics.mideast:\n",
      "['israel', 'israeli', 'jews', 'turkish', 'arab']\n",
      "\n",
      "talk.politics.misc:\n",
      "['com', 'cramer', 'article', 'people', 'clinton']\n",
      "\n",
      "talk.religion.misc:\n",
      "['god', 'sandvik', 'com', 'christian', 'jesus']\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "top_count = 5\n",
    "\n",
    "for idx, target in enumerate(train['target_names']):\n",
    "    top_names = np.argsort(nb.coef_[idx])[-top_count:]\n",
    "    tn_lst = [name for name in feature_names[top_names]]\n",
    "    tn_lst.reverse()\n",
    "\n",
    "    print('\\n{0}:'.format(target))\n",
    "    pp.pprint(tn_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we used feature selection to identify the most\n",
    "important features in our simple classification pipeline. Now that you\n",
    "have run the Notebook, go back and make the following changes to see how\n",
    "the results change.\n",
    "\n",
    "1. Change the vectorizer to change the case of all words an to employ\n",
    "stemming. How do the results (tokens) change?\n",
    "\n",
    "2. Change the classification algorithm to a more accurate method. How do\n",
    "the results change? How does the computational time change?\n",
    "\n",
    "Finally, what do the list of tokens say about the fact we did not remove\n",
    "headers or footers from the newsgroup postings? Feel free to comment on\n",
    "these questions in the course forum.\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "561e14d1756a0b8e8b33a2db17cde032",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "This notebook will be collected automatically at **6pm on Monday** from `/home/data_scientist/assignments/Week7` directory on the course JupyterHub server. If you work on this assignment on the course Jupyterhub server, just make sure that you save your work and instructors will pull your notebooks automatically after the deadline. If you work on this assignment locally, the only way to submit assignments is via Jupyterhub, and you have to place the notebook file in the correct directory with the correct file name before the deadline.\n",
    "\n",
    "1. Make sure everything runs as expected. First, restart the kernel (in the menubar, select `Kernel` → `Restart`) and then run all cells (in the menubar, select `Cell` → `Run All`).\n",
    "2. Make sure you fill in any place that says `YOUR CODE HERE`. Do not write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed by the autograder.\n",
    "3. Do not change the file path or the file name of this notebook.\n",
    "4. Make sure that you save your work (in the menubar, select `File` → `Save and CheckPoint`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7.3. Mining.\n",
    "\n",
    "In this problem, we use the Reuters corpus to perform text mining tasks, such as n-grams, stemming, and clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "1e499f342d6ae876d87fa4df8a454a44",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "import string\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nose.tools import (\n",
    "    assert_equal,\n",
    "    assert_is_instance,\n",
    "    assert_almost_equal,\n",
    "    assert_true\n",
    ")\n",
    "from numpy.testing import assert_array_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyze the NLTK Reuters corpus. See the [NLTK docs](http://www.nltk.org/book/ch02.html#reuters-corpus) for more information.\n",
    "\n",
    "In [Problem 7.2](https://github.com/UI-DataScience/info490-sp16/blob/master/Week7/assignments/w7p2.ipynb), we created the training and test sets from the Reuters corpus. I saved `X_train`, `X_test`, `y_train`, and `y_test` from Problem 7.2 as JSON files.\n",
    "\n",
    "```python\n",
    "import json\n",
    "with open('reuters_X_train.json', 'w') as f:\n",
    "    json.dump(X_train, f)\n",
    "```\n",
    "\n",
    "The JSON files are available in `/home/data_scientist/data/misc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "54554ed1e7f91b6c59cc02b3b01a1ef2",
     "grade": false,
     "grade_id": "load_reuters",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def load_reuters(name):\n",
    "    fpath = '/home/data_scientist/data/misc/reuters_{}.json'.format(name)\n",
    "    with open(fpath) as f:\n",
    "        reuters = json.load(f)\n",
    "    return reuters\n",
    "\n",
    "X_train, X_test, y_train, y_test = map(load_reuters, ['X_train', 'X_test', 'y_train', 'y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## n-grams\n",
    "\n",
    "- Use unigrams, bigrams, and trigrams,\n",
    "- Build a pipeline by using `TfidfVectorizer` and `LinearSVC`,\n",
    "- Name the first step `tf` and the second step `svc`,\n",
    "- Use default parameters for both `TfidfVectorizer` and `LinearSVC`,\n",
    "- Use English stop words,\n",
    "- Impose a minimum feature term that requires a term to be present in at least three documents, and\n",
    "- Set a maximum frequency of 75%, such that any term occurring in more than 75% of all documents will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7fd194d9ccbbf2403fce41115b51c75d",
     "grade": false,
     "grade_id": "ngram_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def ngram(X_train, y_train, X_test, random_state):\n",
    "    '''\n",
    "    Creates a document term matrix and uses SVM classifier to make document classifications.\n",
    "    Uses unigrams, bigrams, and trigrams.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A list of strings.\n",
    "    y_train: A list of strings.\n",
    "    X_test: A list of strings.\n",
    "    random_state: A np.random.RandomState instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (clf, y_pred)\n",
    "    clf: A Pipeline instance.\n",
    "    y_pred: A numpy array.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5a8baa2c71a9662191a7f1367db11e10",
     "grade": false,
     "grade_id": "ngram_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "clf1, y_pred1 = ngram(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "score1 = accuracy_score(y_pred1, y_test)\n",
    "print(\"SVC prediction accuracy = {0:5.1f}%\".format(100.0 * score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "fc6fdd3ff6112d29bb2306a9e19154fd",
     "grade": true,
     "grade_id": "ngram_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(clf1, Pipeline)\n",
    "assert_is_instance(y_pred1, np.ndarray)\n",
    "tf1 = clf1.named_steps['tf']\n",
    "assert_is_instance(tf1, TfidfVectorizer)\n",
    "assert_is_instance(clf1.named_steps['svc'], LinearSVC)\n",
    "assert_equal(tf1.stop_words, 'english')\n",
    "assert_equal(tf1.ngram_range, (1, 3))\n",
    "assert_equal(tf1.max_df, 0.75)\n",
    "assert_equal(tf1.min_df, 3)\n",
    "assert_equal(len(y_pred1), len(y_test))\n",
    "assert_array_equal(y_pred1[:5], ['trade', 'grain', 'crude', 'bop', 'palm-oil'])\n",
    "assert_array_equal(y_pred1[-5:], ['acq', 'dlr', 'ship', 'ipi', 'gold'])\n",
    "assert_almost_equal(score1, 0.90195428949983436)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "- Use the `tokenize` method in the following code cell to incorporate the Porter Stemmer into the classification pipeline,\n",
    "- Use unigrams, bigrams, and trigrams\n",
    "- Build a pipeline by using `TfidfVectorizer` and `LinearSVC`,\n",
    "- Name the first step `tf` and the second step `svc`,\n",
    "- Use default parameters for both `TfidfVectorizer` and `LinearSVC`,\n",
    "- Use English stop words,\n",
    "- Impose a minimum feature term that requires a term to be present in at least two documents, and\n",
    "- Set a maximum frequency of 50%, such that any term occurring in more than 50% of all documents will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "9822d6444475151fbdb543ca8197a3c7",
     "grade": false,
     "grade_id": "tokenize",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = map(stemmer.stem, tokens)\n",
    "    \n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "2e374130ffaac50a33901620a841a434",
     "grade": false,
     "grade_id": "stem_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def stem(X_train, y_train, X_test, random_state):\n",
    "    '''\n",
    "    Creates a document term matrix and uses SVM classifier to make document classifications.\n",
    "    Uses the Porter stemmer.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A list of strings.\n",
    "    y_train: A list of strings.\n",
    "    X_test: A list of strings.\n",
    "    random_state: A np.random.RandomState instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (clf, y_pred)\n",
    "    clf: A Pipeline instance.\n",
    "    y_pred: A numpy array.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return clf, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "746539c1cd15a09835d820327c2cbc48",
     "grade": false,
     "grade_id": "stem_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "clf2, y_pred2 = stem(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "score2 = accuracy_score(y_pred2, y_test)\n",
    "print(\"SVC prediction accuracy = {0:5.1f}%\".format(100.0 * score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "fe644781a8238b865aa59139a1f80ea0",
     "grade": true,
     "grade_id": "stem_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(clf2, Pipeline)\n",
    "assert_is_instance(y_pred2, np.ndarray)\n",
    "tf2 = clf2.named_steps['tf']\n",
    "assert_is_instance(tf2, TfidfVectorizer)\n",
    "assert_is_instance(clf2.named_steps['svc'], LinearSVC)\n",
    "assert_equal(tf2.stop_words, 'english')\n",
    "assert_equal(tf2.ngram_range, (1, 3))\n",
    "assert_equal(tf2.max_df, 0.5)\n",
    "assert_equal(tf2.min_df, 2)\n",
    "assert_equal(len(y_pred2), len(y_test))\n",
    "assert_array_equal(y_pred2[:5], ['trade', 'grain', 'crude', 'bop', 'palm-oil'])\n",
    "assert_array_equal(y_pred2[-5:], ['acq', 'dlr', 'ship', 'ipi', 'gold'])\n",
    "assert_almost_equal(score2, 0.89234845975488575)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Clustering Analysis\n",
    "\n",
    "- Build a pipeline by using `TfidfVectorizer` and `KMeans`,\n",
    "- Name the first step `tf` and the second step `km`,\n",
    "- Use default parameters for both `TfidfVectorizer` and `KMeans`,\n",
    "- Use unigrams only,\n",
    "- Use English stop words, and\n",
    "- Set the number of clusters equal to `true_k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "ff20608a82f8f32f5c7317a89a634ea7",
     "grade": false,
     "grade_id": "cluster_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cluster(X_train, X_test, true_k, random_state):\n",
    "    '''\n",
    "    Applies clustering analysis to a feature matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A list of strings.\n",
    "    X_test: A list of strings.\n",
    "    true_k: An int. The number of clusters.\n",
    "    random_state: A np.random.RandomState instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A Pipeline instance.\n",
    "    '''    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8334a6f305b8ec5e81b7ba02b8344ced",
     "grade": false,
     "grade_id": "cluster_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "clf3 = cluster(X_train, X_test, true_k=len(reuters.categories()), random_state=check_random_state(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3db9feb71b0a60355c24fa8d58c33076",
     "grade": true,
     "grade_id": "cluster_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(clf3, Pipeline)\n",
    "tf3 = clf3.named_steps['tf']\n",
    "assert_is_instance(tf3, TfidfVectorizer)\n",
    "km3 = clf3.named_steps['km']\n",
    "assert_is_instance(km3, KMeans)\n",
    "assert_equal(tf3.stop_words, 'english')\n",
    "assert_equal(tf3.ngram_range, (1, 1))\n",
    "assert_equal(tf3.max_df, 1.0)\n",
    "assert_equal(tf3.min_df, 1)\n",
    "assert_equal(km3.n_clusters, len(reuters.categories()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write a function that identifies the most frequently used words in a cluster.\n",
    "\n",
    "The `cluster` parameter specifies the cluster label. Since we are not using training labels, the cluster labels returned by `KMeans` will be integers.\n",
    "\n",
    "If you set `top_tokens=3`, the function should return a list of top 3 tokens in the specified cluster; similarly, if you set `top_tokens=5`, it should return a list of top 5 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "becb74c2fed6a7c9c15813ecbdba15b8",
     "grade": false,
     "grade_id": "get_top_tokens_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_top_tokens(km, tf, cluster, top_tokens):\n",
    "    '''\n",
    "    Identifies the most frequently used words in \"icluster\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    km: A Kmeans instance.\n",
    "    tf: A TfidfVectorizer instance.\n",
    "    icluster: An int. Which cluster?\n",
    "    top_tokens: An int. How many tokens do you want?\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a724f4b47ffb0aa9096cc1823df39f27",
     "grade": false,
     "grade_id": "get_top_tokens_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "km = clf3.named_steps['km']\n",
    "tf = clf3.named_steps['tf']\n",
    "\n",
    "for i in range(len(reuters.categories())):\n",
    "    print(\"Cluster {0}: {1}\".format(i, ' '.join(get_top_tokens(km, tf, i, 5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f5744709b1448af1e00153faa2df6737",
     "grade": true,
     "grade_id": "get_top_tokens_test",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "token0 = get_top_tokens(km, tf, 0, 3)\n",
    "assert_is_instance(token0, list)\n",
    "assert_true(all(isinstance(t, str) for t in token0))\n",
    "assert_equal(token0, ['units', 'housing', 'starts'])\n",
    "\n",
    "token1 = get_top_tokens(km, tf, 89, 5)\n",
    "assert_is_instance(token1, list)\n",
    "assert_true(all(isinstance(t, str) for t in token1))\n",
    "assert_equal(token1, ['qtly', 'cts', 'div', 'dividend', 'record'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

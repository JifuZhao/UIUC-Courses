{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "561e14d1756a0b8e8b33a2db17cde032",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "This notebook will be collected automatically at **6pm on Monday** from `/home/data_scientist/assignments/Week7` directory on the course JupyterHub server. If you work on this assignment on the course Jupyterhub server, just make sure that you save your work and instructors will pull your notebooks automatically after the deadline. If you work on this assignment locally, the only way to submit assignments is via Jupyterhub, and you have to place the notebook file in the correct directory with the correct file name before the deadline.\n",
    "\n",
    "1. Make sure everything runs as expected. First, restart the kernel (in the menubar, select `Kernel` → `Restart`) and then run all cells (in the menubar, select `Cell` → `Run All`).\n",
    "2. Make sure you fill in any place that says `YOUR CODE HERE`. Do not write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed by the autograder.\n",
    "3. Do not change the file path or the file name of this notebook.\n",
    "4. Make sure that you save your work (in the menubar, select `File` → `Save and CheckPoint`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7.2. Text Classification.\n",
    "\n",
    "In this problem, we perform text classificatoin tasks by using the nltk and the scikit learn machine learning libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c9e4c32c243cbcf96b6bc6c7107bc247",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import re\n",
    "import requests\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nose.tools import (\n",
    "    assert_equal,\n",
    "    assert_is_instance,\n",
    "    assert_almost_equal,\n",
    "    assert_true\n",
    ")\n",
    "from numpy.testing import assert_array_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyze the NLTK Reuters corpus. See the [NLTK docs](http://www.nltk.org/book/ch02.html#reuters-corpus) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2cd30c8732682eaf6565fb9504a2ee3a",
     "grade": false,
     "grade_id": "import_reuters",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories\n",
    "\n",
    "Before delving into text data mining, let's first explore the Reuters data set.\n",
    "\n",
    "- Write a function that, given an NLTK corpus object, returns its categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "043e6335bfcc75670c6f099e8b8600d9",
     "grade": false,
     "grade_id": "get_categories_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_categories(corpus):\n",
    "    '''\n",
    "    Finds categories of an NLTK corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An NLTK corpus object.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "bd83d7db1f8d51b19c2844705cde5ef0",
     "grade": false,
     "grade_id": "get_categories_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "all_categories = get_categories(reuters)\n",
    "print(all_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "89ca95c8cc7fec800dc5397d76903539",
     "grade": true,
     "grade_id": "get_categories_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(all_categories, list)\n",
    "for c in all_categories:\n",
    "    assert_is_instance(c, str)\n",
    "assert_equal(len(all_categories), 90)\n",
    "assert_equal(sorted(all_categories)[:5], ['acq', 'alum', 'barley', 'bop', 'carcass'])\n",
    "assert_equal(sorted(all_categories)[-5:], ['veg-oil', 'wheat', 'wpi', 'yen', 'zinc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fileids\n",
    "\n",
    "- Find all `fileids` of the Reuters corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "be1dd16cb6401b129c162bf46ff0efa6",
     "grade": false,
     "grade_id": "get_fileids_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_fileids(corpus):\n",
    "    '''\n",
    "    Finds all fileids of an NLTK corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An NLTK corpus object.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return fileids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "85c2c97ebd68e8cc4fdac96b236941ad",
     "grade": false,
     "grade_id": "get_fileids_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "fileids = get_fileids(reuters)\n",
    "print(fileids[:5], '...', fileids[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8161e849d65b7220ceed1037aef770e6",
     "grade": true,
     "grade_id": "get_fileids_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(fileids, list)\n",
    "for f in fileids:\n",
    "    assert_is_instance(f, str)\n",
    "assert_equal(len(fileids), 10788)\n",
    "assert_equal(\n",
    "    sorted(fileids)[:5],\n",
    "    ['test/14826', 'test/14828', 'test/14829', 'test/14832', 'test/14833']\n",
    "    )\n",
    "assert_equal(\n",
    "    sorted(fileids)[-5:], \n",
    "    ['training/999', 'training/9992', 'training/9993', 'training/9994', 'training/9995']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List of categories\n",
    "\n",
    "The corpus contains 10,788 documents (`fileids`) which have been classified into 90 topics (`categories`). We will use those 10,788 documents to train machine learning models and try to predict which topic each document belongs to.\n",
    "\n",
    "- Find categories for each element of `fileids`.\n",
    "\n",
    "Note, categories in the Reuters corpus overlap with each other, simply because a news story often covers multiple topics. If a document has more than one category, use **only the first category** (in alphabetical order).\n",
    "\n",
    "Since we using only one category for each `fileid`, the result from `get_categories_from_fileids()` should have the same length as the `fileids` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "4e9b5c2a46cc9a7d59a3af2ca1386249",
     "grade": false,
     "grade_id": "get_categories_from_fields_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_categories_from_fileids(corpus, fileids):\n",
    "    '''\n",
    "    Finds categories for each element of \"fileids\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An NLTK corpus.\n",
    "    fileids: A list of strings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "0c2841884f1f0f617e1a1e33927519c2",
     "grade": false,
     "grade_id": "get_categories_from_fileids_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "categories = get_categories_from_fileids(reuters, fileids)\n",
    "print(categories[:5], '...', categories[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8dc10b0f00f450dbc0d24aa88903dd99",
     "grade": true,
     "grade_id": "get_categories_from_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(categories, list)\n",
    "assert_true(all(isinstance(c, str) for c in categories))\n",
    "assert_equal(len(categories), len(fileids))\n",
    "assert_equal(\n",
    "    categories[:5],\n",
    "    ['trade', 'grain', 'crude', 'corn', 'palm-oil']\n",
    "    )\n",
    "assert_equal(\n",
    "    categories[-5:],\n",
    "    ['interest', 'earn', 'earn', 'earn', 'earn']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test sets\n",
    "\n",
    "The Reuters data set has already been grouped into a training set and a test set. See `fileids`.\n",
    "\n",
    "- To create a training set, iterate through `fileids` and find all `fileids` that start with \"train\". `X_train` should be a list of **raw data** strings, which you can obtain by using the `raw()` method. `y_train`  is a list of categories. Repeat for all `fileids` that start with \"test\". In the end, `train_test_split()` should return a 4-tuple of `X_train`, `X_test`, `y_train`, and `y_test`, each of which is a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "04f25ba0bd6a2feb76f584a2cfd2795c",
     "grade": false,
     "grade_id": "train_test_split_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(corpus, fileids, categories):\n",
    "    '''\n",
    "    Creates a training set and a test from the NLTK Reuters corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An NLTK corpus.\n",
    "    fileids: A list of strings.\n",
    "    categories: A list of strings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A 4-tuple (X_train, X_test, y_train, y_test)\n",
    "    All four elements in the tuple are lists of strings.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "6e4e6ef51f106bf0c9425b5d20342e09",
     "grade": false,
     "grade_id": "train_test_split_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(reuters, fileids, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "acfd9e7ccc3bb8f4446e1637e8a1f730",
     "grade": true,
     "grade_id": "train_test_split_test",
     "locked": true,
     "points": 8,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(X_train, list)\n",
    "assert_is_instance(X_test, list)\n",
    "assert_is_instance(y_train, list)\n",
    "assert_is_instance(y_test, list)\n",
    "\n",
    "assert_true(all(isinstance(elem, str) for elem in X_train))\n",
    "assert_true(all(isinstance(elem, str) for elem in X_test))\n",
    "assert_true(all(isinstance(elem, str) for elem in y_train))\n",
    "assert_true(all(isinstance(elem, str) for elem in y_test))\n",
    "\n",
    "assert_equal(len(X_train), 7769)\n",
    "assert_equal(len(X_test), 3019)\n",
    "assert_equal(len(X_train), len(y_train))\n",
    "assert_equal(len(X_test), len(y_test))\n",
    "\n",
    "assert_equal(X_train[0][:20], 'BAHIA COCOA REVIEW\\n ')\n",
    "assert_equal(y_train[0], 'cocoa')\n",
    "assert_equal(X_test[0][:20], 'ASIAN EXPORTERS FEAR')\n",
    "assert_equal(y_test[0], 'trade')\n",
    "\n",
    "assert_equal(X_train[1][:20], 'COMPUTER TERMINAL SY')\n",
    "assert_equal(y_train[1], 'acq')\n",
    "assert_equal(X_test[1][:20], 'CHINA DAILY SAYS VER')\n",
    "assert_equal(y_test[1], 'grain')\n",
    "\n",
    "assert_equal(X_train[2][:20], 'N.Z. TRADING BANK DE')\n",
    "assert_equal(y_train[2], 'money-supply')\n",
    "assert_equal(X_test[2][:20], 'JAPAN TO REVISE LONG')\n",
    "assert_equal(y_test[2], 'crude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC (no pipeline, no stop words)\n",
    "\n",
    "- Use `CountVectorizer` to create a document term matrix, and apply `LinearSVC` algorithm to classify which topic each news document belongs to. Do not use pipeline (yet). Do not use stop words (yet). Use default parameters for both `CountVectorizer` and `LinearSVC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "97879f4529537743f221f67c210ff5b0",
     "grade": false,
     "grade_id": "cv_svc_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cv_svc(X_train, y_train, X_test, random_state):\n",
    "    '''\n",
    "    Creates a document term matrix and uses SVM classifier to make document classifications.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A list of strings.\n",
    "    y_train: A list of strings.\n",
    "    X_test: A list of strings.\n",
    "    random_state: A np.random.RandomState instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (cv, sv, y_pred)\n",
    "    cv: A CountVectorizer instance.\n",
    "    svc: A LinearSVC instance.\n",
    "    y_pred: A numpy array.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return cv, svc, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "cb64bfe7073821eaef41aa1cc0c3c054",
     "grade": false,
     "grade_id": "cv_svc_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "cv1, svc1, y_pred1 = cv_svc(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "score1 = accuracy_score(y_pred1, y_test)\n",
    "print(\"SVC prediction accuracy = {0:3.1f}%\".format(100.0 * score1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "1418a2ad4967f1ac0bda6dede6e580ff",
     "grade": true,
     "grade_id": "cv_svc_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(cv1, CountVectorizer)\n",
    "assert_is_instance(svc1, LinearSVC)\n",
    "assert_is_instance(y_pred1, np.ndarray)\n",
    "assert_equal(cv1.stop_words, None)\n",
    "assert_equal(len(y_pred1), len(y_test))\n",
    "assert_array_equal(y_pred1[:5], ['trade', 'grain', 'crude', 'corn', 'palm-oil'])\n",
    "assert_array_equal(y_pred1[-5:], ['acq', 'dlr', 'earn', 'ipi', 'gold'])\n",
    "assert_almost_equal(score1, 0.88274263000993702)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC (Pipeline, no stop words)\n",
    "\n",
    "- Build a pipeline by using `CountVectorizer` and `LinearSVC`. Name the first step `cv` and the second step `svc`. Do not use stop words (yet). Use default parameters for both `CountVectorizer` and `LinearSVC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "792d7a058dd0aae228b70304443bf041",
     "grade": false,
     "grade_id": "cv_svc_pipe_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cv_svc_pipe(X_trani, y_train, X_test, random_state):\n",
    "    '''\n",
    "    Creates a document term matrix and uses SVM classifier to make document classifications.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A list of strings.\n",
    "    y_train: A list of strings.\n",
    "    X_test: A list of strings.\n",
    "    random_state: A np.random.RandomState instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (clf, y_pred)\n",
    "    clf: A Pipeline instance.\n",
    "    y_pred: A numpy array.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return clf, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c71d9f23cc54e942fe863321de4e106f",
     "grade": false,
     "grade_id": "cv_svc_pipe_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "clf2, y_pred2 = cv_svc_pipe(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "score2 = accuracy_score(y_pred2, y_test)\n",
    "print(\"SVC prediction accuracy = {0:3.1f}%\".format(100.0 * score2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "75dbe14b1be4e5a62a1ecc95e1b8bf91",
     "grade": true,
     "grade_id": "cv_svc_pipe_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(clf2, Pipeline)\n",
    "assert_is_instance(y_pred2, np.ndarray)\n",
    "cv2 = clf2.named_steps['cv']\n",
    "assert_is_instance(cv2, CountVectorizer)\n",
    "assert_is_instance(clf2.named_steps['svc'], LinearSVC)\n",
    "assert_equal(cv2.stop_words, None)\n",
    "assert_equal(len(y_pred2), len(y_test))\n",
    "assert_array_equal(y_pred1, y_pred2)\n",
    "assert_array_equal(y_pred1, y_pred2)\n",
    "assert_almost_equal(score1, score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC (Pipeline and stop words)\n",
    "\n",
    "- Build a pipeline by using `CountVectorizer` and `LinearSVC`. Name the first step `cv` and the second step `svc`. Use English stop words. Use default parameters for both `CountVectorizer` and `LinearSVC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "c7df701b56731c989d4cbd5f371f9bbe",
     "grade": false,
     "grade_id": "cv_svc_pipe_sw",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cv_svc_pipe_sw(X_train, y_train, X_test, random_state):\n",
    "    '''\n",
    "    Creates a document term matrix and uses SVM classifier to make document classifications.\n",
    "    Uses English stop words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A list of strings.\n",
    "    y_train: A list of strings.\n",
    "    X_test: A list of strings.\n",
    "    random_state: A np.random.RandomState instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (clf, y_pred)\n",
    "    clf: A Pipeline instance.\n",
    "    y_pred: A numpy array.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return clf, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e9179f18ce0969ea4393b0b9e35487c8",
     "grade": false,
     "grade_id": "cv_svc_pipe_sw_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "clf3, y_pred3 = cv_svc_pipe_sw(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "score3 = accuracy_score(y_pred3, y_test)\n",
    "print(\"SVC prediction accuracy = {0:3.1f}%\".format(100.0 * score3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8efd4f4987c1e4c1b780968c5ec7125b",
     "grade": true,
     "grade_id": "cv_svc_pipe_sw_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(clf3, Pipeline)\n",
    "assert_is_instance(y_pred3, np.ndarray)\n",
    "cv3 = clf3.named_steps['cv']\n",
    "assert_is_instance(cv3, CountVectorizer)\n",
    "assert_is_instance(clf3.named_steps['svc'], LinearSVC)\n",
    "assert_equal(cv3.stop_words, 'english')\n",
    "assert_equal(len(y_pred3), len(y_test))\n",
    "assert_array_equal(y_pred3[:5], ['trade', 'grain', 'crude', 'corn', 'palm-oil'])\n",
    "assert_array_equal(y_pred3[-5:], ['acq', 'dlr', 'earn', 'ipi', 'gold'])\n",
    "assert_almost_equal(score3, 0.87777409738323953)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline of TF-IDF and SVM with stop words\n",
    "\n",
    "- Build a pipeline by using `TfidfVectorizer` and `LinearSVC`. Name the first step `tf` and the second step `svc`. Use English stop words. Use default parameters for both `TfidfVectorizer` and `LinearSVC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "0ae5c7fd4dae3261f54698d8bd235924",
     "grade": false,
     "grade_id": "tfidf_svc_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tfidf_svc(X_train, y_train, X_test, random_state):\n",
    "    '''\n",
    "    Creates a document term matrix and uses SVM classifier to make document classifications.\n",
    "    Uses English stop words.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A list of strings.\n",
    "    y_train: A list of strings.\n",
    "    X_test: A list of strings.\n",
    "    random_state: A np.random.RandomState instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (clf, y_pred)\n",
    "    clf: A Pipeline instance.\n",
    "    y_pred: A numpy array.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return clf, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f5ff4e2941491ae84bb545aa2ce37850",
     "grade": false,
     "grade_id": "tfidf_svc_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "clf4, y_pred4 = tfidf_svc(X_train, y_train, X_test, random_state=check_random_state(0))\n",
    "score4 = accuracy_score(y_pred4, y_test)\n",
    "print(\"SVC prediction accuracy = {0:5.1f}%\".format(100.0 * score4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "eaea0d4f685ae9ac6b101f42244faa21",
     "grade": true,
     "grade_id": "tfidf_svc_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(clf4, Pipeline)\n",
    "assert_is_instance(y_pred4, np.ndarray)\n",
    "tf4 = clf4.named_steps['tf']\n",
    "assert_is_instance(tf4, TfidfVectorizer)\n",
    "assert_is_instance(clf4.named_steps['svc'], LinearSVC)\n",
    "assert_equal(tf4.stop_words, 'english')\n",
    "assert_equal(len(y_pred4), len(y_test))\n",
    "assert_array_equal(y_pred4[:5], ['trade', 'grain', 'crude', 'bop', 'palm-oil'])\n",
    "assert_array_equal(y_pred4[-5:], ['acq', 'dlr', 'crude', 'ipi', 'gold'])\n",
    "assert_almost_equal(score4, 0.89831069890692283)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
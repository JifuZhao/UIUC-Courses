{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "561e14d1756a0b8e8b33a2db17cde032",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "This notebook will be collected automatically at **6pm on Monday** from `/home/data_scientist/assignments/Week7` directory on the course JupyterHub server. If you work on this assignment on the course Jupyterhub server, just make sure that you save your work and instructors will pull your notebooks automatically after the deadline. If you work on this assignment locally, the only way to submit assignments is via Jupyterhub, and you have to place the notebook file in the correct directory with the correct file name before the deadline.\n",
    "\n",
    "1. Make sure everything runs as expected. First, restart the kernel (in the menubar, select `Kernel` → `Restart`) and then run all cells (in the menubar, select `Cell` → `Run All`).\n",
    "2. Make sure you fill in any place that says `YOUR CODE HERE`. Do not write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed by the autograder.\n",
    "3. Do not change the file path or the file name of this notebook.\n",
    "4. Make sure that you save your work (in the menubar, select `File` → `Save and CheckPoint`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7.1. Text Analysis.\n",
    "\n",
    "In this problem, we perform basic text analysis tasks,\n",
    "such as accessing data, tokenizing a corpus, and computing token frequencies,\n",
    "on our course syllabus and on the NLTK Reuters corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "2ae2f44bae076d61ac4a256a635ee44d",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import re\n",
    "import requests\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nose.tools import (\n",
    "    assert_equal,\n",
    "    assert_is_instance,\n",
    "    assert_almost_equal,\n",
    "    assert_true,\n",
    "    assert_false\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first half of the problem, we use our [course syllabus](https://github.com/UI-DataScience/info490-sp16/blob/master/orientation/syllabus.md) as a sample text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "bef40ff651267457a5501204250c604f",
     "grade": false,
     "grade_id": "syllabus_text",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "repo_url = 'https://raw.githubusercontent.com/UI-DataScience/info490-sp16'\n",
    "syllabus_path = 'orientation/syllabus.md'\n",
    "commit_hash = '9a70b4f736963ff9bece424b0b34a393ebd574f9'\n",
    "\n",
    "resp = requests.get('{0}/{1}/{2}'.format(repo_url, commit_hash, syllabus_path))\n",
    "syllabus_text = resp.text\n",
    "\n",
    "assert_is_instance(syllabus_text, str)\n",
    "\n",
    "print(syllabus_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "- Tokenize the text string `syllabus_text`.\n",
    "  You should clean up the list of tokens by removing all puntuation tokens\n",
    "  and keeping only tokens with one or more alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "bbd9e93c0a27eb91636e45bdf9cc1cc2",
     "grade": false,
     "grade_id": "tokenize_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_words(text):\n",
    "    '''\n",
    "    Tokenizes the text string, and returns a list of tokens with\n",
    "    one or more alphanumeric characters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: A string.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    words: A list of strings.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "64d6bd63cc50a60e576ea203b59dca52",
     "grade": false,
     "grade_id": "tokenize_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "syllabus_words = get_words(syllabus_text)\n",
    "print(syllabus_words[:5], '...', syllabus_words[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8fde293174b3990cf305aef1bf75a8b5",
     "grade": true,
     "grade_id": "tokenize_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(syllabus_words, list)\n",
    "assert_true(all(isinstance(w, str) for w in syllabus_words))\n",
    "assert_equal(len(syllabus_words), 2363)\n",
    "\n",
    "assert_true(all(all(not c.isupper() for c in w) for w in syllabus_words))\n",
    "assert_true(all(any(c.isalnum() for c in w) for w in syllabus_words))\n",
    "\n",
    "assert_equal(syllabus_words[:5], ['info', '490', 'advanced', 'data', 'science'])\n",
    "assert_equal(syllabus_words[-5:], ['following', 'tuesday', '12', '00', 'pm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Diversity\n",
    "\n",
    "- Compute the the number of tokens, number of words, and lexical diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "5ebb1134ccc89ac2e898cca1796ed761",
     "grade": false,
     "grade_id": "count_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def count(words):\n",
    "    '''\n",
    "    Computes the the number of token, number of words, and lexical diversity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words: A list of of strings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A 3-tuple of (num_tokens, num_words, lex_div)\n",
    "    num_tokens: An int. The number of tokens in \"words\".\n",
    "    num_words: An int. The number of words in \"words\".\n",
    "    lex_div: A float. The lexical diversity of \"words\".\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return num_tokens, num_words, lex_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d0a1c5dd1889769c11a5ebcb19376384",
     "grade": false,
     "grade_id": "count_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "num_tokens, num_words, lex_div = count(syllabus_words)\n",
    "print(\"Syllabus has {0} tokens and {1} words for a lexical diversity of {2:4.3f}\"\n",
    "      \"\".format(num_tokens, num_words, lex_div))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "fd8857a20c52127e3197495c0752ef25",
     "grade": true,
     "grade_id": "count_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(num_tokens, int)\n",
    "assert_is_instance(num_words, int)\n",
    "assert_is_instance(lex_div, float)\n",
    "\n",
    "assert_equal(num_tokens, 702)\n",
    "assert_equal(num_words, 2363)\n",
    "assert_almost_equal(lex_div, 3.366096866096866)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most common occurrences\n",
    "\n",
    "- Compute the most commonly occurring terms and their counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3a0a252f4c45699cf212986e57da5e85",
     "grade": false,
     "grade_id": "get_most_common_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_most_common(words, ntop):\n",
    "    '''\n",
    "    Computes the most commonly occurring terms and their counts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words: A list of of strings.\n",
    "    ntop: An int. The number of most common words that will be returned.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of tuple (token, frequency).\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "94cf04986e6ecc93f648aa481d43a51f",
     "grade": false,
     "grade_id": "get_most_common_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "syllabus_most_common = get_most_common(syllabus_words, 10)\n",
    "\n",
    "print('{0:12s}: {1}'.format('Term', 'Count'))\n",
    "print(20*'-')\n",
    "\n",
    "for token, freq in syllabus_most_common:\n",
    "    print('{0:12s}: {1:4d}'.format(token, freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c662652a0e8b616310bb9b57ef198b2a",
     "grade": true,
     "grade_id": "get_most_common_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(syllabus_most_common, list)\n",
    "assert_true(all(isinstance(t, tuple) for t in syllabus_most_common))\n",
    "assert_true(all(isinstance(t, str) for t, f in syllabus_most_common))\n",
    "assert_true(all(isinstance(f, int) for t, f in syllabus_most_common))\n",
    "\n",
    "assert_equal(len(get_most_common(syllabus_words, 10)), 10)\n",
    "assert_equal(len(get_most_common(syllabus_words, 20)), 20)\n",
    "\n",
    "assert_equal(\n",
    "    set(syllabus_most_common[:10]),\n",
    "    set([('the', 113), ('to', 58), ('will', 51), ('and', 48), ('a', 47),\n",
    "     ('week', 44), ('of', 43), ('be', 39), ('course', 38), ('you', 38)])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hapax\n",
    "\n",
    "- Write a function that finds all hapexes in a text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "12b4444d65192247304560b057919484",
     "grade": false,
     "grade_id": "find_hapaxes_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_hapaxes(words):\n",
    "    '''\n",
    "    Finds hapexes in \"words\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    words: A list of strings.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return hapaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "aa6278b0d73f6e918c8d532b4740819a",
     "grade": false,
     "grade_id": "find_hapaxes_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "syllabus_hapaxes = find_hapaxes(syllabus_words)\n",
    "print(sorted(syllabus_hapaxes)[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e6f33f92c1906d4814d0b0ce66112f32",
     "grade": true,
     "grade_id": "find_hapaxes_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(syllabus_hapaxes, list)\n",
    "assert_true(all(isinstance(w, str) for w in syllabus_hapaxes))\n",
    "assert_equal(len(syllabus_hapaxes), 388)\n",
    "assert_equal(\n",
    "    sorted(syllabus_hapaxes)[-10:],\n",
    "    ['why', 'willing', 'wondering', 'working', 'worth',\n",
    "     'would', 'writing', 'www', 'yourself', 'zero']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK corpus\n",
    "\n",
    "In the second half of the problem, we use the NLTK Reuters corpus. See the [NLTK docs](http://www.nltk.org/book/ch02.html#reuters-corpus) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "2cd30c8732682eaf6565fb9504a2ee3a",
     "grade": false,
     "grade_id": "import_reuters",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical diversity in corpus\n",
    "\n",
    "- Compute the the number of token, number of words, and lexical diversity. Use the `words()` function of the reuters object, which includes non-alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "0521d4010c18311c05037d9e56bc75ec",
     "grade": false,
     "grade_id": "count_corpus_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def count_corpus(corpus):\n",
    "    '''\n",
    "    Computes the the number of token, number of words, and lexical diversity.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An NLTK corpus.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A 3-tuple of (num_tokens, num_words, lex_div)\n",
    "    num_tokens: An int. The number of tokens in \"words\".\n",
    "    num_words: An int. The number of words in \"words\".\n",
    "    lex_div: A float. The lexical diversity of \"words\".\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return num_words, num_tokens, lex_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "13d5ec1770367e7efc73b80ec73df38a",
     "grade": false,
     "grade_id": "count_corpus_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "num_words, num_tokens, lex_div = count_corpus(reuters)\n",
    "print(\"The Reuters corpus has {0} tokens and {1} words for a lexical diversity of {2:4.3f}\"\n",
    "      \"\".format(num_tokens, num_words, lex_div))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "1f2605b8c9b67bdb7df51e0e95a9a535",
     "grade": true,
     "grade_id": "count_corpus_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(num_tokens, int)\n",
    "assert_is_instance(num_words, int)\n",
    "assert_is_instance(lex_div, float)\n",
    "assert_equal(num_tokens, 41600)\n",
    "assert_equal(num_words, 1720901)\n",
    "assert_almost_equal(lex_div, 41.3678125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long words\n",
    "\n",
    "- Search for all words in corpus that are longer than 20 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "0a9860d83e53d48c08aecb73ca552ed0",
     "grade": false,
     "grade_id": "get_long_words_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_long_words(corpus, length=20):\n",
    "    '''\n",
    "    Finds all words in \"corpus\" longer than \"length\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus: An NLTK corpus.\n",
    "    length: An int. Default: 22\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of strings.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return long_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7f8b9a9c147d66772deada4a0cafa25a",
     "grade": false,
     "grade_id": "get_long_words_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "long_words = get_long_words(reuters, length=20)\n",
    "print(long_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "d19094b789a8656893e56e6a4e5667f7",
     "grade": true,
     "grade_id": "get_long_words_test",
     "locked": true,
     "points": 4,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(long_words, list)\n",
    "assert_true(all(isinstance(w, str) for w in long_words))    \n",
    "assert_equal(len(long_words), 5)\n",
    "assert_equal(\n",
    "    set(long_words),\n",
    "    set([\n",
    "        'discontinuedoperations',\n",
    "        'Warenhandelsgesellschaft',\n",
    "        'Gloeielampenfabrieken',\n",
    "        'Beteiligungsgesellschaft',\n",
    "        '..........................................'\n",
    "        ])\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
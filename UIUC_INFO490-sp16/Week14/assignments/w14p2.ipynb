{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "5e77c665676065fe1f5532f0b897b37c",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "This notebook will be collected automatically at **6pm on Monday** from `/home/data_scientist/assignments/Week14` directory on the course JupyterHub server. If you work on this assignment on the course Jupyterhub server, just make sure that you save your work and instructors will pull your notebooks automatically after the deadline. If you work on this assignment locally, the only way to submit assignments is via Jupyterhub, and you have to place the notebook file in the correct directory with the correct file name before the deadline.\n",
    "\n",
    "1. Make sure everything runs as expected. First, restart the kernel (in the menubar, select `Kernel` → `Restart`) and then run all cells (in the menubar, select `Cell` → `Run All`).\n",
    "2. Make sure you fill in any place that says `YOUR CODE HERE`. Do not write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed by the autograder.\n",
    "3. Do not change the file path or the file name of this notebook.\n",
    "4. Make sure that you save your work (in the menubar, select `File` → `Save and CheckPoint`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3c6835a355688b74636d278d9b4583a7",
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Problem 14.2. Spark DataFrames\n",
    "\n",
    "In this problem, we will use the Spark DataFrame to perform basic data processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "09582070780b81c48354fa0bd6a499a0",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType, StringType\n",
    "import pandas as pd\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_instance\n",
    "from pandas.util.testing import assert_frame_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e1f5217e4ddd480f583703de23206888",
     "grade": false,
     "grade_id": "markdown_1",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We run Spark in [local mode](http://spark.apache.org/docs/latest/programming-guide.html#local-vs-cluster-modes) from within our Docker container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "b89f31e49fc9c87324f97bb9c08b5683",
     "grade": false,
     "grade_id": "sparkcontext",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "21fea74682b168caee685771a8b3a4d1",
     "grade": false,
     "grade_id": "markdown_2",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We create a new RDD by reading in the data as a textfile. We use the ratings data from [MovieLens](http://grouplens.org/datasets/movielens/latest/). See [Week 6 Lesson 1](https://github.com/UI-DataScience/info490-sp16/blob/master/Week6/notebooks/intro2rs.ipynb) for more information on this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "21029810e4e5ce27067647f4aebddac2",
     "grade": false,
     "grade_id": "text_file",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "csv_path = '/home/data_scientist/data/ml-latest-small/ratings.csv'\n",
    "text_file = sc.textFile(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "c32f920d678f79b86f3e96c67162fbec",
     "grade": false,
     "grade_id": "markdown_3",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- Write a function that creates a Spark DataFrame from `text_file`. For example, running\n",
    "\n",
    "```python\n",
    ">>> df = create_df(text_file)\n",
    ">>> df.show()\n",
    "```\n",
    "\n",
    "should give\n",
    "\n",
    "```\n",
    "+------+-------+------+----------+\n",
    "|userId|movieId|rating| timestamp|\n",
    "+------+-------+------+----------+\n",
    "|     1|     16|   4.0|1217897793|\n",
    "|     1|     24|   1.5|1217895807|\n",
    "|     1|     32|   4.0|1217896246|\n",
    "|     1|     47|   4.0|1217896556|\n",
    "|     1|     50|   4.0|1217896523|\n",
    "|     1|    110|   4.0|1217896150|\n",
    "|     1|    150|   3.0|1217895940|\n",
    "|     1|    161|   4.0|1217897864|\n",
    "|     1|    165|   3.0|1217897135|\n",
    "|     1|    204|   0.5|1217895786|\n",
    "|     1|    223|   4.0|1217897795|\n",
    "|     1|    256|   0.5|1217895764|\n",
    "|     1|    260|   4.5|1217895864|\n",
    "|     1|    261|   1.5|1217895750|\n",
    "|     1|    277|   0.5|1217895772|\n",
    "|     1|    296|   4.0|1217896125|\n",
    "|     1|    318|   4.0|1217895860|\n",
    "|     1|    349|   4.5|1217897058|\n",
    "|     1|    356|   3.0|1217896231|\n",
    "|     1|    377|   2.5|1217896373|\n",
    "+------+-------+------+----------+\n",
    "only showing top 20 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "6ffbe4a649d91a4fab5a505aee4737fe",
     "grade": false,
     "grade_id": "create_df_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def create_df(rdd):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    rdd: A pyspark.rdd.RDD instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.sql.dataframe.DataFrame instance.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "8e2ab8d3255817c670ecd5134912e71c",
     "grade": false,
     "grade_id": "create_df_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = create_df(text_file)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "9c9361a43a80fa08aefe297a82701d15",
     "grade": true,
     "grade_id": "create_df_test",
     "locked": true,
     "points": 20,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(df, pyspark.sql.dataframe.DataFrame)\n",
    "\n",
    "# we the Spark dataframe to Pandas dataframe\n",
    "df_pd = pd.read_csv(csv_path)\n",
    "assert_frame_equal(df.toPandas(), df_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "903116df67525d9a9e9f183b1c0a442b",
     "grade": false,
     "grade_id": "markdown_4",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- Select from the Spark DataFrame only the rows whose rating is greater than 3.\n",
    "- After filtering, return only two columns: `movieId` and `rating`.\n",
    "\n",
    "```python\n",
    ">>> favorable = filter_favorable_ratings(df)\n",
    ">>> favorable.show()\n",
    "```\n",
    "\n",
    "```\n",
    "+-------+------+\n",
    "|movieId|rating|\n",
    "+-------+------+\n",
    "|     16|   4.0|\n",
    "|     32|   4.0|\n",
    "|     47|   4.0|\n",
    "|     50|   4.0|\n",
    "|    110|   4.0|\n",
    "|    161|   4.0|\n",
    "|    223|   4.0|\n",
    "|    260|   4.5|\n",
    "|    296|   4.0|\n",
    "|    318|   4.0|\n",
    "|    349|   4.5|\n",
    "|    457|   4.0|\n",
    "|    480|   3.5|\n",
    "|    527|   4.5|\n",
    "|    589|   3.5|\n",
    "|    590|   3.5|\n",
    "|    593|   5.0|\n",
    "|    608|   3.5|\n",
    "|    648|   3.5|\n",
    "|    724|   3.5|\n",
    "+-------+------+\n",
    "only showing top 20 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "6c2d97b1a17eec7e5b7318875f08fe2f",
     "grade": false,
     "grade_id": "filter_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def filter_favorable_ratings(df):\n",
    "    '''\n",
    "    Selects rows whose rating is greater than 3.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    A pyspark.sql.dataframe.DataFrame instance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A pyspark.sql.dataframe.DataFrame instance.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f7295539d1855ea349884af46f089a02",
     "grade": false,
     "grade_id": "filter_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "favorable = filter_favorable_ratings(df)\n",
    "favorable.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7e26263a347e8fe5e1747508c428829e",
     "grade": true,
     "grade_id": "filter_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(favorable, pyspark.sql.dataframe.DataFrame)\n",
    "\n",
    "favorable_pd = df_pd.loc[df_pd['rating'] > 3.0, ['movieId', 'rating']].reset_index(drop=True)\n",
    "assert_frame_equal(favorable.toPandas(), favorable_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "02c1c4f8f323bec7848c084876e5ce8f",
     "grade": false,
     "grade_id": "markdown_5",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "- Write a function that, given a `movieId`, computes the number of reviews for that movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "6c2a68a4f9ffff4b9e5e04c159371723",
     "grade": false,
     "grade_id": "find_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_n_reviews(df, movie_id):\n",
    "    '''\n",
    "    Finds the number of reviews for a movie.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    movie_id: An int.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return n_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "a07a035e0a6a7f3bb4a28f8698b98eb0",
     "grade": false,
     "grade_id": "find_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "n_toy_story = find_n_reviews(favorable, 1)\n",
    "print(n_toy_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "4a941b31fa20c3e929d1dc420ede9cd1",
     "grade": true,
     "grade_id": "find_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(n_toy_story, int)\n",
    "\n",
    "test = [find_n_reviews(favorable, n) for n in range(1, 6)]\n",
    "test_pd = favorable_pd.groupby('movieId').size()[:5].tolist()\n",
    "assert_equal(test, test_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1f1cbc95d6cc8f4d200a9e134233cb69",
     "grade": false,
     "grade_id": "markdown_6",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "We must stop the SparkContext in order to release the spark resources before existing this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "ec8c059e7ca934a0d1fb3342fc042680",
     "grade": false,
     "grade_id": "sc_stop",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

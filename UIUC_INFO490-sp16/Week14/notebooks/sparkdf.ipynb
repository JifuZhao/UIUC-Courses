{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<DIV ALIGN=CENTER>\n",
    "\n",
    "# Introduction to Spark: DataFrames\n",
    "## Professor Robert J. Brunner\n",
    "  \n",
    "</DIV>  \n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this IPython Notebook, we explore using Spark to perform data\n",
    "processing in a similar manner to our previous efforts with Pandas. For\n",
    "this we will use the airline data, which has been stored within a\n",
    "filesystem that is accessible from within our Spark cluster. We first\n",
    "initialize our spark environment, which in this Notebook is slightly\n",
    "different since we will connect our Spark environment to our Cassandra\n",
    "database. This requires additional Java libraries to be acquired and\n",
    "installed into the Spark environment, which will cause the Spark Context\n",
    "creation to take longer (so be patient).\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup pySpark to be able to work with Cassandra\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \\\n",
    "    '--packages TargetHolding:pyspark-cassandra:0.3.5 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark version: 1.6.0\n"
     ]
    }
   ],
   "source": [
    "# We release the SparkContext if it exists.\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    pass ;\n",
    "else:\n",
    "    sc.stop()\n",
    "\n",
    "# Now handle initial import statements\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# Create new Spark Configuration \n",
    "# Also set Cassandra host ip\n",
    "myconf = SparkConf()\n",
    "myconf.setMaster('local[*]')\n",
    "myconf.setAppName(\"INFO490 SP16 W14-NB2: Professor Brunner\")\n",
    "myconf.set('spark.executor.memory', '1g')\n",
    "myconf.set('spark.cassandra.connection.host', '40.124.12.119')\n",
    "\n",
    "# Create and initialize a new Spark Context\n",
    "sc = SparkContext(conf=myconf)\n",
    "\n",
    "# Display Spark version information, which also verifies SparkContext is active\n",
    "print(\"\\nSpark version: {0}\".format(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "In this Notebook, we will need sample data. To simplify acquiring data\n",
    "to demonstrate using Spark DataFrames, we include the RDD code from the\n",
    "[Introduction to Spark](intro2spark.ipynb) Notebook in the following\n",
    "cell.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in fields RDD = 480106\n"
     ]
    }
   ],
   "source": [
    "filename = '/home/data_scientist/data/2001/2001-1.csv'\n",
    "\n",
    "text_file = sc.textFile(filename)\n",
    "\n",
    "col_data = text_file.map(lambda l: l.split(\",\")) \\\n",
    "            .map(lambda p: (p[0], p[1], p[2], p[4], p[14], p[15], p[16], p[17], p[18])) \\\n",
    "            .filter(lambda line: 'Year' not in line)\n",
    "\n",
    "cols = col_data.filter(lambda line: 'NA' not in line)\n",
    "\n",
    "fields = cols.map(lambda p: (int(p[0]), int(p[1]), int(p[2]), int(p[3]),\n",
    "                          int(p[4]), int(p[5]), p[6], p[7], int(p[8])))\n",
    "\n",
    "# Should be 480106 if everything works correctly\n",
    "print('Number of entries in fields RDD = {0}'.format(fields.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Spark DataFrame\n",
    "\n",
    "Spark supports a simplified [Data Frame][spdf] as part of the [Spark\n",
    "SQL][spsql] library. We can create a Data Frame from an existing RDD by\n",
    "also specifying the column labels and data types. The data types must\n",
    "be one of the pre-defined [Spark SQL types][spdt]. After creating the\n",
    "new DataFrame (which is backed by an RDD), we can perform many of the\n",
    "same tasks with Spark that we performed with Pandas (but not all, and\n",
    "not in as simple of an approach). The following code cells show how we\n",
    "can take our 2001 flight data RDD and create a new Data Frame, which we\n",
    "subsequently use in several subsequent code cells.\n",
    "\n",
    "-----\n",
    "[spdf]: https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes\n",
    "[spsql]: https://spark.apache.org/sql/\n",
    "[spdt]: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# sc is an existing SparkContext.\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "schemaString = \"Year Month DayOfMonth DepTime ArrDelay DepDelay Origin Destination Distance\"\n",
    "\n",
    "fieldTypes = [IntegerType(), IntegerType(), IntegerType(), \\\n",
    "              IntegerType(), IntegerType(), IntegerType(), \\\n",
    "              StringType(), StringType(), IntegerType()]\n",
    "\n",
    "f_data = [StructField(field_name, field_type, True) \\\n",
    "          for field_name, field_type in zip(schemaString.split(), fieldTypes)]\n",
    "\n",
    "schema = StructType(f_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Year: int, Month: int, DayOfMonth: int, DepTime: int, ArrDelay: int, DepDelay: int, Origin: string, Destination: string, Distance: int]\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(fields, schema)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "In the following three code cells, we `show` the first few lines of the\n",
    "DataFrame, then use the `head` method, which displays more syntactic\n",
    "information for each row, and finally use the `describe` method, which\n",
    "doesn't execute until the `show` action is invoked. While the output is\n",
    "less visually attractive than the Pandas result, we still obtain the\n",
    "necessary information.\n",
    "\n",
    "After these code cells, we access the DataFrame schema, first by using\n",
    "the `printSchema` method to nicely output the schema, and next access a\n",
    "column directly, which we can now do since we have named our DataFrame\n",
    "columns.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "|Year|Month|DayOfMonth|DepTime|ArrDelay|DepDelay|Origin|Destination|Distance|\n",
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "|2001|    1|        17|   1806|      -3|      -4|   BWI|        CLT|     361|\n",
      "|2001|    1|        18|   1805|       4|      -5|   BWI|        CLT|     361|\n",
      "|2001|    1|        19|   1821|      23|      11|   BWI|        CLT|     361|\n",
      "|2001|    1|        20|   1807|      10|      -3|   BWI|        CLT|     361|\n",
      "|2001|    1|        21|   1810|      20|       0|   BWI|        CLT|     361|\n",
      "+----+-----+----------+-------+--------+--------+------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Year=2001, Month=1, DayOfMonth=17, DepTime=1806, ArrDelay=-3, DepDelay=-4, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=18, DepTime=1805, ArrDelay=4, DepDelay=-5, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=19, DepTime=1821, ArrDelay=23, DepDelay=11, Origin='BWI', Destination='CLT', Distance=361),\n",
       " Row(Year=2001, Month=1, DayOfMonth=20, DepTime=1807, ArrDelay=10, DepDelay=-3, Origin='BWI', Destination='CLT', Distance=361)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+-----------------+------------------+-----------------+-----------------+-----------------+\n",
      "|summary|                Year| Month|       DayOfMonth|           DepTime|         ArrDelay|         DepDelay|         Distance|\n",
      "+-------+--------------------+------+-----------------+------------------+-----------------+-----------------+-----------------+\n",
      "|  count|              480106|480106|           480106|            480106|           480106|           480106|           480106|\n",
      "|   mean|              2001.0|   1.0|16.01370530674476| 1359.660206287778|6.382288494624103|8.781523246949632|716.9933556339641|\n",
      "| stddev|1.065161224168148...|   0.0|  8.9369643824565|487.23695943583846|31.04865060768917|27.96630068676185|568.6557196351711|\n",
      "|    min|                2001|     1|                1|                 1|              -80|              -59|               21|\n",
      "|    max|                2001|     1|               31|              2400|             1688|             1692|             4962|\n",
      "+-------+--------------------+------+-----------------+------------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayOfMonth: integer (nullable = true)\n",
      " |-- DepTime: integer (nullable = true)\n",
      " |-- ArrDelay: integer (nullable = true)\n",
      " |-- DepDelay: integer (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Destination: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'Year'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "We can extract data from the DataFrame by using similar techniques to\n",
    "what we used with Pandas. One difference is that we need to `filter` the\n",
    "DataFrame, as opposed to directly access rows. However, we can filter\n",
    "rows to extract flights that left O'Hare, and secondly those flights\n",
    "that left O'Hare more than two hours late. In the second case, we also\n",
    "tranform the output to `select` the _Destination_ column and a new\n",
    "column that is the _Distance_ in kilometers.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27455"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['Origin'] == 'ORD').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|Destination| (Distance * 1.6)|\n",
      "+-----------+-----------------+\n",
      "|        PHL|           1084.8|\n",
      "|        CLT|958.4000000000001|\n",
      "|        MEM|            785.6|\n",
      "|        MEM|            785.6|\n",
      "|        MEM|            785.6|\n",
      "|        STL|            412.8|\n",
      "|        STL|            412.8|\n",
      "|        PVD|           1358.4|\n",
      "|        LAX|           2792.0|\n",
      "|        LAX|           2792.0|\n",
      "+-----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Origin'] == 'ORD').filter(df['DepDelay'] > 120).select(df['Destination'], df['Distance'] * 1.6).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Spark SQL\n",
    "\n",
    "Given a Spark DataFrame, we can apply SQL statements directly against\n",
    "the DataFrame by registering the DataFrame as a Spark temporary SQL\n",
    "table. The following code cells demonstrates this, as we register our\n",
    "DataFrame as a `flights` table, and execute a SQL statement to select\n",
    "the same data we obtained from our previous DataFrame filter.Since the\n",
    "data are unordered, we have different results displayed via the `show`\n",
    "method.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+\n",
      "|Destination|Distance|\n",
      "+-----------+--------+\n",
      "|        PHL|     678|\n",
      "|        CLT|     599|\n",
      "|        MEM|     491|\n",
      "|        MEM|     491|\n",
      "|        MEM|     491|\n",
      "|        STL|     258|\n",
      "|        STL|     258|\n",
      "|        PVD|     849|\n",
      "|        LAX|    1745|\n",
      "|        LAX|    1745|\n",
      "+-----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.createDataFrame(fields, schema)\n",
    "\n",
    "df.registerTempTable(\"flights\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "sql_q = \"SELECT Destination, Distance FROM flights WHERE Origin = 'ORD' AND DepDelay > 120\"\n",
    "\n",
    "results = sqlContext.sql(sql_q)\n",
    "\n",
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "results.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Cassandra Query\n",
    "\n",
    "We now connect to a remote database from a Spark application. In this\n",
    "case, we will use our existing Cassandra instance running on Microsoft\n",
    "Azure. We have already initialized the Spark context to acquire and\n",
    "install the spark-cassandra connector in the first code cell int his\n",
    "Notebook and we specified the host ip address for our Cassandra database\n",
    "as part of the Spark Context configuration parameters in the second code\n",
    "cell. Our next step is to establish a connection to a Cassandra keyspace\n",
    "and read data from a table into a Spark RDD. \n",
    "\n",
    "This last step is performed in the following cell. We first tell Spark\n",
    "to use the spark-cassandra driver, which will run in the Spark JVM, to\n",
    "connect to the database. Next, we load the `airlines` table from the\n",
    "`bigdog` keyspace in our Cassandra database. This table was created by\n",
    "using the following CQL query:\n",
    "\n",
    "```cql\n",
    "drop_schema = '''\n",
    "DROP TABLE IF EXISTS Airlines ;\n",
    "'''\n",
    "\n",
    "create_schema = '''\n",
    "CREATE TABLE Airlines (\n",
    "    Year int,\n",
    "    Month int,\n",
    "    DayOfMonth int,\n",
    "    DepTime int,\n",
    "    ArrDelay int,\n",
    "    DepDelay int,\n",
    "    Origin text,\n",
    "    Destination text,\n",
    "    Distance int,\n",
    "    PRIMARY KEY(Month, DayOfMonth, DepTime, Origin)\n",
    ");\n",
    "'''\n",
    "```\n",
    "\n",
    "The 2001 flight data we have analyzed previously in this Notebook has\n",
    "already been loaded into this table by using the following Python code:\n",
    "\n",
    "```python\n",
    "df.write.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "    options(table='airlines', keyspace='bigdog').save(mode=\"overwrite\")\n",
    "```\n",
    "\n",
    "One change from the previous Spark DataFrame used in this Notebook is\n",
    "the creation of the Column names (since CQL is case insensitive, while\n",
    "Spark is case sensitive), for the creation of this table, df was created\n",
    "with the following column names:\n",
    "\n",
    "```python\n",
    "schemaString = \"year month dayofmonth deptime arrdelay depdelay origin destination distance\"\n",
    "```\n",
    "\n",
    "After we load the data into the new flights RDD, we display the first\n",
    "few rows, and the column datatypes.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flights = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\\\n",
    "               load(keyspace=\"bigdog\", table=\"airlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+------+--------+--------+-----------+--------+----+\n",
      "|month|dayofmonth|deptime|origin|arrdelay|depdelay|destination|distance|year|\n",
      "+-----+----------+-------+------+--------+--------+-----------+--------+----+\n",
      "|    1|         1|      1|   ANC|     -13|      -4|        LAX|    2345|2001|\n",
      "|    1|         1|      1|   LAS|      33|      41|        PHL|    2176|2001|\n",
      "|    1|         1|      1|   MCI|      14|      11|        MDW|     405|2001|\n",
      "|    1|         1|      1|   RIC|       1|       2|        ORF|      75|2001|\n",
      "|    1|         1|      2|   DTW|      22|      52|        LAX|    1979|2001|\n",
      "+-----+----------+-------+------+--------+--------+-----------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('month', 'int'),\n",
       " ('dayofmonth', 'int'),\n",
       " ('deptime', 'int'),\n",
       " ('origin', 'string'),\n",
       " ('arrdelay', 'int'),\n",
       " ('depdelay', 'int'),\n",
       " ('destination', 'string'),\n",
       " ('distance', 'int'),\n",
       " ('year', 'int')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "Given this Spark RDD, we can now perform subsequent operations as\n",
    "demonstrated in the [Introduction to Spark][intro2spark.ipynb] Notebook.\n",
    "Below, we apply several filters to the RDD to generate a subset of the\n",
    "full data. In this case, we select long flights from the Baltimore\n",
    "Washington International airport. Another option would be to create a\n",
    "DataFrame from this RDD and use the Spark DataFrame techniques presented\n",
    "earlier in this Notebook.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bwi = flights.filter(flights.origin == 'BWI').filter(flights.distance > 1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+------+--------+--------+-----------+--------+----+\n",
      "|month|dayofmonth|deptime|origin|arrdelay|depdelay|destination|distance|year|\n",
      "+-----+----------+-------+------+--------+--------+-----------+--------+----+\n",
      "|    1|         1|    655|   BWI|     -37|      -5|        SFO|    2457|2001|\n",
      "|    1|         1|    658|   BWI|     -40|      -2|        SFO|    2457|2001|\n",
      "|    1|         1|    715|   BWI|     -25|       0|        LAS|    2106|2001|\n",
      "|    1|         1|    731|   BWI|     -36|      -4|        LAX|    2329|2001|\n",
      "|    1|         1|    751|   BWI|      12|      19|        PHX|    1999|2001|\n",
      "+-----+----------+-------+------+--------+--------+-----------+--------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bwi.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "### Student Activity\n",
    "\n",
    "In the preceding cells, we introduced Spark DataFrames and Spark SQL.\n",
    "Now that you have run the Notebook, go back and make the following\n",
    "changes to see how the results change.\n",
    "\n",
    "1. Change the DataFrame to include different columns from the flights\n",
    "data. You might review the original [airline data\n",
    "set](http://stat-computing.org/dataexpo/2009/) website to see the column\n",
    "descriptions.\n",
    "\n",
    "2. Use a SQL query on the `df` DataFrame to compute the mean distance\n",
    "between all flights from O'Hare to Los Angeles International Airport\n",
    "(LAX).\n",
    "\n",
    "4. Add an index column to this Spark DataFrame, which sequentially\n",
    "increases.\n",
    "\n",
    "Additional, more advanced problems:\n",
    "\n",
    "1. Turn the Cassandra SQL RDD obtained previously in this Notebook into\n",
    "a Spark DataFrame and output the results of the `describe` function on\n",
    "all numeric columns.\n",
    "\n",
    "2. Turn this Spark DataFrame into a Pandas DataFrame and make a\n",
    "regression plot of the arrival delay versus the departure delay by using\n",
    "Seaborn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "### Ending the Spark Session\n",
    "\n",
    "We must stop the `SparkContext` in order to release resources on the\n",
    "instructional cluster before existing this Notebook.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

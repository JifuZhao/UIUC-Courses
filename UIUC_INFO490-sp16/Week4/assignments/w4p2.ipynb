{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e86311be21f81c722c21243bd984de5f",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "This notebook will be collected automatically at **6pm on Monday** from `/home/data_scientist/assignments/Week4` directory on the course JupyterHub server. If you work on this assignment on the course Jupyterhub server, just make sure that you save your work and instructors will pull your notebooks automatically after the deadline. If you work on this assignment locally, the only way to submit assignments is via Jupyterhub, and you have to place the notebook file in the correct directory with the correct file name before the deadline.\n",
    "\n",
    "1. Make sure everything runs as expected. First, restart the kernel (in the menubar, select `Kernel` → `Restart`) and then run all cells (in the menubar, select `Cell` → `Run All`).\n",
    "2. Make sure you fill in any place that says `YOUR CODE HERE`. Do not write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed by the autograder.\n",
    "3. Do not change the file path or the file name of this notebook.\n",
    "4. Make sure that you save your work (in the menubar, select `File` → `Save and CheckPoint`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4.2. Random Forests\n",
    "\n",
    "In this problem, we will use the Random Forests classifier to see if we can use machine learning techniques to predict departure delays at the O'Hare airport (ORD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "4bff09e4d9d0d6370205d83b12829a63",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import train_test_split, KFold\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.metrics import log_loss, roc_curve, roc_auc_score, auc\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_instance, assert_is_not, assert_in\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal\n",
    "from pandas.util.testing import assert_frame_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Problem 4.1](https://github.com/UI-DataScience/info490-sp16/blob/master/Week4/assignments/w4p1.ipynb), we had a data frame with weather information. I dumped the `local_visi` data frame from the previous problem to create a CSV file. We will use this file to prepare our data set.\n",
    "\n",
    "```python\n",
    ">>> print(local_visi.head())\n",
    "```\n",
    "\n",
    "```\n",
    "        Month  DayofMonth  CRSDepTime  Delayed  Visibility\n",
    "398444      1           1        1905        1          10\n",
    "398445      1           2        1905        1           9\n",
    "398446      1           3        1905        1           5\n",
    "398447      1           4        1905        0           8\n",
    "398448      1           5        1905        1          10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "f53ee9bf1e1af1112475ae5fb19e51e4",
     "grade": false,
     "grade_id": "local_visi",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "local_visi = pd.read_csv('/home/data_scientist/data/weather/local_visi.csv', index_col=0)\n",
    "print(local_visi.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Random Forests classifier, we will add more columns than what we used in Problem 4.1 (i.e., the columns in `local_visi`), so let's import the following columns from `2001.csv`.\n",
    "\n",
    "- Column 1: Month, 1-12\n",
    "- Column 2: DayofMonth, 1-31\n",
    "- Column 3: DayOfWeek, 1 (Monday) - 7 (Sunday)\n",
    "- Column 5: CRSDepTime, scheduled departure time (local, hhmm)\n",
    "- Column 7: CRSArrTime, scheduled arrival time (local, hhmm)\n",
    "- Column 18: Distance, in miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "493f81468807fe7d23989d22ef38ccd5",
     "grade": false,
     "grade_id": "read_csv",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    '/home/data_scientist/data/2001.csv',\n",
    "    encoding='latin-1',\n",
    "    usecols=(1, 2, 3, 5, 7, 18)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the index of `local_visi` to extract the rows we need.\n",
    "\n",
    "```python\n",
    ">>> print(local.head())\n",
    "```\n",
    "\n",
    "```\n",
    "   Month  DayofMonth  DayOfWeek  CRSDepTime  CRSArrTime  Distance  Visibility  \\\n",
    "0      1           1          1        1905        2159      1846          10   \n",
    "1      1           2          2        1905        2159      1846           9   \n",
    "2      1           3          3        1905        2159      1846           5   \n",
    "3      1           4          4        1905        2159      1846           8   \n",
    "4      1           5          5        1905        2159      1846          10   \n",
    "\n",
    "   Delayed  \n",
    "0        1  \n",
    "1        1  \n",
    "2        1  \n",
    "3        0  \n",
    "4        1  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "78b1e88c120c618e5de8bc2655ea9726",
     "grade": false,
     "grade_id": "local",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = df.loc[local_visi.index, :]\n",
    "df.loc[local_visi.index, 'Visibility'] = local_visi['Visibility']\n",
    "df.loc[local_visi.index, 'Delayed'] = local_visi['Delayed']\n",
    "local = df.reset_index(drop=True) # easier to reset the index in this problem\n",
    "\n",
    "print(local.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split\n",
    "\n",
    "This function is the same function from [Problem 3.1](https://github.com/UI-DataScience/info490-sp16/blob/master/Week3/assignments/w3p1.ipynb). You can copy-paste your answer. I'll try not to make you write this again in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "44c0df76f1074c13ffd6881436ca5487",
     "grade": false,
     "grade_id": "split_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def split(df, test_column, test_size, random_state):\n",
    "    '''\n",
    "    Uses sklearn.train_test_split to split \"df\" into a testing set and a test set.\n",
    "    The \"test_columns\" lists the column that we are trying to predict.\n",
    "    All columns in \"df\" except \"test_columns\" will be used for training.\n",
    "    The \"test_size\" should be between 0.0 and 1.0 and represents the proportion of the\n",
    "    dataset to include in the test split.\n",
    "    The \"random_state\" parameter is used in sklearn.train_test_split.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: A pandas.DataFrame\n",
    "    test_columns: A list of strings\n",
    "    test_size: A float\n",
    "    random_state: A numpy.random.RandomState instance\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A 4-tuple of pandas.DataFrames\n",
    "    '''\n",
    "    ##################\n",
    "    # YOUR CODE HERE\n",
    "    ##################\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split `local_visi` into 80:20 training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "bda3d5b43f6794e3a8b892664967abe1",
     "grade": false,
     "grade_id": "split_run_test",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split(\n",
    "    df=df,\n",
    "    test_column=['Delayed'],\n",
    "    test_size=0.2,\n",
    "    random_state=check_random_state(0)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use sklearn.KFold\n",
    "\n",
    "In the previous problems, we used a validation set to adjust the hyperparameters. The <a href=\"https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation\">k-fold cross-validation</a> technique extends the idea of validation set. You can read more about $k$-fold CV on Wikipedia or [scikit-learn docs](http://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "We are going to use $k$-fold CV to\n",
    "\n",
    "1. measure the performance increase with increasing number of trees, and\n",
    "2. optimize one of the hyperparameters, `max_features`.\n",
    "\n",
    "I'm going to break the process into small steps, but in practice it might be better to combine all the steps and write everything into one or two functions.\n",
    "\n",
    "- Use [sklearn.cross_validation.KFold()](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.KFold.html) to write a function named `get_cv_indices()` that takes a data frame and returns a $k$-fold cross-validation iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e95bb52ad4e34614ac0a950bc5d837d2",
     "grade": false,
     "grade_id": "get_cv_indices_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_cv_indices(df, n_folds, random_state):\n",
    "    '''\n",
    "    Provides train/test indices to split data in train test sets.\n",
    "    Split dataset into \"n_folds\" consecutive folds (no shuffling).\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    df: A pandas.DataFrame\n",
    "    n_folds: integer\n",
    "    random_state: A numpy.random.RandomState instance\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    An sklearn.cross_validation.KFold instance.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5fede9bfc6cc7338b411487855fe838c",
     "grade": true,
     "grade_id": "get_cv_indices_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_kf = get_cv_indices(\n",
    "    pd.DataFrame({'a': np.arange(1000)}), 10, check_random_state(1)\n",
    "    )\n",
    "assert_is_instance(test_kf, KFold)\n",
    "assert_equal(test_kf.n, 1000)\n",
    "assert_equal(test_kf.n_folds, 10)\n",
    "assert_array_equal(test_kf.random_state.choice(100, 5), check_random_state(1).choice(100, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "We are going to use the Random Forest classifier and adjust two hyperparameters, `n_estimators` and `max_features`. You should use default values for all other parameters.\n",
    "\n",
    "`n_estimators` represents the number of trees in the forest, and the default value is 10, which is usually too small for big data sets. It is somewhat trivial to find the best value, because the classifer performance usually increases with the number of trees. So you should have as many trees as possible, but only up to a point because you get diminishing returns once you have enough trees.\n",
    "\n",
    "`max_features` is the number of features that are considered for each split. In my experience, this is the most important parameter. The rule of thumb that gets thrown around a lot is the square root of total number of features, but it's best to fine tune this parameter.\n",
    "\n",
    "- Write a function named `get_rfc()` that accepts two hyperparameters, `n_estimators` and `max_featurs` (don't forget the `random_state`). It returns an `sklearn.RandomForestClassifier` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "b4025251755ffc0e72e5b370ed07de12",
     "grade": false,
     "grade_id": "get_rfc_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_rfc(n_estimators, max_features, random_state):\n",
    "    '''\n",
    "    A random forest classifier with two adjustable parameters:\n",
    "    \"n_estimators\" and \"max_features\".\n",
    "    Uses the default sklearn values for the remaining parameters.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_estimators: An int\n",
    "    max_features: An int\n",
    "    random_state: A numpy.random.RandomState instance\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An sklearn.ensemble.forest.RandomForestClassifier\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "214b96fd266cc107a2b305a4da93efa7",
     "grade": true,
     "grade_id": "get_rfc_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_rfc = get_rfc(100, 10, check_random_state(1))\n",
    "assert_is_instance(test_rfc, ensemble.RandomForestClassifier)\n",
    "assert_equal(test_rfc.n_estimators, 100)\n",
    "assert_equal(test_rfc.max_features, 10)\n",
    "assert_array_equal(test_rfc.random_state.choice(100, 5), check_random_state(1).choice(100, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and predict\n",
    "\n",
    "- Write a function named `get_proba()` that makes probabilistic predictions on the validation set.\n",
    "\n",
    "You should make **probabilistic** predictions. Use [predict_proba()](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba).\n",
    "\n",
    "The paremeters `idx_train` and `idx_valid` are what you would get from the `get_cv_indices()` iterator. For example, \n",
    "\n",
    "```python\n",
    ">>> example_kf = get_cv_indices(\n",
    "...     pd.DataFrame({'a': np.arange(20)}), 4, check_random_state(1)\n",
    "...     )\n",
    "\n",
    ">>> for idx_train, idx_valid in example_kf:\n",
    "...     print(idx_train, idx_valid)\n",
    "```\n",
    "\n",
    "```\n",
    "[ 5  6  7  8  9 10 11 12 13 14 15 16 17 18 19] [0 1 2 3 4]\n",
    "[ 0  1  2  3  4 10 11 12 13 14 15 16 17 18 19] [5 6 7 8 9]\n",
    "[ 0  1  2  3  4  5  6  7  8  9 15 16 17 18 19] [10 11 12 13 14]\n",
    "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14] [15 16 17 18 19]\n",
    "```\n",
    "\n",
    "And they are just numpy arrays:\n",
    "\n",
    "```python\n",
    ">>> print(type(idx_train))\n",
    "```\n",
    "\n",
    "```\n",
    "<class 'numpy.ndarray'>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "efef5479253a32c2b201ee0650a84178",
     "grade": false,
     "grade_id": "get_proba_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_proba(clf, X_train, y_train, idx_train, idx_valid):\n",
    "    '''\n",
    "    \n",
    "    Fits the \"clf\" model on X_train[idx_train] and y_train[idx_train].\n",
    "    Makes predictions on X_train[idx_valid].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    clf: An sklearn classifier instance.\n",
    "    X_train: A pandas.DataFrame\n",
    "    y_train: A pandas.DataFrame\n",
    "    idx_train: A numpy array\n",
    "    idx_valid: A numpy array\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A two-dimensional numpy array\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "450d148a659f42723d17635f31281454",
     "grade": true,
     "grade_id": "get_proba_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_X = pd.DataFrame({\n",
    "    'X0': np.arange(10),\n",
    "    'X1': np.arange(10),\n",
    "    'X2': np.arange(10)\n",
    "    })\n",
    "\n",
    "test_y = pd.DataFrame({\n",
    "    'y': np.arange(10)\n",
    "    })\n",
    "\n",
    "test_rfc = get_rfc(10, 2, check_random_state(2))\n",
    "\n",
    "test_proba = get_proba(test_rfc, test_X, test_y,\n",
    "    np.arange(10)[::2], np.arange(10)[::2] + 1)\n",
    "\n",
    "assert_array_equal(test_proba,\n",
    "    np.array(\n",
    "        [[ 0.6,  0.2,  0.1,  0.1,  0. ],\n",
    "         [ 0. ,  0.6,  0.3,  0.1,  0. ],\n",
    "         [ 0. ,  0.2,  0.5,  0.3,  0. ],\n",
    "         [ 0. ,  0.1,  0.1,  0.5,  0.3],\n",
    "         [ 0. ,  0.1,  0.1,  0.1,  0.7]])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area under the ROC curve\n",
    "\n",
    "- Write a function named `get_auc()` that uses the `get_proba()` function to train a Random Forest classifier (`rfc`) on a subset of X_train/y_train, makes predictions on the validation data, and returns a one-dimensional Numpy array.\n",
    "\n",
    "The $k$-th element in the array is the AUC score evaluated on the $k$-th cross-validation data. So if we use 10-fold cross-validation, we will get a Numpy array with length 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "ebda6124b39d7a598e6463097b004eb2",
     "grade": false,
     "grade_id": "get_auc_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_auc(kf, rfc, X_train, y_train):\n",
    "    '''\n",
    "    Iterates through the cross-validation folds and\n",
    "    returns the area under the ROC curve for the validation data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    kf: An sklearn.KFold instance\n",
    "    rfc: An sklearn.RandomForestClassifer instance\n",
    "    X_train: A pandas.DataFrame\n",
    "    y_train: A pandas.DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "7ec6194c3543b36a10acccb0b2fc8757",
     "grade": true,
     "grade_id": "get_auc_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_X = pd.DataFrame({\n",
    "    'X0': np.arange(100),\n",
    "    'X1': np.arange(100),\n",
    "    'X2': np.arange(100)\n",
    "    })\n",
    "\n",
    "test_y = pd.DataFrame({\n",
    "    'y': [0, 1] * 50\n",
    "    })\n",
    "\n",
    "test_kf = get_cv_indices(test_X, 10, check_random_state(1))\n",
    "\n",
    "test_rfc = get_rfc(10, 2, check_random_state(2))\n",
    "\n",
    "test_auc = get_auc(test_kf, test_rfc, test_X, test_y)\n",
    "\n",
    "assert_array_almost_equal(test_auc,\n",
    "    [0.5, 0.44, 0.44, 0.4, 0.4, 0.48, 0.48, 0.38, 0.4, 0.5]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The number of trees\n",
    "\n",
    "We will increase the number of trees from 10 to 100 and plot the relationship between the number of trees and the area under the ROC curve.\n",
    "\n",
    "You don't have to write code for this part. Simply run the following two code cells.\n",
    "\n",
    "```\n",
    "Trees   AUC\n",
    "------------\n",
    " 10    0.702\n",
    " 20    0.714\n",
    " 30    0.722\n",
    " 40    0.724\n",
    " 50    0.727\n",
    " 60    0.728\n",
    " 70    0.729\n",
    " 80    0.729\n",
    " 90    0.730\n",
    "100    0.731\n",
    "```\n",
    "\n",
    "As we increase the number of trees, we get better performance but diminishing returns. The gain in performance flattens out after about 100 trees.\n",
    "\n",
    "![](https://raw.githubusercontent.com/UI-DataScience/info490-sp16/master/Week4/assignments/images/roc_vs_trees.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "6ad92ec378a793645ee53d30a2875092",
     "grade": false,
     "grade_id": "print_trees_auc",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "print('Trees   AUC\\n'\n",
    "      '------------')\n",
    "\n",
    "for n_trees in range(10, 110, 10):\n",
    "    kf = get_cv_indices(X_train, 3, check_random_state(0))\n",
    "    rfc = get_rfc(n_trees, 3, check_random_state(0))\n",
    "    mean_auc = get_auc(kf, rfc, X_train, y_train).mean()\n",
    "    result.append((n_trees, mean_auc))\n",
    "    print('{0:3d}    {1:0.3f}'.format(n_trees, mean_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "fd62840246fd508b28bdd247fd617a57",
     "grade": false,
     "grade_id": "plot_trees_auc",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "result = np.array(result)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(result[:, 0], result[:, 1])\n",
    "ax.set_title('AUC ROC vs number of trees')\n",
    "ax.set_xlabel('Number of trees')\n",
    "ax.set_ylabel('AUC ROC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize number of features\n",
    "\n",
    "Vary the number of features at each split to find the best values of `max_features`. You should search from `max_features=1` to `max_features=7` inclusive.\n",
    "\n",
    "Use 4-fold cross-validation, `n_folds=4`. We will also use `n_trees=20` to save time. But we will eventually train with 100 trees in the end, so in practice you should find the optimal hypermater with 100 trees. \n",
    "\n",
    "We need two different `check_random_state()` instances, one for `get_cv_indices()` and another one for `get_rfc()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "40cadb9d266716b7a49daf236201db4d",
     "grade": false,
     "grade_id": "optimize_max_features_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def optimize_max_features(X_train, y_train, cv_random_state, clf_random_state, n_folds=4, n_trees=20):\n",
    "    '''\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A pandas.DataFrame\n",
    "    y_train: A pandas.DataFrame\n",
    "    cv_random_state: A RandomState instance for get_cv_indices()\n",
    "    clf_random_state: A RandomState instance for get_auc()\n",
    "    \n",
    "    Optional\n",
    "    --------\n",
    "    n_folds: An int. 4 by default.\n",
    "    n_trees: An int. 20 by default.\n",
    "    '''\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a short test in the following code cell to check if your function behaves correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "0e3f067ba62cbc598bb99c9a3ec0a913",
     "grade": true,
     "grade_id": "optimize_max_features_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "test_omf = optimize_max_features(X_train[:100], y_train[:100], check_random_state(0), check_random_state(0))\n",
    "assert_is_instance(test_omf, list)\n",
    "assert_array_almost_equal(test_omf,\n",
    "    [(1, 0.70678053830227749),\n",
    "     (2, 0.69804606625258803),\n",
    "     (3, 0.76811594202898559),\n",
    "     (4, 0.73162525879917184),\n",
    "     (5, 0.65430900621118016),\n",
    "     (6, 0.69998706004140798),\n",
    "     (7, 0.69306418219461707)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your function passes the above tests, run the following cell to compute average AUC for different values of `max_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "6436140e6a447c886a1b0842d2f8b795",
     "grade": false,
     "grade_id": "max_features",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "max_features = optimize_max_features(X_train, y_train, check_random_state(0), check_random_state(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our classifier performs best when we use `max_features=5`.\n",
    "\n",
    "```python\n",
    ">>> print('The best value for max_features is {}.'.format(best_max_features))\n",
    "```\n",
    "\n",
    "```\n",
    "The best value for max_features is 5.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "907735513b86f517ecc98493810565a5",
     "grade": false,
     "grade_id": "best_max_features",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "best_max_features = np.argmax(np.array(max_features)[:, 1]) + 1\n",
    "print('The best value for max_features is {}.'.format(best_max_features))\n",
    "assert_equal(best_max_features, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Random Forest model\n",
    "\n",
    "Now that we have optimized the `max_features` parameter, let's use this value to train a model with `max_features=5` and `n_trees=100`. Once we train the model on the entire training set, we will make probabilistic preditions for the test set.\n",
    "\n",
    "You should make **probabilistic** predictions. Use `predict_proba()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "d38303bbc20611ce3ea49dc56a2a968f",
     "grade": false,
     "grade_id": "get_final_rfc_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_final_rfc(X_train, y_train, X_test, max_features, random_state, n_trees=100):\n",
    "    '''\n",
    "    Trains a Random Forest classifier on the entire training set\n",
    "    using the optimized \"max_features\".\n",
    "    Predicts\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train: A pandas.DataFrame\n",
    "    y_train: A pandas.DataFrame\n",
    "    X_test: A pandas.DataFrame\n",
    "    max_features: An int\n",
    "    random_state: A RandomState instance\n",
    "    \n",
    "    Optional\n",
    "    --------\n",
    "    n_trees: An int. 100 by default\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A two-dimensional numpy array\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run `get_final_rfc()` in the following code cell, `y_pred` will be a two-dimensional array.\n",
    "\n",
    "```python\n",
    ">>> print(y_pred)\n",
    "```\n",
    "\n",
    "```\n",
    "[[ 0.63  0.37]\n",
    " [ 0.98  0.02]\n",
    " [ 0.93  0.07]\n",
    " ..., \n",
    " [ 0.99  0.01]\n",
    " [ 0.98  0.02]\n",
    " [ 0.89  0.11]]\n",
    "```\n",
    "\n",
    "Think about why it is a two-dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "c3d4ea02650a58388719291d52657097",
     "grade": false,
     "grade_id": "y_pred_final",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = get_final_rfc(X_train, y_train, X_test, best_max_features, check_random_state(0))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "756ea25c94cee2999c1ed3bc9d992759",
     "grade": true,
     "grade_id": "get_final_rfc_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(y_pred, np.ndarray)\n",
    "assert_array_almost_equal(y_pred[:5],\n",
    "    [[ 0.63,  0.37],\n",
    "     [ 0.98,  0.02],\n",
    "     [ 0.93,  0.07],\n",
    "     [ 0.96,  0.04],\n",
    "     [ 0.76,  0.24]]\n",
    "    )\n",
    "assert_array_almost_equal(y_pred[-5:],\n",
    "    [[ 0.78,  0.22],\n",
    "     [ 0.87,  0.13],\n",
    "     [ 0.99,  0.01],\n",
    "     [ 0.98,  0.02],\n",
    "     [ 0.89,  0.11]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot ROC curve\n",
    "\n",
    "- Use `sklearn.roc_curve()` to plot the ROC curve. The False Positive Rate should go on the $x$-axis, and the True Positive Rate on the $y$-axis. Your plot should have a title and axis labels. You should also display the area under the ROC curve in a legend.\n",
    "\n",
    "Here's an example:\n",
    "\n",
    "![](https://raw.githubusercontent.com/UI-DataScience/info490-sp16/master/Week4/assignments/images/roc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "89f3c2ee9bb864bfcaa4373b3c941fcc",
     "grade": false,
     "grade_id": "plot_roc_curve_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_test, y_pred):\n",
    "    '''\n",
    "    Plots ROC curve with FPR on the x-axis and TPR on the y-axis.\n",
    "    Displays AUC ROC in the legend.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    y_test: A pandas.DataFrame\n",
    "    y_pred: A two dimensional array from get_final_rfc()\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A matplotlib.Axes instance\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "6a630b277bf67f5fc1daa2b90a3f6d50",
     "grade": false,
     "grade_id": "ax",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ax = plot_roc_curve(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "07cd079fd3f99c021c253480ed0c6d96",
     "grade": true,
     "grade_id": "plot_roc_curve_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(ax, mpl.axes.Axes, msg=\"Your function should return a matplotlib.axes.Axes object.\")\n",
    "\n",
    "assert_equal(len(ax.lines), 1)\n",
    "\n",
    "assert_is_not(len(ax.title.get_text()), 0, msg=\"Your plot doesn't have a title.\")\n",
    "assert_is_not(ax.xaxis.get_label_text(), '', msg=\"Change the x-axis label to something more descriptive.\")\n",
    "assert_is_not(ax.yaxis.get_label_text(), '', msg=\"Change the y-axis label to something more descriptive.\")\n",
    "    \n",
    "line = ax.get_lines()[0]\n",
    "xdata = line.get_xdata()\n",
    "ydata = line.get_ydata()\n",
    "assert_array_almost_equal(\n",
    "    xdata[:5], \n",
    "    [0., 5.76103238e-05, 1.72830971e-04, 4.60882590e-04, 7.48934209e-04]\n",
    "    )\n",
    "assert_array_almost_equal(\n",
    "    ydata[:5],\n",
    "    [0., 0.00339426, 0.00966057, 0.01697128, 0.0227154])\n",
    "\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "assert_equal('area' in labels[0].lower() or 'auc' in labels[0].lower(), True)\n",
    "assert_equal('{:0.2f}'.format(roc_auc_score(y_test, y_pred[:, 1])) in labels[0], True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
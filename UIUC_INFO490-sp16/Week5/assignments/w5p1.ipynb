{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will be collected automatically at **6pm on Monday** from `/home/data_scientist/assignments/Week5` directory on the course JupyterHub server. If you work on this assignment on the course Jupyterhub server, just make sure that you save your work and instructors will pull your notebooks automatically after the deadline. If you work on this assignment locally, the only way to submit assignments is via Jupyterhub, and you have to place the notebook file in the correct directory with the correct file name before the deadline.\n",
    "\n",
    "1. Make sure everything runs as expected. First, restart the kernel (in the menubar, select `Kernel` → `Restart`) and then run all cells (in the menubar, select `Cell` → `Run All`).\n",
    "2. Make sure you fill in any place that says `YOUR CODE HERE`. Do not write your answer in anywhere else other than where it says `YOUR CODE HERE`. Anything you write anywhere else will be removed by the autograder.\n",
    "3. Do not change the file path or the file name of this notebook.\n",
    "4. Make sure that you save your work (in the menubar, select `File` → `Save and CheckPoint`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5.1. Dimensional Reduction.\n",
    "\n",
    "This problem will give you a chance to practice using a dimensional reduction technique (PCA)  on Delta Airline's aircrafts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e6667997d142205a5705d39ee4e90ab3",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from nose.tools import assert_equal, assert_is_instance, assert_is_not\n",
    "from numpy.testing import assert_array_equal, assert_array_almost_equal, assert_almost_equal\n",
    "from pandas.util.testing import assert_frame_equal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta Airline (and other major airlines) has data on all of their aircrafts on their [website](http://www.delta.com/content/www/en_US/traveling-with-us/airports-and-aircraft/Aircraft.html). For example, the following image shows the specifications of AIRBUS A319 VIP.\n",
    "\n",
    "![](https://raw.githubusercontent.com/UI-DataScience/info490-sp16/master/Week5/assignments/images/AIRBUS_A319_VIP.png)\n",
    "\n",
    "### Download delta.csv.\n",
    "\n",
    "In this problem, we will use [`delta.csv`](https://github.com/INFO490/spring2015/blob/master/week13/delta.csv), a CSV file that has aircraft data taken from the Delta Airline website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e3a3ca7800a50fb1fbd41fa4b953a41f",
     "grade": false,
     "grade_id": "read_csv",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/data_scientist/data/delta.csv', index_col='Aircraft')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set has 34 columns (including the names of the aircrafts)\n",
    "  on 44 aircrafts. It inclues both quantitative measurements such as cruising speed,\n",
    "  accommodation and range in miles, as well as categorical data,\n",
    "  such as whether a particular aircraft has Wi-Fi or video.\n",
    "  These binary are assigned values of either 1 or 0, for yes or no respectively.\n",
    "  \n",
    "```python\n",
    ">>> print(df.head())\n",
    "```\n",
    "```\n",
    "                  Seat Width (Club)  Seat Pitch (Club)  Seat (Club)  \\\n",
    "Aircraft                                                              \n",
    "Airbus A319                     0.0                  0            0   \n",
    "Airbus A319 VIP                19.4                 44           12   \n",
    "Airbus A320                     0.0                  0            0   \n",
    "Airbus A320 32-R                0.0                  0            0   \n",
    "Airbus A330-200                 0.0                  0            0   \n",
    "\n",
    "                  Seat Width (First Class)  Seat Pitch (First Class)  \\\n",
    "Aircraft                                                               \n",
    "Airbus A319                           21.0                        36   \n",
    "Airbus A319 VIP                       19.4                        40   \n",
    "Airbus A320                           21.0                        36   \n",
    "Airbus A320 32-R                      21.0                        36   \n",
    "Airbus A330-200                        0.0                         0   \n",
    "\n",
    "                  Seats (First Class)  Seat Width (Business)  \\\n",
    "Aircraft                                                       \n",
    "Airbus A319                        12                      0   \n",
    "Airbus A319 VIP                    28                     21   \n",
    "Airbus A320                        12                      0   \n",
    "Airbus A320 32-R                   12                      0   \n",
    "Airbus A330-200                     0                     21   \n",
    "\n",
    "                  Seat Pitch (Business)  Seats (Business)  \\\n",
    "Aircraft                                                    \n",
    "Airbus A319                           0                 0   \n",
    "Airbus A319 VIP                      59                14   \n",
    "Airbus A320                           0                 0   \n",
    "Airbus A320 32-R                      0                 0   \n",
    "Airbus A330-200                      60                32   \n",
    "\n",
    "                  Seat Width (Eco Comfort)   ...     Video  Power  Satellite  \\\n",
    "Aircraft                                     ...                               \n",
    "Airbus A319                           17.2   ...         0      0          0   \n",
    "Airbus A319 VIP                        0.0   ...         1      0          0   \n",
    "Airbus A320                           17.2   ...         0      0          0   \n",
    "Airbus A320 32-R                      17.2   ...         0      0          0   \n",
    "Airbus A330-200                       18.0   ...         1      1          0   \n",
    "\n",
    "                  Flat-bed  Sleeper  Club  First Class  Business  Eco Comfort  \\\n",
    "Aircraft                                                                        \n",
    "Airbus A319              0        0     0            1         0            1   \n",
    "Airbus A319 VIP          0        0     1            1         1            0   \n",
    "Airbus A320              0        0     0            1         0            1   \n",
    "Airbus A320 32-R         0        0     0            1         0            1   \n",
    "Airbus A330-200          1        0     0            0         1            1   \n",
    "\n",
    "                  Economy  \n",
    "Aircraft                   \n",
    "Airbus A319             1  \n",
    "Airbus A319 VIP         0  \n",
    "Airbus A320             1  \n",
    "Airbus A320 32-R        1  \n",
    "Airbus A330-200         1  \n",
    "\n",
    "[5 rows x 33 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical Characteristics\n",
    "\n",
    "First, let's look at the attributes related to the aircraft physical characteristics:\n",
    "\n",
    "- Cruising Speed (mph)\n",
    "- Range (miles)\n",
    "- Engines\n",
    "- Wingspan (ft)\n",
    "- Tail Height (ft)\n",
    "- Length (ft)\n",
    "\n",
    "These six variables are about in the middle of the data frame (and it's part of your task to figure out where they are located).\n",
    "\n",
    "- Write a function named `plot_pairgrid()` that takes a pandas.DataFrame and uses [seaborn.PairGrid](http://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.PairGrid.html#) to visualize the attributes related to the six physical characteristics listed above. The plots on the diagonal should be histograms of corresponding attributes, and the off-diagonal should be scatter plots.\n",
    "\n",
    "Here's an example plot:\n",
    "\n",
    "![](https://raw.githubusercontent.com/UI-DataScience/info490-sp16/master/Week5/assignments/images/pair_grid_physical.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "4f1c2a09650f27c938c408c5b7624d89",
     "grade": false,
     "grade_id": "plot_pairgrid_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_pairgrid(df):\n",
    "    '''\n",
    "    Uses seaborn.PairGrid to visualize the attributes related to the six physical characteristics.\n",
    "    Diagonal plots are histograms. The off-diagonal plots are scatter plots.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: A pandas.DataFrame. Comes from importing delta.csv.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A seaborn.axisgrid.PairGrid instance.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "e292cfd6da8da87c578a0c0830af6d48",
     "grade": false,
     "grade_id": "run_plot_pairgrid",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "pg = plot_pairgrid(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are pretty strong positive correlations between all these variables, as all of them are related to the aircraft’s overall size. Remarkably there is an almost perfectly linear relationship between wingspan and tail height.\n",
    "\n",
    "The exception here is the variable right in the middle which is the number of engines. There is one lone outlier which has four engines, while all the other aircraft have two. In this way the engines variable is really more like a categorical variable, but we shall as the analysis progresses that this is not really important, as there are other variables which more strongly discern the aircraft from one another than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3b4a94cc5b0364f9f3c694114b8faafa",
     "grade": true,
     "grade_id": "plot_pairgrid_test",
     "locked": true,
     "points": 7,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "cols = ['Cruising Speed (mph)', 'Range (miles)', 'Engines',\n",
    "        'Wingspan (ft)', 'Tail Height (ft)', 'Length (ft)']\n",
    "\n",
    "assert_is_instance(pg.fig, plt.Figure)\n",
    "assert_equal(set(pg.data.columns), set(cols))\n",
    "\n",
    "for ax in pg.diag_axes:\n",
    "    assert_equal(len(ax.patches), 10)\n",
    "\n",
    "for i, j in zip(*np.triu_indices_from(pg.axes, 1)):\n",
    "    ax = pg.axes[i, j]\n",
    "    x_in = df[cols[j]]\n",
    "    y_in = df[cols[i]]\n",
    "    x_out, y_out = ax.collections[0].get_offsets().T\n",
    "    assert_array_equal(x_in, x_out)\n",
    "    assert_array_equal(y_in, y_out)\n",
    "\n",
    "for i, j in zip(*np.tril_indices_from(pg.axes, -1)):\n",
    "    ax = pg.axes[i, j]\n",
    "    x_in = df[cols[j]]\n",
    "    y_in = df[cols[i]]\n",
    "    x_out, y_out = ax.collections[0].get_offsets().T\n",
    "    assert_array_equal(x_in, x_out)\n",
    "    assert_array_equal(y_in, y_out)\n",
    "\n",
    "for i, j in zip(*np.diag_indices_from(pg.axes)):\n",
    "    ax = pg.axes[i, j]\n",
    "    assert_equal(len(ax.collections), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis (A naive approach)\n",
    "\n",
    "Next, let’s say we know nothing about dimensionality reduction techniques and just naively apply principle components to the data.\n",
    "\n",
    "- Write a function named `fit_pca()` that takes a pandas.DataFrame and uses [sklearn.decomposition.PCA](http://scikit-learn.org/0.16/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA) to fit a PCA model on all values of `df`. It returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "a898ebed06248cb4da506edd5f369e76",
     "grade": false,
     "grade_id": "fit_pca_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def fit_pca(df, n_components):\n",
    "    '''\n",
    "    Uses sklearn.decomposition.PCA to fit a PCA model on \"df\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: A pandas.DataFrame. Comes from delta.csv.\n",
    "    n_components: An int. Number of principal components to keep.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    An sklearn.decomposition.pca.PCA instance.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "95c96409fa4bd71fbed6c9c9dea9c4fc",
     "grade": false,
     "grade_id": "fit_pca_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# we keep all components by setting n_components = no of cols in df\n",
    "pca_naive = fit_pca(df, n_components=df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5f0e915d1673466756b44cefba5ac910",
     "grade": true,
     "grade_id": "fit_pca_test",
     "locked": true,
     "points": 5,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(pca_naive, PCA)\n",
    "assert_almost_equal(pca_naive.explained_variance_ratio_.sum(), 1.0, 3)\n",
    "assert_equal(pca_naive.n_components_, df.shape[1])\n",
    "assert_equal(pca_naive.whiten, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot explained variance\n",
    "\n",
    "- Write a function named `plot_naive_variance()` that visualizes the percentage of variance explained by each of the selected components.\n",
    "\n",
    "You should plot the explained variance of at least 4 components. Also note that the y-axis of the following plot is on a log scale. Remember that all sample plots are just examples. You don't have use a log scale here, and you don't have to try to make all your plots look exactly like my examples.\n",
    "\n",
    "Here's a sample plot:\n",
    "\n",
    "![](https://raw.githubusercontent.com/UI-DataScience/info490-sp16/master/Week5/assignments/images/var_naive.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "87d3284df87429a6c4b57a8e06d05279",
     "grade": false,
     "grade_id": "plot_naive_variance_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_naive_variance(pca):\n",
    "    '''\n",
    "    Plots the variance explained by each of the principal components.\n",
    "    Attributes are not scaled, hence a naive approach.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pca: An sklearn.decomposition.pca.PCA instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A matplotlib.Axes instance.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "dd8abff0a17ab7b0ecb67ef6a8e9d18b",
     "grade": false,
     "grade_id": "plot_naive_variance_run",
     "locked": true,
     "points": 10,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "naive_var = plot_naive_variance(pca_naive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "baa8a5f200012fe700ec9183a84ee2ad",
     "grade": true,
     "grade_id": "plot_naive_variance_test",
     "locked": true,
     "points": 7,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(naive_var, mpl.axes.Axes)\n",
    "assert_equal(len(naive_var.lines), 1)\n",
    "\n",
    "assert_is_not(len(naive_var.title.get_text()), 0,\n",
    "    msg=\"Your plot doesn't have a title.\")\n",
    "assert_is_not(naive_var.xaxis.get_label_text(), '',\n",
    "    msg=\"Change the x-axis label to something more descriptive.\")\n",
    "assert_is_not(naive_var.yaxis.get_label_text(), '',\n",
    "    msg=\"Change the y-axis label to something more descriptive.\")\n",
    "\n",
    "xdata, ydata = naive_var.lines[0].get_xydata().T\n",
    "assert_array_equal(xdata[:4], list(range(4)))\n",
    "assert_array_almost_equal(ydata[:4], pca_naive.explained_variance_ratio_[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking this naive approach, we can see that the first principal component accounts for 99.9% of the variance in the data. (Note the y-axis is on a log scale.) Looking more closely, we see that the first principle component is just the range in miles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "2019e0d441da5ff5e8a815971234d649",
     "grade": false,
     "grade_id": "find_percent_var",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "abs_val = np.abs(pca_naive.components_[0])\n",
    "max_pos = abs_val.argmax()\n",
    "max_val = abs_val.max()\n",
    "\n",
    "print('\"{0}\" accounts for {1:0.3f} % of the variance.'.format(df.columns[max_pos], max_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is because the scale of the different variables in the data set is quite variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization\n",
    "\n",
    "PCA is a scale-dependent method. For example, if the range of one column is [-100, 100],\n",
    "  while the that of another column is [-0.1, 0.1], PCA will place more weight\n",
    "  on the feature with larger values.\n",
    "  One way to avoid this is to *standardize* a data set by\n",
    "  scaling each feature so that the individual features all look like\n",
    "  Gausssian distributions with zero mean and unit variance.\n",
    "  \n",
    "For further detail, see\n",
    "  [Preprocessing data](http://scikit-learn.org/stable/modules/preprocessing.html).\n",
    "  The function scale provides a quick and easy way to\n",
    "  perform this operation on a single array-like dataset.\n",
    "  \n",
    "- Write a function named `standardize()` that uses [sklearn.preprocessing.StandardScaler](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler) to scale each features so that they have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "290ff7a91ed64773a5150bed6c11a77d",
     "grade": false,
     "grade_id": "standardize_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def standardize(df):\n",
    "    '''\n",
    "    Uses sklearn.preprocessing.StandardScaler to make each features look like\n",
    "    a Gaussian with zero mean and unit variance.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: A pandas.DataFrame\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A numpy array.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "3cb21b567b0548676cb2827de8b293da",
     "grade": false,
     "grade_id": "standardize_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "scaled = standardize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "9c49f1fd0ac1e8c38e439b61f2f7d6eb",
     "grade": true,
     "grade_id": "standardize_test",
     "locked": true,
     "points": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "n_samples, n_features = 4, 5\n",
    "\n",
    "df_t1 = pd.DataFrame(\n",
    "    rng.randn(n_samples, n_features),\n",
    "    index=[i for i in 'abcd'],\n",
    "    columns=[c for c  in 'abcde']\n",
    "    )\n",
    "df_t1.loc[:, 'a'] = 0.0  # make first feature zero\n",
    "\n",
    "scaled_t1 = standardize(df_t1)\n",
    "\n",
    "assert_is_not(df_t1, scaled_t1)\n",
    "assert_is_instance(scaled_t1, np.ndarray)\n",
    "assert_array_almost_equal(\n",
    "    scaled_t1.mean(axis=0),\n",
    "    n_features * [0.0] # scaled data should have mean zero\n",
    "    ) \n",
    "assert_array_almost_equal(\n",
    "    scaled_t1.std(axis=0),\n",
    "    [0., 1., 1., 1., 1.] # unit variance except for 1st feature\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "ed7c55a43da869c133e44ec46e719a1f",
     "grade": false,
     "grade_id": "run_scaled",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# we keep only 10 components\n",
    "n_components = 10\n",
    "pca = fit_pca(scaled, n_components=n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot explained variance\n",
    "\n",
    "- Visualize the explained variance of the first 10 principal components from the scaled data.\n",
    "\n",
    "Here's an example plot:\n",
    "\n",
    "![](https://raw.githubusercontent.com/UI-DataScience/info490-sp16/master/Week5/assignments/images/var_scaled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "5ecef84edcb46629c306b93f157b8a02",
     "grade": false,
     "grade_id": "plot_scaled_variance_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_scaled_variance(pca):\n",
    "    '''\n",
    "    Plots the variance explained by each of the principal components.\n",
    "    Features are scaled with sklearn.StandardScaler.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pca: An sklearn.decomposition.pca.PCA instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A matplotlib.Axes instance.\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "81fcd43672f4e867c53a4f29867b2108",
     "grade": false,
     "grade_id": "plot_scaled_variance_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "ax = plot_scaled_variance(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "961f497219179e0b6bfb87ea12ea108d",
     "grade": true,
     "grade_id": "plot_scaled_variance_test",
     "locked": true,
     "points": 7,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(ax, mpl.axes.Axes)\n",
    "assert_equal(len(ax.lines), 1)\n",
    "\n",
    "assert_is_not(len(ax.title.get_text()), 0, msg=\"Your plot doesn't have a title.\")\n",
    "assert_is_not(ax.xaxis.get_label_text(), '', msg=\"Change the x-axis label to something more descriptive.\")\n",
    "assert_is_not(ax.yaxis.get_label_text(), '', msg=\"Change the y-axis label to something more descriptive.\")\n",
    "\n",
    "xdata, ydata = ax.lines[0].get_xydata().T\n",
    "assert_array_equal(xdata, list(range(n_components)))\n",
    "assert_array_almost_equal(ydata, pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Great, so now we’re in business. There are various rules of thumb for selecting the number of principal components to retain in an analysis of this type, one of which I’ve read about is\n",
    "\n",
    "- Pick the number of components which explain 85% or greater of the variation.\n",
    "\n",
    "So, we will keep the first 4 principal components (remember that we are counting from zero, so we are keeping 0th, 1st, 2nd, and 3rd components&mdash;four components). In Problem 8.2, we will use these four components to fit a $k$-means model. Before we move on to the next problem, let's apply the dimensional reduction on the scaled data. (In the previous sections, we didn't actually have to apply `transform()`. This step is to make sure that the scaled data is actually \"transformed\".)\n",
    "\n",
    "- Write a function named `reduce()` that takes a PCA model (that is already trained on array) and a Numpy array, and applies dimensional reduction on the array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "f88decf261e2b69075e20f0962f25c07",
     "grade": false,
     "grade_id": "reduce_answer",
     "locked": false,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def reduce(pca, array):\n",
    "    '''\n",
    "    Applies the `pca` model on array.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pca: An sklearn.decomposition.PCA instance.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A Numpy array\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "nbgrader": {
     "checksum": "9225b92f02b36edc51ddf92a2a433275",
     "grade": false,
     "grade_id": "reduce_run",
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "reduced = reduce(pca, scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": false,
    "nbgrader": {
     "checksum": "66c4f7973642f08dfcb459310f273a5e",
     "grade": true,
     "grade_id": "reduce_test",
     "locked": true,
     "points": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_is_instance(reduced, np.ndarray)\n",
    "assert_array_almost_equal(reduced, pca.fit_transform(scaled))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}